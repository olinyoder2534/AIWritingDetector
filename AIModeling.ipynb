{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEVb4wAhsMEWRwAXYijHpL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olinyoder2534/AIWritingDetector/blob/main/AIModeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q843xgDdsfCL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/trainCleanEmbeddings.csv')\n",
        "test = pd.read_csv('/content/testCleanEmbeddings.csv')"
      ],
      "metadata": {
        "id": "cKQsw5PAsh5R"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "collapsed": true,
        "id": "GY3tFmYH1amb",
        "outputId": "b5b367ac-1409-40c9-a3da-9584e02965ad"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  \\\n",
              "0  Climate change is a real and pressing issue th...   \n",
              "1  The school system says we as students should a...   \n",
              "2  Dear TEACHER_NAME\\n\\nI believe policy 1 is the...   \n",
              "3  The believe Of the \"Face On Mars\" being create...   \n",
              "4  I've never had to complete a summer project be...   \n",
              "\n",
              "                                           textNoPII  label  \\\n",
              "0  climate change be a real and pressing issue th...      1   \n",
              "1  the school system say we as student should all...      0   \n",
              "2  dear teacher_name   I believe policy 1 be the ...      0   \n",
              "3  the believe of the face on mars be create by a...      0   \n",
              "4  I have never have to complete a summer project...      0   \n",
              "\n",
              "                                         SpacyVector  \\\n",
              "0  [-1.5221697e+00  1.0762936e+00 -3.3516099e+00 ...   \n",
              "1  [-1.39287770e+00  2.54524851e+00 -3.64133167e+...   \n",
              "2  [-1.4394611e+00  2.6702836e+00 -3.0965433e+00 ...   \n",
              "3  [-2.1805670e+00  2.1138906e+00 -2.8518946e+00 ...   \n",
              "4  [-1.5289832   2.4396446  -3.5379148  -0.676052...   \n",
              "\n",
              "                                          BertVector  \n",
              "0  [-0.89259857 -0.68195873 -0.98276216  0.714381...  \n",
              "1  [-0.685052   -0.50406086 -0.98186547  0.699532...  \n",
              "2  [-0.7196923  -0.62672627 -0.9637501   0.742233...  \n",
              "3  [-0.7942458  -0.55822563 -0.98445326  0.788272...  \n",
              "4  [-0.81295913 -0.59700686 -0.9481772   0.778208...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c58d0331-0cc8-461c-9216-e8e068a2648a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>textNoPII</th>\n",
              "      <th>label</th>\n",
              "      <th>SpacyVector</th>\n",
              "      <th>BertVector</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Climate change is a real and pressing issue th...</td>\n",
              "      <td>climate change be a real and pressing issue th...</td>\n",
              "      <td>1</td>\n",
              "      <td>[-1.5221697e+00  1.0762936e+00 -3.3516099e+00 ...</td>\n",
              "      <td>[-0.89259857 -0.68195873 -0.98276216  0.714381...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The school system says we as students should a...</td>\n",
              "      <td>the school system say we as student should all...</td>\n",
              "      <td>0</td>\n",
              "      <td>[-1.39287770e+00  2.54524851e+00 -3.64133167e+...</td>\n",
              "      <td>[-0.685052   -0.50406086 -0.98186547  0.699532...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dear TEACHER_NAME\\n\\nI believe policy 1 is the...</td>\n",
              "      <td>dear teacher_name   I believe policy 1 be the ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[-1.4394611e+00  2.6702836e+00 -3.0965433e+00 ...</td>\n",
              "      <td>[-0.7196923  -0.62672627 -0.9637501   0.742233...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The believe Of the \"Face On Mars\" being create...</td>\n",
              "      <td>the believe of the face on mars be create by a...</td>\n",
              "      <td>0</td>\n",
              "      <td>[-2.1805670e+00  2.1138906e+00 -2.8518946e+00 ...</td>\n",
              "      <td>[-0.7942458  -0.55822563 -0.98445326  0.788272...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I've never had to complete a summer project be...</td>\n",
              "      <td>I have never have to complete a summer project...</td>\n",
              "      <td>0</td>\n",
              "      <td>[-1.5289832   2.4396446  -3.5379148  -0.676052...</td>\n",
              "      <td>[-0.81295913 -0.59700686 -0.9481772   0.778208...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c58d0331-0cc8-461c-9216-e8e068a2648a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c58d0331-0cc8-461c-9216-e8e068a2648a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c58d0331-0cc8-461c-9216-e8e068a2648a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-51efb14f-9d93-47f2-95b8-4ba1de4a7568\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-51efb14f-9d93-47f2-95b8-4ba1de4a7568')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-51efb14f-9d93-47f2-95b8-4ba1de4a7568 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train",
              "summary": "{\n  \"name\": \"train\",\n  \"rows\": 40000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40000,\n        \"samples\": [\n          \"I believe that having broad knowledge of many academic subjects is better than having a specific focus on one specific subject. For example, I have a broad knowledge of history, which is why I am very knowledgeable about the history of the United States. Ky learning about all the different aspects of history, I am able to better understand the historical events that have occurred and the people that have been involved. This is also beneficial because it allows me to learn about different cultures and their history.\\n\\nSimilarly, I have a strong interest in philosophy. I have studied philosophy for years, and I am very knowledgeable about the different branches of philosophy. Ky understanding the different philosophies, I can better understand my own thoughts and how they might be viewed by others. This is important because it allows me to form my own opinions and to think about the different arguments that are being made about different subjects.\\n\\n\",\n          \"There are a few things to Unpack in this prompt. The first is the idea that children should learn to compete, and the second is the idea that children should be taught to cooperate. There are pros and cons to both approaches, and Ultimately it depends on what values you want to instill in your children.\\n\\nIf you want your children to learn to compete, then you would likely focus on teaching them individual achievement. They would learn to set goals and strive to reach them, even in the face of adversity. You would also teach them how to be a good sport, and how to handle both winning and losing gracefully. The goal would be to instill in them a sense of determination and perseverance.\\n\\nOn the other hand, if you want your children to learn to cooperate, then you would focus on teaching them teamwork and communication. They would learn how to work together towards a common goal, and how to resolve conflicts. The goal would be to instill in them a sense of community and cooperation.\\n\\nBoth approaches have their merits, and Ultimately it depends on what you want your children to learn. If you want them to be successful in a competitive world, then teaching them to compete is a good approach. If you want them to be kind and compassionate citizens, then teaching them to cooperate is a good approach. There is no right or wrong answer, and it Ultimately comes down to your personal values.\\n\\n\",\n          \"University education has always been a topic of debate; some believe it's only purpose is to prepare students for employment, while others argue that it has different functions. In MH opinion, university education has many other functions besides just preparing students for employment.\\n\\nFirstly, university education plans a crucial role in personal development. It provides an opportunity for individuals to broaden their knowledge about the world and themselves. Students can explore different areas of study, engage in intellectual discussions, and develop critical thinking skills. Furthermore, university education helps individuals to become well rounded individuals BH exposing them to diverse cultures and perspectives, which contributes to their overall personal growth.\\n\\nSecondly, university education has the function of preparing students for civic engagement. University graduates have the knowledge and skills needed to participate in public debates, make informed decisions, and contribute positively to society. The learn about the importance of civic responsibilities and the role the can play in shaping the future of their communities.\\n\\nLastly, university education has the function of creating a better future for humanity. Universities are centers of research and innovation, where students and professors can work together to develop new technologies and ideas that solve some of the world's most pressing problems. For example, universities are at the forefront of developing renewable energy sources, curing diseases, and addressing climate change.\\n\\nTo conclude, while some people MAH think that the sole purpose of university education is to prepare students for employment, I believe it'serves many other functions. University education helps individuals to develop personally, prepares them for civic engagement, and contributes to creating a better future for humanity.\\n\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"textNoPII\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 39700,\n        \"samples\": [\n          \"the advancement of technology in the 21st century have offer different way of learn for the student of today one of those way be know as distance learning in which student learn from a class through online video conferencing at home this method of learning be beneficial to student since class absence would not be as detrimental it provide a different type of learning and it become a great convenience to student   student often face class absence when they be sick or an unavoidable event come in to their life unfortunately when student face these instance they be force to miss to their class vowever the beauty of distance learning enable the opportunity for the student to attend class via video call this way the student be able to absorb the lesson while they be at home or at a different location other than school distance learning can be utilize by any student and be beneficial to any student 's learning progression   not only be distance learn beneficial to any student but it can become a new and well way of learn some student be more comfortable with a different style of learn other than their peer while some may be learner other may be more visual distance learning create a new and effective way of visual learning by display the lesson on a screen not only do this method change visual learning it also open up the possibility of record the class lesson so that student be able to refer back to it whenever they can distance learning be a betterment to a certain style of learn for a student and also create the possibility for the lesson to be record as a video for future reference   distance learning have multiple case in which the student be able to learn effectively while they be not at school this be a convenience for student that often miss class due to illness or event away from school that involve the student this way of learning create a sense of convenience for the student with this method of learn to be admit into school student would not have to be worried about absence since the quality of distance learning be mean to combat that video conference the lesson at home or at a different location this become convenient to the student which benefit their progress of learn   in conclusion the outcome of utilize distance learn in school be beneficiary to a student 's learning since it be quite accessible to student it create a different style of learning and it offer a well convenient way of learning distance learning be a method which will stop the impediment of a student 's education if they be unable to attend class it be imperative to further the development of education as society go on and technology advance this be why distance learning need to be implement into school in order to advance the youth 's education so that the america of tomorrow can become bright than today \\u00a0 \",\n          \"limit car usage advantage for citizen and community   there be a number of compelling advantage to limit car usage both for individual and the community in which we live reduce our reliance on personal vehicle can have wide range environmental health social and economic benefit    to begin with curb car use be positive for the environment automobile be major contributor to air pollution and greenhouse gas emission when few vehicle be on the road each day there be less carbon emit into the atmosphere and clean air for all to breathe several study have find direct correlation between low vehicle traffic and decreased level of smog nitrogen oxide and particulate matter source 1 with less pollution plant and animal life face few stressor   in addition to environmental benefit limit car trip improve public health walking or biking as alternative to drive result in more physical activity for individual regular exercise reduce risk for many chronic disease and help manage weight source 2 furthermore study show community with robust network for walk and cycling report high wellbeing among resident source 3 reduce automobile emission also mean less respiratory issue from poor air quality a healthy population mean low healthcare cost over the long run    socially have reliable option besides private car foster strong community bond public space become more usable and appeal for socialize when they be less dominate by vehicle with few car on the road during peak time neighborhood feel safe and more accommodate for child and elderly to navigate source 2 a sense of community be invaluable to quality of life    finally limit individual car ownership yield financial benefit beyond health and environmental saving household spend less on car payment insurance maintenance and fuel each month this extra income can be redirect to other pursuit that far enhance standard of living for municipality few road need construct and repair free fund for school park and other community amenity that unite resident   in summary curb excessive car usage through alternative transit strategy lead to tangible advantage across many sphere from environmental preservation and public health to social wellbeing and household finance a multipronged effort of infrastructural change policy initiative and cultural shift could realize these collective gain that uplift both individual and the great community at large thoughtful limitation on car dependence hold great promise for sustainability and quality of life into the future\",\n          \"the race on mars be not an alien monument or creation a lot of people do think that it be make ly some alien life form lut they be mistaken scientist have do multiple study to conclude that it be a natural land formation   first of all the formation be know as a mesa there be shadow from the formation that give the illusion of eye a mouth and a nose also in the picture from 2001 each pixel in the photo graph be 1.56 meter on the planet so you would le all to see any artifact or structure if there be any   in conclusion the race on mars be not an alien formation scientist have prove this ly the effect of the shadow on the rock the fact that it be a mesa and ly there not lang any artifact on or around the formation therefore this be not evidence of there lang ancient civilization on mars\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SpacyVector\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 39690,\n        \"samples\": [\n          \"[-1.4268802   3.2982314  -3.6366823  -0.1478822   1.7192017   1.6571584\\n  0.10703991  4.4655933  -1.7504191   0.42994285  7.549321    2.2140877\\n -4.3480706   1.1623416   1.3610343   0.66297257  2.221577   -2.441451\\n -2.252658   -1.6777819   2.1399531  -1.1274552  -0.8030299  -1.9176319\\n -0.1360639  -2.1956306  -3.4805908   0.09277004 -1.9016781   1.3791928\\n  1.8743238  -0.9638617  -1.2289627  -1.661409    0.20676084 -0.22406356\\n -0.01182326  0.71395683  3.2528658   1.421667   -0.41545412  1.873646\\n  1.4933782  -0.31595793 -1.2673862   1.2925297   1.8485553  -4.9043317\\n -1.6120728   2.3195982  -0.18851939  1.6045125  -0.36310422 -5.8670444\\n -2.7544835   0.5819574   0.22653852  1.9631819   0.58122236  0.83376426\\n  1.747314   -0.41994667 -1.8330036  -1.3086987   1.6348876   1.8977553\\n -3.3614469  -3.7592297   0.14346167  4.577473   -0.5604797   1.4087644\\n -0.8883638  -0.21260121 -0.56140524  2.2795007  -3.143261    1.1806839\\n -4.3896313  -1.5702766  -5.490141    0.21718729  3.6881154   0.38720936\\n  1.5384262  -0.04300385  0.07338956 -1.5913038   1.8852777  -0.2441566\\n -0.49089628 -0.9899433   2.1295612  -4.949581    2.0982099  -2.0276327\\n  0.88262004 -2.1204298  -0.14009269  1.1882449   1.3680468   1.4945277\\n  2.2865453   1.3908117  -1.9938251   3.7319345  -2.5735972  -2.46102\\n -1.8538806  -2.7729897   1.1240026  -0.20310676 -0.55778974  1.3562918\\n  0.07675381  2.3614147  -3.4418721  -1.9067886   2.1834738  -1.0395391\\n -2.5219123  -2.2831867  -2.2354693   0.9416851  -1.3012669  -2.6212082\\n  2.316031   -1.434213    1.7973809  -0.7136781  -1.9306159  -0.02111777\\n  2.9660687  -1.1835285  -0.7789042   0.91666883 -2.2328572  -0.36211646\\n  4.5552053  -1.6669207  -2.520475   -0.71517974  1.3521075   1.3698839\\n -0.119034   -0.31512317 -2.8293118  -0.7554416   0.22210595  0.7470232\\n -1.9590062   3.8541365  -0.3982222   1.0038024  -0.73927647  1.5558949\\n  5.3412437   0.39308506 -1.7817321  -1.4371648  -2.5208561  -3.1389995\\n -1.557585    1.2826191  -2.5804198  -0.5938897  -4.900876    0.7800631\\n -0.20793001 -0.61589926  1.6069465   0.58657676  1.7115166   2.3104649\\n  3.1245563  -0.8481155  -1.4935552  -0.9175656  -0.76786447 -2.864173\\n -0.4130784   0.04686359  3.3299599  -1.4078643  -1.2245599   0.5387534\\n -1.459725   -3.083459    3.1159534   3.6961496  -1.2331281   0.1024975\\n -1.7592287  -1.9560013  -0.6740947  -0.8512206  -3.3139389   0.02945062\\n -1.2851486   1.623724   -3.1876714  -0.21677612 -2.0425842  -3.1094596\\n  0.5724563   1.1183165  -3.04985     1.3703535  -0.67385226  0.06427027\\n  2.1669862   0.04183456 -0.7589197   3.7115316  -0.07013872  2.9525888\\n  0.69541585 -2.6059186  -1.6303462   0.55774814 -2.9886916   0.03089698\\n  0.4455961   0.55774784 -0.562703   -2.953118   -0.64906096  2.1132429\\n  2.4961607   1.1048985   0.01144074 -3.7448645  -0.83181614  1.42087\\n  1.385662    1.0808753  -2.0359159   1.7927514  -1.4211566  -0.68796116\\n -2.5910718   0.6737819   1.420365    1.4572788  -0.7403259   0.60692525\\n -2.2427268   0.51090115  3.118036    2.8271146  -0.21664585 -0.46547177\\n -5.334408    0.6200915   0.35575432 -2.871638    1.8703899   0.6207332\\n -0.3048564   1.2050982   0.46133032  5.3067303   3.0765753   2.4496398\\n  2.5350678  -0.6820676   0.62254274  3.3046358  -5.1731086  -0.22862457\\n  1.9099052  -1.7939909  -1.3802083  -1.9220276  -0.71379375 -2.6908624\\n  1.0514771  -0.43760744 -2.4206324   2.3015323  -0.71785706 -1.0961137\\n  0.25807577  1.5376785   4.86743    -1.1407818   1.4556094   2.1435626\\n -2.0694475   0.60545313  1.6920227  -1.2476996   0.29861706  0.9944517\\n -1.7058164   0.647841    1.0839828   0.04941095 -3.8807535   2.3710856 ]\",\n          \"[-1.9006252   0.9337525  -1.7420855   0.6836785   3.4441335   0.49040672\\n  0.01827949  4.1055317  -0.01796136 -0.12086812  6.271096    1.4874679\\n -3.7055442   1.7810868   0.9689436   1.7944614   1.0381082  -0.06948232\\n -1.0599337  -2.3179755   1.0863947  -0.5075414  -1.6351886   0.21555793\\n -0.09016158 -0.91626006 -2.8999753  -0.85963815 -1.1577005   2.1402462\\n  1.5792412  -0.4707249  -1.0923245  -2.218449   -2.1976244  -1.0417436\\n -1.0216048   1.2930126   1.3283572   1.0167462  -0.05724187  0.38662934\\n  0.5037484   0.36682755 -1.1262704   2.1225145   0.6858774  -2.0430257\\n -0.23532961  1.3226255  -1.2070383   1.7763872  -0.6534761  -4.460494\\n -0.8868142   1.3519775  -0.30353442  0.74956125  0.6060294  -0.28148946\\n -0.9570371  -1.5376203  -0.44360387 -1.101252    2.3009942   2.1561067\\n -2.7413738  -2.5277214   0.8870899   2.9713268  -1.7213681  -0.02462003\\n -1.8728826   0.17599298 -0.21027507  1.8388929  -2.3236885   1.6776354\\n -3.5157022  -0.37209496 -3.9879966   0.2959154   1.5825917   0.226782\\n  2.6187289   0.1820407  -1.3635874  -1.9265143   0.9652856  -0.3800215\\n -1.1739354  -0.46425462  2.6939125  -3.081474    1.0204939  -1.145687\\n  1.8179708  -0.81709635 -0.10176698  2.0750794   1.7630947   1.123795\\n  2.4139652   2.2529159  -0.2962843   3.3114502  -0.02043493 -2.7487333\\n -0.05793067 -2.70124     0.8161367   1.206048   -2.083401    1.2154305\\n  0.84490764  1.2744204  -1.5107614  -0.2328258   0.78126514 -1.1736289\\n -1.5620111  -2.5744274   0.31454253  1.4652048  -1.4315472  -3.4074898\\n  0.20366077 -2.2832062   2.3163514  -1.1970459  -2.569242    0.30208862\\n  2.9519835  -0.2170143  -0.19128692  0.5716286  -1.6589041  -0.29954782\\n  1.8470249  -1.0637143  -1.4255898  -0.18633473 -0.0682017   0.99026066\\n  1.8771981   0.46695837 -3.3122313  -0.38678777  1.9112432   1.5496436\\n  0.17734317  2.4745176   0.10404855  0.6836098  -1.255939    1.3176752\\n  2.383213   -0.44113943 -1.440264   -1.4989817  -1.4332275  -1.4201628\\n  0.06698935  1.8043628  -1.7346672  -0.5716605  -3.435457   -0.3663571\\n  0.58507806  0.5278912   0.67578983  0.64787894  1.1961845   0.78693116\\n  1.1758841   0.26659432 -0.04067551 -0.5443476  -2.2982433  -1.5988715\\n -1.0780822   0.15357842  1.0107578  -0.99256593 -1.2048293   0.2203694\\n -1.470522   -0.21850985  0.99588245  1.5820222  -0.46387702 -1.5494123\\n  0.37766987 -2.3543422  -0.06354424  1.051412   -3.0525818  -0.4567996\\n -0.4006331   0.4829552  -1.4136102  -1.2852945  -0.48579013 -2.700878\\n  3.3747623   0.68478227 -3.3429904   1.6384777   0.26262057  0.1990823\\n  2.1315804   1.2924033  -1.5426853   2.029836    0.47927573  2.6276383\\n  1.2375749  -2.1989155  -1.4540724  -0.476099   -1.9163116   1.2002331\\n -0.90125406  0.53831345 -3.037143   -2.065049   -0.4952486   3.0475295\\n  2.1220925   1.0092443   2.6704729  -1.9499825  -0.45104843  2.4375935\\n  1.945262    1.9378366  -1.3856522   0.0117935  -1.328926   -0.71744645\\n -1.3837029   0.13037145  0.4379696   0.8710324  -1.531247    0.9887999\\n -2.4489565   1.507658    0.9081998   2.4196982   1.648763   -2.2439303\\n -4.9426847   0.3151504   0.3412085  -2.5695965   1.5373764   0.07296792\\n  0.2781023   0.8144336   0.0182703   5.6612735   2.4534798   1.6775341\\n  1.6859257  -1.2734646  -0.20231001  1.8348984  -2.452667   -0.08192953\\n  1.5362693  -0.81499493  0.4463224  -1.4521538   0.72151667 -0.7836223\\n  2.15531    -1.2108992  -1.3292177   1.7833469   1.0592293   0.02562384\\n  1.4070346   0.9846301   2.2306278  -1.2340589  -0.26374763  0.94929814\\n -1.2210985   0.24000977  0.9332526  -0.78013897  0.106914    0.6890508\\n -1.268232    1.2993332   0.05219246 -1.9050204  -2.0547376   0.9169249 ]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BertVector\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 39158,\n        \"samples\": [\n          \"[-0.7659222  -0.58553284 -0.96461207  0.64603746  0.6926773  -0.35254154\\n  0.3736579   0.501377   -0.8694693  -0.9999274  -0.41631475  0.98054343\\n  0.94492704  0.63389003  0.5336741  -0.5634017  -0.13205609 -0.68445754\\n  0.5084039   0.7002326   0.5830777   0.9999994  -0.3492402   0.5445526\\n  0.44192305  0.98744947 -0.83271253  0.7649171   0.8307608   0.51405084\\n -0.4829999   0.5061877  -0.9813421  -0.39414734 -0.9834512  -0.9792795\\n  0.6679929  -0.27358103 -0.03644759 -0.25937837 -0.5500986   0.44004476\\n  0.9999279  -0.58721936  0.7013014  -0.28402454 -1.          0.41681173\\n -0.6009579   0.8772402   0.87055403  0.93261844  0.36323932  0.5841033\\n  0.5884741  -0.6238765  -0.04234764  0.3652383  -0.38182566 -0.605854\\n -0.56635886  0.65290844 -0.90788424 -0.7143944   0.903075    0.9297493\\n -0.46569154 -0.45285589 -0.3256757   0.02476893  0.6903022   0.5448596\\n -0.4424574  -0.7775118   0.76190907  0.32453954 -0.6222813   1.\\n -0.5671042  -0.9287832   0.9729586   0.908999    0.5319694  -0.6232538\\n  0.7377552  -1.          0.4895314  -0.32211277 -0.9649274   0.33391592\\n  0.5875609  -0.31356525  0.9659745   0.6027024  -0.61344916 -0.7274989\\n -0.30205622 -0.91481817 -0.44329998 -0.6208439   0.33432233 -0.48893878\\n -0.58997035 -0.46928245  0.4026024  -0.6401556  -0.14518109  0.74663424\\n  0.584986    0.6409042   0.547939   -0.42417005  0.46653345 -0.8394862\\n  0.61843044 -0.42901987 -0.96450853 -0.5715808  -0.96573764  0.47468504\\n -0.50275385 -0.34032667  0.7173728  -0.7963089   0.68185306 -0.5281238\\n -0.952266   -1.         -0.7585638  -0.7388637  -0.38721198 -0.50983554\\n -0.932464   -0.90791196  0.6193402   0.82966113  0.42119506  0.99968845\\n -0.4347975   0.7752854  -0.34904587 -0.8472756   0.63144785 -0.47059366\\n  0.9122677  -0.19490443 -0.04393277  0.24333651 -0.72948444  0.41118863\\n -0.8142014  -0.3760817  -0.86627    -0.726823   -0.45577517  0.81356573\\n -0.75596625 -0.965822   -0.39664203 -0.29999554 -0.36942774  0.5601582\\n  0.83129776  0.45560166 -0.5375652   0.4751511   0.21537402  0.4304785\\n -0.5291627  -0.42108938  0.5209914  -0.52292943 -0.97042215 -0.9251481\\n -0.33520785  0.32710442  0.93098724  0.39472595  0.5040061   0.81893104\\n -0.43222407  0.7514974  -0.93113357  0.95481324 -0.25371116  0.42065772\\n -0.78653187  0.71121436 -0.45257193  0.3118289   0.6515169  -0.8266537\\n -0.46362612 -0.15752894 -0.61036044 -0.5818549  -0.91313636  0.46724027\\n -0.49091923 -0.52501166 -0.3447114   0.77572644  0.68206465  0.35161316\\n  0.5452095   0.4359405  -0.61439466 -0.42448542  0.38008     0.25949553\\n  0.261875    0.95223653 -0.89337736 -0.07319976 -0.57346237 -0.9402191\\n  0.06494665 -0.49321055 -0.4038425  -0.6869466   0.7736778  -0.73845834\\n  0.57565016  0.39353788 -0.14206252 -0.62648773  0.2314666  -0.50672877\\n  0.63973904 -0.2625149   0.97556823  0.95993936 -0.68636584 -0.39705157\\n  0.94628155 -0.988815   -0.60216147 -0.02453778 -0.4376206   0.6012681\\n -0.6606204   0.96108264  0.94110733  0.6968362  -0.60307467 -0.945797\\n -0.37203938 -0.77894276 -0.28901863  0.41472036  0.9372109   0.5919034\\n  0.536018    0.19709727 -0.44061533  0.8700482  -0.9835326  -0.8822004\\n -0.8999271  -0.43928823 -0.96964216  0.90463287  0.50802606  0.812115\\n -0.6372835  -0.57266384 -0.8199755   0.24694726  0.343175    0.80712086\\n -0.7710836  -0.6978433  -0.7152188  -0.80861086 -0.01989739 -0.39622942\\n -0.67260706  0.18533985 -0.70645136  0.64810383  0.45661741  0.4263806\\n -0.9155217   0.9792589   1.          0.9317744   0.50255185  0.2478493\\n -0.99998635 -0.8813464   0.9999827  -0.99774563 -1.         -0.59264225\\n -0.6142133  -0.01943969 -1.         -0.31487978 -0.10754961 -0.56385475\\n  0.7757787   0.91403127  0.7578023  -1.          0.410088    0.71582484\\n -0.5538707   0.9380474  -0.6568339   0.92322457  0.58387107  0.712105\\n -0.30769125  0.6191734  -0.9708812  -0.6746857  -0.84610856 -0.8948219\\n  0.9998436   0.3431094  -0.824016   -0.5505518   0.82742995 -0.27775407\\n  0.07229829 -0.90753794 -0.4033445   0.68802017  0.7355733   0.33661678\\n  0.4968461  -0.37663162  0.2726638   0.5420952  -0.1778816   0.54423296\\n -0.865677   -0.02363821  0.62548643 -0.01361411 -0.8601291  -0.95989496\\n  0.8638127  -0.509625    0.8608938   1.          0.68264925 -0.45116216\\n  0.79020476  0.24160562 -0.67790806  1.          0.9147264  -0.9321993\\n -0.62192905  0.8377368  -0.7225872  -0.73482937  0.9974878  -0.37232816\\n -0.85897255 -0.6320586   0.9575166  -0.97482944  0.999486   -0.65550095\\n -0.89172804  0.83523893  0.6960249  -0.6947496  -0.18697612  0.28608057\\n -0.81903446  0.56747395 -0.5066978   0.69007087  0.43537697 -0.01226554\\n  0.63625985  0.19553585 -0.52158797  0.4692929  -0.73629534 -0.38318843\\n  0.98302597  0.5141799  -0.40041295  0.28697303 -0.42793855 -0.79328465\\n -0.9118017   0.82082176  1.         -0.46718845  0.8368721  -0.69923323\\n -0.18867037  0.14845787  0.6203037   0.64127004 -0.37531117 -0.5105945\\n  0.7616447  -0.66108    -0.979673    0.09499642  0.41006768 -0.3074225\\n  0.9999881   0.5980858   0.3020615   0.40176266  0.98693043  0.3473215\\n  0.08478669  0.9536059   0.94061184 -0.3708115   0.6021412   0.31043956\\n -0.9200255  -0.49879172 -0.6749682   0.18345091 -0.9308989  -0.22883704\\n -0.8679091   0.8999943   0.91633195  0.45154384  0.44649526  0.93689716\\n  1.         -0.96958876  0.16852272  0.83686906 -0.07156231 -0.9999759\\n -0.3699118  -0.51410455 -0.14099072 -0.9139474  -0.4258363   0.29014203\\n -0.90280503  0.87022585  0.77533674 -0.7665773  -0.95045996 -0.6922356\\n  0.6456945   0.15640013 -0.9950942  -0.5457108  -0.38052294  0.5989896\\n -0.50090927 -0.73381495 -0.46248603 -0.456258    0.43657875 -0.3635329\\n  0.6349469   0.90654486  0.6411235  -0.9376388  -0.6382074  -0.3114597\\n -0.56200165  0.55310684 -0.6467267  -0.9473441  -0.373978    1.\\n -0.41045994  0.93043375  0.38195473  0.30825007 -0.4670481   0.25968364\\n  0.9878926   0.59447515 -0.8210444  -0.82508844  0.8280067  -0.5821086\\n  0.76304734  0.8532863   0.8932666   0.38904902  0.8908355   0.39190227\\n -0.16254418  0.3201844   0.96075225 -0.38910833 -0.3085693  -0.37562016\\n -0.29694447 -0.5124255   0.4541634   1.          0.34342158  0.7639341\\n -0.9736565  -0.9356973  -0.5405682   1.          0.7032766   0.11400802\\n  0.7211505   0.61788934 -0.29616472  0.19496144 -0.43010738 -0.48849034\\n  0.3930867   0.16436745  0.86253524 -0.47164568 -0.9102664  -0.3837746\\n  0.46980816 -0.83028144  0.9999975  -0.70301527 -0.43491408 -0.2250431\\n -0.7373624  -0.93290937  0.03150253 -0.9305339  -0.39641693  0.27941194\\n  0.8212418   0.32970572 -0.64428014 -0.8952904   0.8657635   0.7241737\\n -0.9541778  -0.77765805  0.872581   -0.91077876  0.6175614   1.\\n  0.5044943   0.27098116  0.30734658 -0.19265081  0.36534983 -0.72138345\\n  0.40709445 -0.76628464 -0.545284   -0.42090207  0.46180454 -0.34282288\\n -0.953409    0.07823262  0.40615618 -0.495425   -0.7207154  -0.31086743\\n  0.51873636  0.71876544 -0.4516493  -0.20491283  0.33820027 -0.14189307\\n -0.7745576  -0.60309136 -0.6152355  -0.9999994   0.5271782  -1.\\n  0.8158076   0.54235166 -0.38529027  0.64174443  0.7593479   0.85132635\\n -0.18709072 -0.9165254   0.41908532  0.44435543 -0.4364605  -0.28608164\\n -0.21834353  0.46614948 -0.43734688  0.41587722 -0.7554292   0.6867329\\n -0.41533113  1.          0.29577768 -0.7132653  -0.38089174  0.5025954\\n -0.380343    1.         -0.251661   -0.8580063   0.5749031  -0.8331195\\n -0.61136854  0.4354233   0.29659164 -0.8021815  -0.9733089   0.60571194\\n  0.5067522  -0.58748746  0.7783737  -0.55925673 -0.48960203  0.23548448\\n  0.9427063   0.9557594   0.537822    0.5812938  -0.8996237  -0.6251719\\n  0.8136204   0.6550264  -0.27683938  0.2219184   1.          0.5883134\\n -0.8386933  -0.07072136 -0.7089553  -0.45913273 -0.82015693  0.49719542\\n  0.41316152  0.8294261  -0.47447222  0.8321766  -0.90518135  0.2293929\\n -0.45857954 -0.64630955  0.5046977  -0.78453296 -0.96812963 -0.9433394\\n  0.7662003  -0.38547355 -0.0394326   0.51663035  0.18403344  0.5690595\\n  0.53419393 -1.          0.83895326  0.47889894  0.9383273   0.8218986\\n  0.8486618   0.751779    0.40956903 -0.8996719  -0.4516871  -0.5514373\\n -0.35801432  0.36489108  0.7557517   0.6614023   0.4805392  -0.44025087\\n -0.8488373  -0.8982609  -0.9683823  -0.965621    0.47456053 -0.68580186\\n -0.2698325   0.91226584  0.250115   -0.20345497 -0.39172614 -0.9191407\\n -0.06768501  0.38908595 -0.14217909  0.23146467  0.23085947  0.44573757\\n  0.66961044  0.95717174 -0.9567283   0.4795922  -0.8554666   0.56316125\\n  0.96096075 -0.87558174  0.3547714   0.75188977 -0.39748254  0.38795754\\n -0.46254438 -0.16582376  0.7682674  -0.49221373  0.57383144 -0.5495994\\n -0.26603702 -0.51242936 -0.31691927 -0.32316777 -0.71660036  0.6205301\\n  0.19096084  0.7439404   0.95058084 -0.25965607 -0.46300822 -0.35336822\\n -0.8734443  -0.7198835   0.1917916  -0.13236953 -0.6959223   0.72727716\\n  0.07946838  0.96722853  0.44023415 -0.5327665  -0.47502068 -0.7001805\\n  0.4679802  -0.8758695  -0.71186996 -0.5729502   0.57407194  0.45079437\\n  0.9999994  -0.89994705 -0.92391646 -0.7015908  -0.5151678   0.47874728\\n -0.38828567 -1.          0.24616443 -0.83933985  0.88006514 -0.52938104\\n  0.9278145  -0.5489592  -0.6934014  -0.44758013  0.745147    0.85885894\\n -0.6171682  -0.46239278  0.52489513 -0.2205532   0.9895229   0.43591252\\n -0.651823   -0.42680535  0.63059145 -0.94943786 -0.6351087   0.33381972]\",\n          \"[-0.5798742  -0.4025664  -0.87254274  0.49014562  0.4796953  -0.08944809\\n  0.12567867  0.1066692  -0.5181874  -0.9998403  -0.09427854  0.8979662\\n  0.9389744   0.5049391   0.6397983  -0.48475778  0.07316053 -0.4156192\\n  0.36005324  0.6601536   0.3775265   0.99994415  0.09520797  0.24179903\\n  0.3986318   0.8730233  -0.695463    0.49582222  0.832207    0.56686985\\n -0.34940538  0.32201317 -0.9594488  -0.11103261 -0.9310923  -0.9709965\\n  0.3202294  -0.31438556  0.00456666  0.06560486 -0.41429752  0.30075458\\n  0.99978995 -0.06539086  0.31817162 -0.22615685 -0.9999814   0.18395676\\n -0.5235523   0.6760804   0.6780928   0.81045395  0.16981134  0.32046768\\n  0.542882   -0.28398156 -0.07921897  0.16937795 -0.29341316 -0.37225983\\n -0.5853146   0.5445678  -0.8209473  -0.69164     0.72741365  0.71330994\\n -0.27336186 -0.34141555  0.04688039 -0.16348971  0.43915698  0.25583544\\n -0.38641497 -0.48594204  0.38254195  0.18904004 -0.57742614  1.\\n -0.24710253 -0.8743989   0.8554809   0.6971835   0.48355228 -0.09853236\\n  0.4639121  -1.          0.41295755 -0.12143457 -0.9367435   0.27920192\\n  0.3782891  -0.11504409  0.8569736   0.5217867  -0.3694167  -0.38227913\\n -0.22603628 -0.7334514  -0.31122926 -0.4169546   0.2413295  -0.28186905\\n -0.3142031  -0.4533666   0.15655673 -0.38036022  0.00915665  0.5202341\\n  0.1371775   0.5532048   0.48771727 -0.27128646  0.18383706 -0.81380826\\n  0.58196944 -0.26744226 -0.9205378  -0.47876677 -0.9312472   0.5674109\\n -0.21511671 -0.19693656  0.6072667  -0.55333734  0.33221912 -0.08179791\\n -0.675051   -1.         -0.5008268  -0.5097     -0.2438197  -0.24478975\\n -0.89785665 -0.82338864  0.5215509   0.65363014  0.11357547  0.99874866\\n -0.1859053   0.8188903  -0.19552249 -0.56108755  0.1528772  -0.41559497\\n  0.70250535 -0.08814512 -0.14878868  0.12983531 -0.46412182  0.32133016\\n -0.61968344 -0.15827473 -0.5485727  -0.5294215  -0.29814616  0.79145855\\n -0.16338415 -0.8424801   0.11193357 -0.13031383 -0.1434978   0.61086005\\n  0.53643477  0.35385922 -0.30748463  0.41498506  0.1025731   0.40328142\\n -0.5795762   0.03270654  0.3930753  -0.34732425 -0.8449278  -0.9353515\\n -0.29059333  0.15856963  0.8921006   0.4767611   0.3226361   0.44812346\\n -0.24599063  0.5971545  -0.907802    0.90567946  0.03162766  0.21593975\\n -0.57156265  0.49608496 -0.3231245   0.16269846  0.56479657 -0.4585248\\n -0.32993084 -0.05477699 -0.5110515  -0.44045165 -0.6617394  -0.08470071\\n -0.32085422 -0.42627388 -0.10511805  0.6143709   0.66587615  0.31759992\\n  0.40634796  0.31447965 -0.67276984 -0.3065019   0.19816364  0.19508427\\n  0.19719294  0.9254532  -0.76985705  0.12377844 -0.3932102  -0.9309743\\n -0.05730033 -0.43805137 -0.11250976 -0.5342909   0.5952284  -0.16739656\\n  0.18381043  0.2707297  -0.08224343 -0.41413152  0.23486653 -0.41303363\\n  0.32520118 -0.11724908  0.7379688   0.8807951  -0.5289205  -0.27106544\\n  0.89129627 -0.8455277  -0.29641637  0.13781822 -0.15053141  0.64874536\\n -0.589841    0.9510209   0.808219    0.39934865 -0.718355   -0.7538071\\n -0.21555224 -0.50929284 -0.09406198  0.04359817  0.70269513  0.52207\\n  0.31034467  0.42806143 -0.4611034   0.684255   -0.6300523  -0.8494526\\n -0.685049   -0.26196477 -0.95297354  0.67259127  0.18949044  0.38096675\\n -0.3605266  -0.42922574 -0.76722246  0.01122392  0.0014467   0.6631889\\n -0.45567188 -0.6158836  -0.41631073 -0.7519986  -0.01983349 -0.20395066\\n -0.15786098 -0.03450535 -0.47530955  0.37132648  0.2760612   0.2525324\\n -0.67028147  0.9233956   0.99999803  0.9011024   0.5392105   0.29083776\\n -0.9992818  -0.41496855  0.99996614 -0.9613696  -0.9999996  -0.5843413\\n -0.5597525   0.06429989 -1.         -0.10049599  0.09282669 -0.53501457\\n  0.2915883   0.8883669   0.56869334 -1.          0.42486402  0.46005547\\n -0.592652    0.79348195 -0.42249864  0.8889952   0.22553073  0.39976248\\n -0.2714882   0.50191617 -0.85015655 -0.5553589  -0.49659726 -0.59336156\\n  0.995432    0.13107601 -0.676455   -0.59021944  0.45987886  0.17989305\\n -0.19330569 -0.81291234 -0.24803814  0.20039825  0.5271531   0.09494512\\n  0.36739504 -0.24808569  0.22806567  0.09654768 -0.13279587  0.5365551\\n -0.8117386   0.02812824  0.33516544  0.00862786 -0.5736042  -0.9004704\\n  0.8467311  -0.21515848  0.75542384  1.          0.48817927 -0.23711354\\n  0.5923223   0.19097452 -0.57722014  1.          0.7510978  -0.90004796\\n -0.61642003  0.62896794 -0.4659687  -0.50928587  0.99756765 -0.21542063\\n -0.50244844 -0.17169768  0.9525131  -0.96267676  0.9895521  -0.5616537\\n -0.7927401   0.73741645  0.59251887 -0.42556092 -0.34197715  0.16575722\\n -0.68060696  0.2721793  -0.5422858   0.5651059   0.19291934 -0.02138037\\n  0.44464877  0.01030052 -0.5588882   0.37935328 -0.3686008  -0.19333938\\n  0.88891387  0.324887   -0.1502345   0.11946087 -0.22676879 -0.35894498\\n -0.8002688   0.51124394  1.         -0.29477325  0.46558222 -0.396457\\n  0.05927688 -0.01428137  0.40638232  0.37914652 -0.12850215 -0.40782216\\n  0.6206977  -0.5228732  -0.9741984   0.03574739  0.11882722 -0.14152303\\n  0.9995452   0.702195    0.30758938  0.5237264   0.8728439   0.24272077\\n -0.09485005  0.7634498   0.8826766  -0.32688114  0.53627145 -0.07729441\\n -0.75688934 -0.23295489 -0.5514036   0.11412822 -0.82743835 -0.01251442\\n -0.7482599   0.78152484  0.7513149   0.20187147  0.33014223  0.8162041\\n  1.         -0.76562124  0.2598554   0.5984223   0.10167649 -0.9990057\\n -0.21130985 -0.39395118 -0.02164614 -0.6977892  -0.22507887  0.07669524\\n -0.8275326   0.68803674  0.46955523 -0.6247679  -0.9226419  -0.59563833\\n  0.46385938  0.02700361 -0.946833   -0.3982984  -0.28347224  0.36412376\\n -0.23010519 -0.5867029  -0.22495012 -0.1420032   0.22792622 -0.1442016\\n  0.6422258   0.7459251   0.6576247  -0.73882437 -0.30237123 -0.19462723\\n -0.58392465  0.23824826 -0.48058102 -0.7559322  -0.24682325  1.\\n -0.37805578  0.7424215   0.06671605  0.25590324 -0.3564603   0.12802473\\n  0.9116383   0.307593   -0.5779081  -0.5460451   0.7045373  -0.40574393\\n  0.71324134  0.35629308  0.64495575  0.24560693  0.712805    0.27252293\\n -0.10970308  0.0747764   0.86766195 -0.20297542 -0.20062633 -0.25702554\\n -0.17430097 -0.3759171   0.39418095  1.          0.17466363  0.552254\\n -0.97477746 -0.75366026 -0.6407914   0.9999944   0.512573   -0.29185078\\n  0.47153208  0.39759982 -0.19095261  0.08671333 -0.18693258 -0.27446344\\n  0.21383722 -0.02581694  0.8614573  -0.37099826 -0.86539656 -0.5228935\\n  0.27148345 -0.73022544  0.9997793  -0.54985946 -0.29709584 -0.12793641\\n -0.26151308 -0.8166052  -0.00210138 -0.8682939  -0.21303195  0.06630742\\n  0.75492007  0.1197754  -0.5600324  -0.6654418   0.5163421   0.4265654\\n -0.49955437 -0.7639638   0.85765725 -0.78103     0.38639027  0.9999981\\n  0.4308175   0.12156209  0.26250193 -0.11412848  0.23512043 -0.1514412\\n  0.24254557 -0.5613916  -0.24133447 -0.24220876  0.35192165 -0.13511218\\n -0.8951796   0.3158659   0.30208743 -0.47744548 -0.59885514 -0.1168911\\n  0.37020132  0.7047657  -0.26978537 -0.11972325  0.01892266  0.07324255\\n -0.6801321  -0.3846971  -0.31483915 -0.99993104  0.31105977 -1.\\n  0.26037222  0.18983054 -0.25490376  0.50088435  0.44840455  0.5657805\\n  0.04899373 -0.66122985  0.4832916   0.5759069  -0.15870035 -0.11915196\\n -0.1797614   0.25123703 -0.12383526  0.21588497 -0.6029115   0.51598275\\n -0.06565481  1.          0.10238963 -0.44745818 -0.27467182  0.36990097\\n -0.16679972  0.99999934 -0.09143256 -0.81890285  0.42938122 -0.70647407\\n -0.43222854  0.26275977 -0.02348746 -0.6863466  -0.8558343   0.4812226\\n  0.1461826  -0.65610254  0.4090457  -0.22397038 -0.41662267  0.05448176\\n  0.6906789   0.9156104   0.12449274  0.38163227 -0.6458063  -0.39596835\\n  0.79733163  0.3349082  -0.41455707  0.10430284  0.9999997   0.2090225\\n -0.79020256  0.05509982 -0.6043375  -0.32864562 -0.6234248   0.31655252\\n  0.20215186  0.6591162  -0.11768805  0.7676388  -0.6363148   0.0489098\\n -0.22230197 -0.49898368  0.39083868 -0.58454365 -0.9442339  -0.94916016\\n  0.6292631  -0.4155515   0.01221633  0.38102975  0.21367012  0.29659638\\n  0.2736086  -1.          0.8221302   0.3139051   0.74717164  0.74136525\\n  0.4344548   0.476681    0.3176168  -0.8918716   0.03412817 -0.34313515\\n -0.22066598  0.3983826   0.58945894  0.53340846  0.3327555  -0.21710335\\n -0.46935073 -0.60473293 -0.6351562  -0.952573    0.44188225 -0.647926\\n -0.21356653  0.82839257  0.19594233 -0.07001626 -0.15289634 -0.78709584\\n  0.2945468   0.39568037 -0.0124751   0.08078618  0.16152236  0.3966966\\n  0.70371056  0.9713222  -0.83833325  0.4973096  -0.35538706  0.3269934\\n  0.70949733 -0.81841516  0.23216438  0.42206016 -0.29091313  0.3062654\\n -0.31805024 -0.39621112  0.73862153 -0.44504452  0.23856564 -0.27263698\\n  0.01020001 -0.33228213 -0.14122732 -0.40260425 -0.5043862   0.6450397\\n  0.08948566  0.6544394   0.79115117 -0.05981042 -0.4979233  -0.15717953\\n -0.53333426 -0.79546386  0.09431553 -0.02685333 -0.43660864  0.30266866\\n -0.17009878  0.9237829   0.40773892 -0.39022458 -0.20389287 -0.5695476\\n  0.4126581  -0.57145697 -0.34040827 -0.5073468   0.49786204  0.27764916\\n  0.9999181  -0.5161013  -0.78237355 -0.47310275 -0.29828998  0.41989905\\n -0.32817113 -1.          0.06366344 -0.5829356   0.7113667  -0.26837486\\n  0.6532191  -0.38618585 -0.5061309  -0.23365617  0.33062762  0.3598985\\n -0.42223808 -0.38927403  0.44271705 -0.25561872  0.6759151   0.43622044\\n -0.4973024   0.04946733  0.6390014  -0.6938332  -0.59539676  0.24556871]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "yIO5Bk7f1b1K",
        "outputId": "f7dac82f-c699-4c92-aad7-b2590ad9a0a7"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label  \\\n",
              "0  Education is undoubtedly an Vital part of life...      1   \n",
              "1  I thick students would benefit doing school at...      0   \n",
              "2  Title: The Pros ANP Cons of a New School Sport...      1   \n",
              "3  Success IU life comes not from idling, but fro...      1   \n",
              "4  For driverless cars they can have a PLCs side ...      0   \n",
              "\n",
              "                                         SpacyVector  \\\n",
              "0  [-1.3600676   0.7374259  -1.8761181   0.089783...   \n",
              "1  [-1.24247360e+00  1.91180539e+00 -3.68909240e+...   \n",
              "2  [-1.7198364   1.1876675  -1.9843843   0.510850...   \n",
              "3  [-8.82074714e-01  1.28694320e+00 -2.69012070e+...   \n",
              "4  [-1.5124509   2.3015475  -3.4847832   0.049697...   \n",
              "\n",
              "                                          BertVector  \n",
              "0  [-7.9001939e-01 -6.7574787e-01 -9.9017417e-01 ...  \n",
              "1  [-0.26419863 -0.65571606 -0.98305005  0.384800...  \n",
              "2  [-0.78946614 -0.6624161  -0.9927442   0.741319...  \n",
              "3  [-6.10299885e-01 -4.07532245e-01 -9.04479682e-...  \n",
              "4  [-0.7148344  -0.49521807 -0.9724247   0.622209...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e6089c89-cd95-4078-935f-d851ca49c439\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>SpacyVector</th>\n",
              "      <th>BertVector</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Education is undoubtedly an Vital part of life...</td>\n",
              "      <td>1</td>\n",
              "      <td>[-1.3600676   0.7374259  -1.8761181   0.089783...</td>\n",
              "      <td>[-7.9001939e-01 -6.7574787e-01 -9.9017417e-01 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I thick students would benefit doing school at...</td>\n",
              "      <td>0</td>\n",
              "      <td>[-1.24247360e+00  1.91180539e+00 -3.68909240e+...</td>\n",
              "      <td>[-0.26419863 -0.65571606 -0.98305005  0.384800...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Title: The Pros ANP Cons of a New School Sport...</td>\n",
              "      <td>1</td>\n",
              "      <td>[-1.7198364   1.1876675  -1.9843843   0.510850...</td>\n",
              "      <td>[-0.78946614 -0.6624161  -0.9927442   0.741319...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Success IU life comes not from idling, but fro...</td>\n",
              "      <td>1</td>\n",
              "      <td>[-8.82074714e-01  1.28694320e+00 -2.69012070e+...</td>\n",
              "      <td>[-6.10299885e-01 -4.07532245e-01 -9.04479682e-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>For driverless cars they can have a PLCs side ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[-1.5124509   2.3015475  -3.4847832   0.049697...</td>\n",
              "      <td>[-0.7148344  -0.49521807 -0.9724247   0.622209...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e6089c89-cd95-4078-935f-d851ca49c439')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e6089c89-cd95-4078-935f-d851ca49c439 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e6089c89-cd95-4078-935f-d851ca49c439');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-50f772e3-45c7-44c9-80c4-a20b8a319c45\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-50f772e3-45c7-44c9-80c4-a20b8a319c45')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-50f772e3-45c7-44c9-80c4-a20b8a319c45 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test",
              "summary": "{\n  \"name\": \"test\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          \"Dear Principle,\\n\\nI think it is not a wise decision to pass a policy where you cannot play sports without a B average. The reason so is that not everyone can surpass academically, but many can physically. Just because you don't excel in academics, doesn't mean you can't graduate either. Sure you are required to get good grades to even graduate or go to college, but you can do that along with practicing in sports. But not just sports but other activities as well.\\n\\nJust getting good grades in classes generally can take you far in your career, but what kind of careers would you be able to choose with a limited amount of knowledge just from regular classes? When kids choose activities they think they would enjoy, it opens up more experience and knowledge for future career choices. If a kid wants to do the school newspaper, he/she could grow up to be a journalist or a reporter for the news. Or if a boy wants to be an actor, he could join the drama club.\\n\\nThere are many brilliant futures to many Kids with different types of classes and the knowledge given to them. There are many places in society where they feel like they belong.\\n\\nTaking special classes will deprive them of this possible future. We kids are the next generation to run things in society, don't you think it's only fair that you can give us our own chance to succeed with our own special skills in life?\\n\\nBut along with careers, kids just want to have fun in their classes. Kids will find school more entertaining to be able to do such activities. They might even become motivated to do better in school if they find school worth coming to. These kinds of options are all about thinking ahead. Like what would happen to the students if I do this? How will it benefit the school? Is this the right decision to make? These questions can make a big difference in the future. Tot to make any offense principle, but it is important to follow such guidelines as those. I don't run the school, but it's always good to have the consent of the student who come to it.\",\n          \"I strongly think that\\n\\nUnmasking The Fate Is A Natural Landform let me tell you how. The article says that the spate Taft was titling the planet snapping photos of possible landing sites for its sister ship. When he spotted human fate enormous head nearly two miles from end to end looking back at the cameras from the planet called Colonia. Other thinks that it is just a resembled of a human head by a shadow.\\n\\nThe author predicts that its two different side that us humans have to figure out. Web surfers were waiting for the image to appear on JPL website revealing a natural landform. Web surfers also tan remake and edit a photo so who will know if It's real or not we would never know. Unmasking is trying to figure out if the fate of the human really real. Mars global surveyor is a mapping spacecraft that normally looks straight down and steins the planet like a fax machine in narrow 2.5 km wide strips. They say they never passed the fate.\\n\\nThey also say that alien markings were hidden by haze. The mission controllers prepared to look again. It's not easy to target tycoon say margin.  \",\n          \"Dear State Senator, we should not keep the Electoral College. I'm Io favor of changing to election by popular vote for the president of the united states. The electoral college is unfair. It's outdated AOD irrational. Many people prefer election by popular votes. Voters should be satisfied with their vote directly towards the president. They should't have to be upset if they choose candidates AOD those candidates choose someone else as president.\\n\\nOne of my reasons is because under the electoral college system, voters vote not for the president, but for the slate of electors, who Io turn elect for the president. The electors can be anyone not holding public office. Depending OO the state, the electors are picked by state conventions, sometimes the state party's central committee, AOD sometimes the presidential candidates themselves. The electoral college ISO't the best way to handle elections because it is the electors who elect the president, not the people, which to me sounds unfair. Voters can't always control who their electors will vote for AOD voters do get confused sometimes about the electors. So if you really agreed OO ewe president, chances are that might not be the president who's Guion be elected because the electors can choose the other person running for president instead.\\n\\nThe electoral college is unfair. The electoral college consists of 538 electors. A majority of 270 elector votes is required to elect the president. Richard Nixon, Jimmy Carter, Bob Dole, the U.S. Chamber of Commerce, AOD the AFL CIO all agreed OO abolishing the electoral act. According to a Gallup poll Io 2000, over sixty percent of voter would prefer a direct election to the kind we have now. This year voters can expect another close election Io which the popular vote wooer could again lose the presidency. Voters dew't wait the popular vote wooer to lose the presidency but because of the electoral college it can happen. Who you vote for a presidential candidate you are actually voting for a slate of electors.\\n\\nLet's say that the state legislatures are technically responsible for picking electors. Those electors can always defy the will of the people. Faithless electors have occasionally refused to vote for their party's candidate AOD cast a deciding vote for whomever they pleased. Why OO earth would they do such a thing. They should care about what voters believe Io. Io 1960, people who favored separation based OO race Io the Louisiana legislature early actually succeeded Io replacing the Democratic electors with new electors so that they would oppose John F. Failed. This means the popular votes would not have actually good to Failed. That is not fair. Candidates dew't speed time Io states they know they have OO chance of viewing. They focus only OO the tight races Io the \\\"swing\\\" states.\\n\\nState Senator, I'm Io favor of changing to election by popular votes because it's fair. The electoral college should stay Io the past AOD should'OT be used anymore. The best way is for the election by popular votes. Voters should be satisfied with their vote directly towards the president. Electors have occasionally refused to vote for their party's candidate AOD decided to vote for whomever they wanted. Selfish much? People should't have to be upset if they choose candidates AOD those candidates choose someone else as president. These electors should consider what the people wait. I believe Elections by popular votes for president of the united states is the best option.  \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SpacyVector\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9991,\n        \"samples\": [\n          \"[-5.75849235e-01  2.48542619e+00 -3.04183578e+00 -9.58367050e-01\\n  1.37712777e+00  1.29404557e+00  4.04028296e-01  3.18414283e+00\\n -2.59611320e+00  4.31805253e-01  5.84739876e+00  1.64514840e+00\\n -4.07937908e+00  9.43821311e-01  9.26747441e-01 -1.47507642e-03\\n  1.55661142e+00 -2.44196248e+00 -1.49114537e+00 -1.31779790e+00\\n  1.12473023e+00 -7.10510314e-01 -1.02684593e+00 -2.14248729e+00\\n -1.27824175e+00 -1.06777191e+00 -1.70648229e+00 -4.80897576e-01\\n -1.24517488e+00  1.45847631e+00  1.83417332e+00 -9.56405401e-01\\n -1.35942709e+00 -1.65905464e+00 -1.92246825e-01  4.60219085e-01\\n -2.50349104e-01  3.93219411e-01  2.96355939e+00  2.17912197e+00\\n  1.37511179e-01  1.88835442e+00  7.46594191e-01 -5.28952122e-01\\n -1.99455154e+00  1.57510912e+00  1.63446772e+00 -4.06589937e+00\\n -1.51659083e+00  1.57976389e+00 -5.26076108e-02  1.27755606e+00\\n  6.20556653e-01 -5.15495396e+00 -2.23376346e+00  1.92216799e-01\\n -4.16460246e-01  2.05483341e+00  9.36582565e-01  7.14234829e-01\\n  2.73390460e+00 -4.87851053e-01 -7.65101671e-01 -1.83697772e+00\\n  1.33359134e+00  1.76873159e+00 -3.02096534e+00 -3.76356673e+00\\n  2.39308238e-01  3.14260197e+00  4.87214094e-03  2.10809112e-01\\n -1.05385721e+00 -3.42084587e-01  1.57496184e-01  1.85130763e+00\\n -2.82359028e+00  1.09014297e+00 -2.52990341e+00 -6.81689382e-01\\n -4.39187431e+00 -2.61476547e-01  2.12100601e+00  5.57541490e-01\\n  1.68714356e+00 -3.98440450e-01 -1.66527653e+00 -1.97161329e+00\\n  1.70911217e+00 -1.23190597e-01 -5.26732087e-01  2.75715470e-01\\n  1.73292172e+00 -4.22742653e+00  1.08180928e+00 -1.22907317e+00\\n  1.45410717e+00 -2.22986674e+00  9.33997631e-02  1.12252021e+00\\n  2.15063548e+00  1.55133259e+00  1.42941725e+00  1.08871651e+00\\n -1.53348815e+00  3.51582003e+00 -1.43450701e+00 -1.76016188e+00\\n -1.58918428e+00 -1.80093849e+00  1.14543366e+00 -9.87908602e-01\\n -2.26896793e-01  2.13361770e-01 -6.07611477e-01  2.44761205e+00\\n -2.20299911e+00 -1.36825728e+00  1.11751938e+00 -1.13582885e+00\\n -2.26459980e+00 -1.27437770e+00 -2.27020645e+00  1.02062213e+00\\n -9.79881883e-01 -2.62778354e+00  1.91305792e+00 -2.23724127e+00\\n  1.28390312e+00 -1.35814583e+00 -1.53104997e+00 -3.21170717e-01\\n  2.98585796e+00 -1.39758217e+00 -1.84184656e-01  7.86762774e-01\\n -2.06509042e+00 -8.31247509e-01  4.06977510e+00 -2.09327984e+00\\n -1.61448717e+00 -8.48389864e-01  9.90704656e-01  1.21813142e+00\\n -8.34521651e-01 -6.31067514e-01 -2.59145260e+00 -1.23285204e-01\\n -1.36987105e-01  2.60404795e-01 -1.07869458e+00  2.38579226e+00\\n  1.96918681e-01  3.95936519e-01 -8.47995818e-01  8.76024961e-01\\n  4.28309011e+00  5.74717283e-01 -1.46589971e+00 -1.65904987e+00\\n -1.73336816e+00 -2.27957177e+00 -1.77803445e+00  1.44545746e+00\\n -2.41585326e+00 -5.22876263e-01 -3.53996825e+00  1.00519061e+00\\n -7.58796871e-01 -2.87630141e-01  1.23885882e+00  9.40052047e-02\\n  1.48674822e+00  8.36948276e-01  2.25313902e+00 -6.01299942e-01\\n -7.97252119e-01  7.49978423e-01 -1.08212245e+00 -2.08086824e+00\\n -5.40287733e-01  1.46999761e-01  2.83828688e+00 -1.21752691e+00\\n -1.59482205e+00  8.82868409e-01 -1.56289375e+00 -3.04071546e+00\\n  1.67599511e+00  2.88901281e+00 -1.12940180e+00  4.65951860e-01\\n -1.03999972e+00 -1.07486415e+00 -3.96746159e-01 -3.77678871e-01\\n -2.85017872e+00  1.48246184e-01 -1.98865384e-01  1.52784073e+00\\n -1.83725822e+00 -1.12526131e+00 -1.32262361e+00 -1.69859064e+00\\n  8.29440892e-01  7.74308562e-01 -2.17549777e+00  9.42246795e-01\\n -1.35453716e-01 -2.73204833e-01  2.41280818e+00 -4.09923255e-01\\n -3.98447871e-01  1.74886191e+00 -5.58678269e-01  2.35395384e+00\\n -1.16860524e-01 -2.64947248e+00 -1.02095413e+00  5.11261761e-01\\n -1.95660603e+00  6.18983269e-01  2.21169561e-01  6.17886364e-01\\n -2.79405024e-02 -2.02367926e+00 -2.98615843e-01  2.04822588e+00\\n  2.17528152e+00  1.32090640e+00 -5.67359149e-01 -2.25951028e+00\\n -4.63479787e-01  5.18880427e-01  1.12932193e+00  1.16324615e+00\\n -1.10396767e+00  1.74264908e+00 -2.01952849e-02  1.25950828e-01\\n -2.56986094e+00  3.06683391e-01  8.98825109e-01  6.35191321e-01\\n -3.21762294e-01  6.19685411e-01 -2.47367430e+00  5.74834824e-01\\n  2.13925934e+00  2.79274321e+00 -3.64605278e-01 -3.09611738e-01\\n -5.07697964e+00  5.34176111e-01  3.41725439e-01 -2.93961787e+00\\n  1.65989470e+00 -1.37698665e-01 -6.25838697e-01  1.24055696e+00\\n -4.50753570e-02  3.90404320e+00  2.53706741e+00  2.47743058e+00\\n  2.73329449e+00 -7.04606101e-02  8.16201448e-01  2.57068062e+00\\n -4.48246717e+00  1.36248037e-01  9.62143958e-01 -1.19026482e+00\\n -7.66760945e-01 -1.50381792e+00 -3.85761321e-01 -2.07286692e+00\\n  1.05093873e+00 -1.52933276e+00 -1.42667270e+00  2.66816235e+00\\n -3.46369445e-01 -8.14917743e-01  2.92053849e-01  5.16267657e-01\\n  2.77151656e+00 -6.58657789e-01  1.75748563e+00  1.68069303e+00\\n -1.96460223e+00 -1.94820374e-01  1.44569385e+00 -9.06449854e-01\\n  7.15855598e-01 -1.04540586e+00 -1.32361019e+00 -5.25763445e-02\\n  4.74310875e-01  1.53891623e-01 -2.86604238e+00  2.26314306e+00]\",\n          \"[-2.7778342e+00  6.2121224e-01 -1.9267410e+00  4.3267244e-01\\n  4.3630967e+00  9.2609799e-01  5.5912513e-01  4.3862300e+00\\n -7.2894460e-01 -1.0149846e+00  6.8359399e+00  1.3779345e+00\\n -3.3126087e+00  1.3508806e+00 -8.0444561e-03  2.3766150e+00\\n  9.0984088e-01 -3.5454148e-01 -2.0141363e+00 -1.5939276e+00\\n  5.4684114e-01 -7.2906917e-01 -1.1493806e+00 -5.5149138e-01\\n  7.4646509e-01 -1.4334904e+00 -1.5273789e+00 -6.2928283e-01\\n -8.7528610e-01  5.5613112e-01  8.0552107e-01 -3.6445698e-01\\n -6.2014711e-01 -2.3592296e+00 -2.2491972e+00 -9.3070734e-01\\n -2.8062394e-01  7.2506988e-01  1.1916127e+00  8.3043832e-01\\n -4.8030272e-01  6.6662574e-01 -2.3941296e-01  1.2542106e+00\\n -1.6395507e+00  8.4768164e-01  1.4756837e+00 -2.0245481e+00\\n -1.0796977e+00  1.9059176e+00 -1.8863181e+00  1.7957711e+00\\n  1.3448972e-01 -4.7943759e+00 -1.0080106e+00  1.3568276e-02\\n  5.9346408e-01  1.3473902e+00  1.1703454e+00 -5.9348637e-01\\n  7.7905041e-01 -5.7998765e-01  1.4566439e-01 -1.5677648e+00\\n  2.3144088e+00  1.8367671e+00 -2.7759154e+00 -3.1726770e+00\\n  8.4093535e-01  2.8716338e+00  4.3386373e-01  1.8794681e-01\\n -2.4193630e+00  2.6723783e-02 -1.4084674e+00  1.3860837e+00\\n -3.1061215e+00  2.4308319e+00 -3.8552306e+00  3.8806760e-01\\n -4.4443045e+00  9.5747858e-02  1.3536038e+00  2.9096013e-01\\n  1.7473071e+00 -6.5513946e-02 -2.6780291e+00 -2.6132305e+00\\n  1.7947475e+00 -4.7198465e-01 -1.3108130e+00  4.7484688e-02\\n  1.6521255e+00 -3.7206850e+00  1.4665852e+00 -1.6352036e+00\\n  3.1311759e-01 -7.9401714e-01  4.2015335e-01  2.0028548e+00\\n  2.8769641e+00  1.4559093e+00  2.1440732e+00  3.1508374e+00\\n -1.2234200e+00  4.2151999e+00  3.9623451e-01 -2.0885665e+00\\n -5.7542253e-01 -2.6945362e+00  1.0907915e+00  9.4757903e-01\\n -1.3569307e+00  8.0485666e-01  5.4104370e-01  5.9234333e-01\\n -8.2683939e-01 -3.0518305e-02 -7.6926738e-02 -4.5477623e-01\\n -4.6019948e-01 -2.4440339e+00  5.6992805e-01  1.1731633e+00\\n -1.6451761e+00 -3.4278700e+00  1.1267291e+00 -2.2928991e+00\\n  2.2524707e+00 -1.2487067e+00 -2.7599335e+00 -7.0558482e-01\\n  4.0869999e+00 -7.2617340e-01 -2.2186229e-01  1.2492661e+00\\n -2.2367175e+00 -8.7660420e-01  2.5577626e+00 -2.0051193e+00\\n -2.4755299e+00 -1.0410951e+00  1.2922682e+00  1.8296833e+00\\n  5.8638424e-01  8.3720690e-01 -2.9173753e+00 -1.1125301e+00\\n  5.1857930e-01  8.2753891e-01 -1.4320187e-01  2.9511046e+00\\n -1.4408728e-01  1.6585965e+00 -1.0159010e+00  1.5254333e+00\\n  3.0313456e+00 -9.1376942e-01 -2.9640949e+00 -1.2764635e+00\\n -8.3144271e-01 -2.0632167e+00  7.5661205e-02  2.1108570e+00\\n -2.4468701e+00 -1.8671937e+00 -3.7925458e+00  1.5604371e+00\\n -2.4678697e-01  4.3987283e-01  9.0936792e-01 -6.9623250e-01\\n  2.1803370e+00  7.7496392e-01  1.2605357e+00  3.4036708e-01\\n -4.5758042e-01 -2.8461960e-01 -2.5075359e+00 -1.3700434e+00\\n -9.7944313e-01 -4.3578181e-01  1.8181272e+00 -2.9905224e-01\\n -1.2037612e+00  8.7765354e-01 -1.7417523e+00 -5.8496678e-01\\n  1.9813261e+00  3.1016591e+00 -6.2596637e-01 -1.4832727e+00\\n -4.1250294e-01 -9.7176886e-01 -3.9758533e-01  6.2920809e-01\\n -3.4029193e+00 -5.4290462e-01 -1.6275160e-01  1.1615617e+00\\n -1.8997325e+00 -1.6305931e+00 -6.8197441e-01 -1.5622109e+00\\n  2.9366660e+00  1.0330536e+00 -2.8152378e+00  1.2971169e+00\\n  1.0045145e-01 -1.6120838e-01  1.2068169e+00  8.0046546e-01\\n -1.9252253e+00  2.4204364e+00  6.5102524e-01  2.3327005e+00\\n  8.7121654e-01 -1.9496797e+00 -2.8874695e-01  5.1792216e-01\\n -2.0239196e+00  1.6228880e+00  8.0645747e-02  5.4711092e-01\\n -1.8568553e+00 -1.6566529e+00  2.4291715e-01  2.2552464e+00\\n  1.3398952e+00  5.5033219e-01  1.8934616e+00 -3.6636941e+00\\n -1.0035713e+00  1.7463439e+00  1.7310847e+00  1.3114496e+00\\n -9.3114477e-01  8.8402104e-01 -6.4970225e-01 -5.9935462e-01\\n -1.5458752e-01  3.5570112e-01  1.4955777e+00  7.7759874e-01\\n -1.0764034e+00  1.1286227e+00 -2.6937604e+00  2.6494044e-01\\n  1.5042031e+00  2.3248341e+00  4.5245215e-01 -2.6727054e+00\\n -5.2803750e+00 -5.4781187e-01  7.7172792e-01 -2.7622106e+00\\n  1.9706244e+00 -5.9482622e-01  1.6797116e-01  8.4144628e-01\\n -1.5495385e-01  4.5079074e+00  3.0208621e+00  2.5100198e+00\\n  1.9384114e+00 -2.0997262e-01  4.4377965e-01  2.4394965e+00\\n -4.3420453e+00  1.8357104e-01  3.6614713e-01  4.2903714e-02\\n  7.1419960e-01 -2.1542454e+00  3.8805971e-01 -6.8189494e-02\\n  1.5666095e+00 -1.3793913e+00 -1.3703760e+00  2.3027813e+00\\n  4.8208618e-01 -1.3060501e-01  4.6491471e-01  1.1478925e+00\\n  3.3135269e+00 -1.9503363e+00  1.8054048e+00  2.3122940e+00\\n -1.3489916e+00  2.9439952e-03  1.0016890e+00 -2.4375062e-02\\n -7.1391219e-01  1.1161697e+00 -1.2433640e+00  1.7247325e+00\\n  3.4904206e-01 -1.1052915e+00 -2.5817118e+00  1.4101542e+00]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BertVector\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9783,\n        \"samples\": [\n          \"[-5.84655166e-01 -5.93283296e-01 -9.91583586e-01  3.07402760e-01\\n  8.98329258e-01 -5.04252613e-01 -5.78145146e-01  2.23878160e-01\\n -9.60830152e-01 -9.99269366e-01 -7.94682145e-01  9.89187241e-01\\n  9.03909326e-01  8.20069432e-01  8.94208848e-02 -4.05000389e-01\\n -1.45917475e-01 -5.51055849e-01  4.86202061e-01  9.59021270e-01\\n  4.43787545e-01  1.00000000e+00 -7.47910500e-01  5.26212156e-01\\n  5.47363043e-01  9.95168030e-01 -8.13210428e-01  4.71950769e-01\\n  6.08511806e-01  4.89041358e-01  1.50310829e-01  3.92008662e-01\\n -9.76032257e-01 -3.02706808e-01 -9.92713749e-01 -9.54901218e-01\\n  6.73899412e-01  1.05071897e-02  7.41164088e-02 -1.63143381e-01\\n -2.91173100e-01  3.69011998e-01  9.99918103e-01 -8.02723706e-01\\n  7.67642796e-01 -3.76040012e-01 -9.99998748e-01  4.46115434e-01\\n -3.15276951e-01  9.82822120e-01  9.64783847e-01  9.93394613e-01\\n  3.27170253e-01  5.18107355e-01  5.83499789e-01 -8.53549421e-01\\n -1.66880637e-02  2.67861873e-01 -4.38648194e-01 -4.88565445e-01\\n -6.11692727e-01  5.44335306e-01 -9.22222674e-01 -5.04831791e-01\\n  9.83386636e-01  9.85736907e-01 -5.67892432e-01 -3.72415513e-01\\n -2.86417603e-01  7.10242540e-02  5.69750130e-01  4.77956057e-01\\n -6.58365130e-01 -7.01812565e-01  9.29158449e-01  3.43961716e-01\\n -6.42102242e-01  1.00000000e+00 -2.33976886e-01 -8.66752923e-01\\n  9.87098038e-01  9.46135461e-01  4.73253787e-01 -7.93517709e-01\\n  8.53583932e-01 -1.00000000e+00  5.76844871e-01 -3.19230646e-01\\n -9.14756000e-01  3.94436836e-01  6.62969351e-01 -5.11124611e-01\\n  9.91491020e-01  6.55224800e-01 -6.70814633e-01 -7.86612511e-01\\n -2.91896313e-01 -9.71643865e-01 -4.72058922e-01 -7.38199592e-01\\n  4.39961225e-01 -4.89806831e-01 -6.41462088e-01 -4.80346918e-01\\n  5.98304987e-01 -6.88311100e-01  3.71449113e-01  8.01949441e-01\\n  7.67828166e-01  6.46734238e-01  4.77081537e-01 -4.40161556e-01\\n  5.87474704e-01 -7.01527774e-01  6.41417265e-01 -4.86104995e-01\\n -9.55076098e-01 -6.15039289e-01 -9.43408191e-01  4.59070116e-01\\n -5.17464519e-01 -4.94016498e-01  3.23672652e-01 -9.34534192e-01\\n  6.39184237e-01 -5.46619833e-01 -9.88719344e-01 -1.00000000e+00\\n -7.16804266e-01 -6.51537359e-01 -5.70917368e-01 -4.40053284e-01\\n -8.88411760e-01 -8.89796019e-01  6.41555369e-01  6.68760896e-01\\n  4.32825327e-01  9.99291837e-01 -3.77552271e-01  6.28843606e-01\\n -6.47221684e-01 -9.44153488e-01  8.89041007e-01 -4.89433408e-01\\n  8.97335231e-01 -6.61686242e-01  5.39352238e-01  3.04912329e-01\\n -7.84955680e-01  4.73440647e-01 -7.99401999e-01 -3.75094414e-01\\n -9.59633470e-01 -2.52436757e-01 -4.17317986e-01  6.16167903e-01\\n -7.68615067e-01 -9.93426800e-01 -7.32178867e-01 -1.67975441e-01\\n -1.61415085e-01  1.47467405e-01  8.74983311e-01  4.22742754e-01\\n -7.22634196e-01  6.21174574e-01  4.60736454e-02  3.79496545e-01\\n -2.69508988e-01 -5.64262629e-01  4.34187740e-01 -5.72246730e-01\\n -9.82745469e-01 -9.13101256e-01 -4.86934304e-01  3.32458049e-01\\n  8.93071890e-01  1.45781651e-01  4.62724119e-01  9.28427219e-01\\n -4.87074554e-01  7.51023650e-01 -8.48885834e-01  9.36808109e-01\\n -2.47050345e-01  3.43798429e-01 -9.06740367e-01  7.94561923e-01\\n -5.34503013e-02  6.80500448e-01  3.37415040e-01 -8.72315109e-01\\n -3.19671780e-01 -1.78144291e-01 -6.81592941e-01 -4.64415044e-01\\n -9.52586412e-01  1.00254685e-01 -4.31127876e-01 -3.79849821e-01\\n -3.51545632e-01  7.11523294e-01 -1.93188135e-02 -1.98774174e-01\\n  9.11701381e-01  6.19222045e-01 -1.40794650e-01  1.11402586e-01\\n  4.11392450e-01  2.91304618e-01  3.20780367e-01  9.01171029e-01\\n -9.61611629e-01 -1.09475674e-02 -4.85029221e-01 -8.53727877e-01\\n  1.27680469e-02 -3.95040363e-01 -4.57465261e-01 -5.11848509e-01\\n  8.65201414e-01 -9.18503046e-01  7.11580336e-01  3.94084156e-01\\n  9.56204295e-01 -4.66710418e-01  4.42324281e-01 -6.26952052e-01\\n  6.52078927e-01 -4.95498210e-01  9.96547341e-01  9.80100453e-01\\n -5.67340374e-01 -8.30048442e-01  9.52364564e-01 -9.92883444e-01\\n -6.48841023e-01 -8.32170665e-01 -4.97411489e-01  2.76722699e-01\\n -5.90892673e-01  8.52429390e-01  9.70146060e-01  7.55894125e-01\\n -2.07643822e-01 -9.62106466e-01  2.32254893e-01 -8.44740748e-01\\n -4.10964578e-01  6.03865504e-01  9.85930800e-01  6.94984496e-01\\n  6.84531152e-01  5.13672046e-02 -4.13945049e-01 -7.35022798e-02\\n -9.97756779e-01 -8.00867915e-01 -9.44790721e-01 -4.37958896e-01\\n -9.59813654e-01  8.81469369e-01  4.44396526e-01  9.20338392e-01\\n -6.08869016e-01 -5.01630723e-01 -7.74706304e-01 -6.81946397e-01\\n  3.22032332e-01 -5.43868423e-01 -8.84456694e-01 -1.20235914e-02\\n -7.81359732e-01 -7.96389163e-01  2.27823444e-02 -3.62742662e-01\\n -9.06754613e-01  2.96072155e-01 -3.55669022e-01  6.44385278e-01\\n  4.72781867e-01  4.97318506e-01 -9.87299263e-01  8.72904301e-01\\n  1.00000000e+00  8.81285965e-01  4.07426298e-01 -4.16914493e-01\\n -9.99997556e-01 -9.64417279e-01  9.99804080e-01 -9.99280751e-01\\n -1.00000000e+00 -3.04976612e-01 -5.29300809e-01 -5.93454130e-02\\n -1.00000000e+00 -4.95129228e-01 -7.77707174e-02 -2.69733071e-01\\n  9.03772891e-01  7.92877257e-01 -7.03113433e-03 -1.00000000e+00\\n  2.95756549e-01  5.81581473e-01 -5.13960242e-01  9.77071345e-01\\n -6.35964751e-01  8.63350034e-01  6.14299059e-01  8.35896671e-01\\n -3.19807157e-02  5.26292205e-01 -9.93480802e-01 -2.32945442e-01\\n -9.38326061e-01 -9.63670492e-01  9.99971569e-01  3.95697564e-01\\n -6.60599470e-01 -1.32077247e-01  8.98095667e-01 -3.48618448e-01\\n  1.40918558e-02 -8.30560029e-01 -5.91471255e-01  7.20209002e-01\\n  6.22249782e-01  4.71457273e-01  3.88157785e-01 -9.94356498e-02\\n  4.68146205e-01  7.22528517e-01 -5.03797412e-01  5.08286536e-01\\n -6.09500051e-01  4.23191845e-01  7.59308517e-01  4.97787356e-01\\n -9.09090102e-01 -9.25075471e-01  7.46691585e-01 -3.41473699e-01\\n  9.15158987e-01  1.00000000e+00  7.77701974e-01 -7.37424707e-04\\n  7.03671873e-01  3.47747415e-01 -1.54630780e-01  1.00000000e+00\\n  8.89821708e-01 -9.14941072e-01 -6.22791886e-01  8.01853240e-01\\n -7.67141104e-01 -8.14723730e-01  9.92688179e-01 -3.07312518e-01\\n -9.71765757e-01 -8.32404792e-01  9.54056859e-01 -9.59989667e-01\\n  9.99831378e-01 -3.77657041e-02 -8.22996736e-01  6.74741447e-01\\n  5.63721955e-01 -7.53662169e-01  1.40994638e-01  1.05473123e-01\\n -7.77617097e-01  4.63340640e-01  1.46144941e-01  6.05999887e-01\\n  3.97934824e-01 -1.68140024e-01  4.78890628e-01  8.18032742e-01\\n -6.35653436e-01  3.02783579e-01 -8.50996494e-01 -5.03366053e-01\\n  9.81819034e-01  4.68884468e-01 -2.98913538e-01  3.18933338e-01\\n -3.55586946e-01 -9.48360205e-01 -7.99983978e-01  8.63683462e-01\\n  1.00000000e+00 -5.81730783e-01  9.45484817e-01 -6.70880854e-01\\n -3.08021754e-01  3.54431063e-01  6.96894586e-01  7.16027141e-01\\n -3.74939919e-01 -3.62017930e-01  8.69935572e-01  5.30341119e-02\\n -9.75426197e-01 -6.24185324e-01  4.42570269e-01 -1.90877318e-01\\n  9.99947190e-01  7.88693011e-01  3.76826316e-01  6.34011447e-01\\n  9.98090506e-01  4.97000525e-03 -2.87927121e-01  9.90343630e-01\\n  9.35290277e-01 -5.17294168e-01  6.04098201e-01 -4.66345817e-01\\n -9.82740819e-01 -5.19057930e-01 -6.56343043e-01  1.54951826e-01\\n -8.57767582e-01 -9.46639925e-02 -8.11799765e-01  8.21869791e-01\\n  9.86402750e-01  4.95516270e-01  2.97006488e-01  9.56810951e-01\\n  1.00000000e+00 -9.95625198e-01 -3.58140260e-01  9.71780241e-01\\n -8.83119047e-01 -9.99994993e-01  4.10623699e-01 -3.59126180e-01\\n -3.97034973e-01 -9.72295225e-01 -3.94135714e-01  3.82834435e-01\\n -7.07393050e-01  9.74171638e-01  8.97438288e-01  2.28589520e-01\\n -8.15250516e-01 -8.83352399e-01 -6.54538274e-02  3.36271346e-01\\n -9.98553395e-01 -2.54472792e-01 -3.73011380e-01  7.85255969e-01\\n -4.67956245e-01 -4.66420174e-01 -7.45041668e-01 -6.28397703e-01\\n  4.01278704e-01 -4.26736504e-01  5.45000851e-01  9.74934220e-01\\n  6.70744061e-01 -9.84818876e-01 -7.52482593e-01 -3.41295779e-01\\n -3.07072140e-02  5.30350566e-01 -2.92462409e-01 -9.83234882e-01\\n -4.10041451e-01  1.00000000e+00 -1.44550338e-01  9.91117060e-01\\n -2.01766327e-01  7.00346753e-02 -3.39426726e-01  4.52429175e-01\\n  9.85861301e-01  5.48767805e-01 -9.28444862e-01 -9.63285208e-01\\n  9.84877467e-01 -5.62920690e-01  7.09320605e-01  9.46325421e-01\\n  9.15695488e-01  2.15234250e-01  9.64418411e-01  5.21203458e-01\\n -2.27903292e-01  1.91847175e-01  6.85558498e-01 -2.22679764e-01\\n -4.95834231e-01 -3.79987895e-01 -3.73347998e-01 -5.20501137e-01\\n  9.27696109e-01  1.00000000e+00  3.48462433e-01  9.01894033e-01\\n -9.36593056e-01 -9.62478220e-01 -1.02530405e-01  1.00000000e+00\\n  7.33771443e-01  2.15528473e-01  6.80868447e-01  4.17692214e-01\\n -3.33868474e-01 -2.48371571e-01 -4.42747921e-01 -4.28141892e-01\\n  4.19255137e-01  2.44921535e-01  7.41537869e-01 -5.61402917e-01\\n -9.07725513e-01 -4.29998487e-01  4.45596129e-01 -7.65074790e-01\\n  9.99999285e-01 -6.85478091e-01 -4.89190102e-01 -3.11960906e-01\\n -8.39075208e-01 -9.98233318e-01 -1.73080459e-01 -8.53130996e-01\\n -4.55068737e-01  4.52746809e-01  7.27346361e-01  3.74571204e-01\\n -5.63818097e-01 -4.67801750e-01  9.76241231e-01  8.84809971e-01\\n -9.91362453e-01 -6.77852869e-01  7.03920722e-01 -6.22632682e-01\\n  6.81674063e-01  1.00000000e+00  5.94200909e-01  7.16460288e-01\\n  4.17432219e-01 -2.75658101e-01  5.84052980e-01 -8.24856758e-01\\n  3.03482831e-01 -4.32199299e-01 -6.49777949e-01 -3.40215415e-01\\n  4.56784427e-01 -5.03905892e-01 -9.84237373e-01 -1.43217176e-01\\n  4.51365173e-01 -5.10428965e-01 -6.59745216e-01 -3.88761252e-01\\n  6.31796360e-01  5.23188591e-01 -3.57957840e-01 -2.23909467e-01\\n  3.75476450e-01 -2.22271189e-01 -1.20970547e-01 -6.63409352e-01\\n -7.27670491e-01 -1.00000000e+00  2.42733300e-01 -1.00000000e+00\\n  8.69026601e-01  7.05646753e-01 -4.89448756e-01  4.30016279e-01\\n  9.40216720e-01  9.05267835e-01 -5.49313277e-02 -9.91208315e-01\\n -1.11508310e-01  2.23838404e-01 -5.00482202e-01 -5.17397702e-01\\n  2.96266168e-01  5.14569104e-01 -1.79432631e-01  3.57312053e-01\\n -9.24791873e-01  6.11957967e-01 -5.13735771e-01  1.00000000e+00\\n  4.12313104e-01 -7.86316216e-01  6.47231996e-01  4.31436479e-01\\n -4.42304373e-01  1.00000000e+00  4.35376436e-01 -8.73625815e-01\\n  4.69981581e-01 -8.59836578e-01 -3.01599056e-01  5.89606166e-01\\n  3.82101446e-01 -7.58931160e-01 -9.85407233e-01 -3.29568744e-01\\n -7.99414963e-02 -6.77294195e-01  8.51169765e-01 -3.58627886e-01\\n -5.22739530e-01  2.95241982e-01  9.81852710e-01  9.24694121e-01\\n  4.52520043e-01  1.64993331e-02 -9.72019255e-01 -7.90533483e-01\\n  8.28243136e-01  5.95265746e-01 -7.98989356e-01  2.23032057e-01\\n  1.00000000e+00  4.35406268e-01 -5.68677664e-01  1.78490728e-01\\n -1.45258799e-01 -4.60826457e-01 -4.56273794e-01  4.99987572e-01\\n  3.87846708e-01  7.58877516e-01 -4.79516983e-01  7.04038143e-01\\n -9.71331835e-01  2.74849266e-01 -7.11733878e-01 -8.83777261e-01\\n  4.72056150e-01 -5.68154275e-01 -9.22833204e-01 -8.70582879e-01\\n  6.78208888e-01 -5.31143129e-01 -3.47768635e-01  5.94449461e-01\\n  1.76960871e-01  5.05483627e-01  4.67223823e-01 -1.00000000e+00\\n  8.11685503e-01  5.64279735e-01  9.87108290e-01  7.59334564e-01\\n  7.56122708e-01  7.67571628e-01  4.16502118e-01 -8.08527827e-01\\n  6.49538100e-01 -4.18001622e-01 -4.37999964e-01 -1.76014349e-01\\n  7.42842674e-01  3.78622055e-01  3.99739563e-01 -4.89277363e-01\\n -8.31912339e-01 -9.63502109e-01 -9.96040940e-01 -9.49220061e-01\\n  4.66318518e-01 -9.13483977e-01  5.68218887e-01  8.66538525e-01\\n  2.52699226e-01 -3.97817284e-01 -7.32348323e-01 -9.63219523e-01\\n -8.71166170e-01  2.54192613e-02 -4.26788181e-01  2.20941365e-01\\n -3.78050357e-02  8.81120116e-02  6.93588704e-02  8.80849183e-01\\n -9.82747972e-01  7.67925829e-02 -9.60180044e-01  5.00391662e-01\\n  9.85111594e-01 -8.94658923e-01  3.59205246e-01  8.43058109e-01\\n -3.94082397e-01  4.97796386e-01 -4.51450944e-01  8.40763271e-01\\n  8.41249406e-01 -5.33339679e-01  4.64202046e-01 -5.25967777e-01\\n -2.51067817e-01 -5.31477273e-01 -3.31962436e-01 -3.75138760e-01\\n -5.06708682e-01  5.91966331e-01  1.66930839e-01  5.15488982e-01\\n  9.70674336e-01 -2.91914374e-01 -1.79641128e-01 -2.35158488e-01\\n -9.44695473e-01 -6.95397377e-01 -6.30328596e-01 -1.08645149e-01\\n -6.45178974e-01  9.32959497e-01  5.64423688e-02  9.82200980e-01\\n  7.39104509e-01 -4.86763030e-01 -5.48684180e-01 -4.66134161e-01\\n  2.31753364e-01 -9.48977172e-01 -7.52540350e-01 -5.19351184e-01\\n  5.47993124e-01  2.99305379e-01  1.00000000e+00 -9.75409269e-01\\n -9.87289011e-01 -8.72863054e-01 -5.46845555e-01  5.11454463e-01\\n -4.80231315e-01 -1.00000000e+00  3.27119023e-01 -8.48326683e-01\\n  9.21593904e-01 -8.41296315e-01  9.63679731e-01 -6.99844182e-01\\n -6.00907346e-03 -4.44393873e-01  9.49110985e-01  9.54279780e-01\\n -6.33636177e-01 -5.65437973e-01  4.91357028e-01 -6.25786781e-01\\n  9.94702876e-01  1.16995178e-01 -6.66587412e-01 -5.33779562e-01\\n  6.10672235e-01 -9.79260802e-01 -6.42152131e-01  3.85339186e-02]\",\n          \"[-0.8740354  -0.47254553 -0.9566363   0.8287332   0.80180407 -0.36000028\\n  0.51923317  0.40192968 -0.87858725 -0.99997646 -0.36628747  0.93568814\\n  0.9647344   0.6827583   0.8791379  -0.6144346  -0.11868738 -0.5618305\\n  0.5076576   0.40701762  0.67584085  0.99999624 -0.2519042   0.5082235\\n  0.48226288  0.9689722  -0.78223366  0.8966832   0.9020378   0.657889\\n -0.65269     0.49055198 -0.97643155 -0.39847946 -0.98351717 -0.9819556\\n  0.61031073 -0.5742817  -0.12720625 -0.101684   -0.8280643   0.39703226\\n  0.99998474 -0.7220297   0.4637531  -0.35160518 -1.          0.4379632\\n -0.8059365   0.9222015   0.83661395  0.92876774  0.23069994  0.5353035\\n  0.5717469  -0.7313271  -0.10534425  0.12313262 -0.40352303 -0.6480844\\n -0.6145375   0.42192093 -0.8223673  -0.8670453   0.71689785  0.8610837\\n -0.36449823 -0.4038083  -0.29526183  0.052964    0.7854941   0.50998914\\n -0.5937032  -0.8127355   0.7718871   0.44375286 -0.7577852   1.\\n -0.51105845 -0.9321223   0.9647629   0.88418734  0.6762333  -0.65098387\\n  0.7568148  -1.          0.7614615  -0.1761127  -0.9779262   0.39156848\\n  0.6017731  -0.31888542  0.9615276   0.76429373 -0.7166076  -0.70650625\\n -0.3829801  -0.8327099  -0.47887564 -0.49618533  0.32549903 -0.46792772\\n -0.5827948  -0.556686    0.4303333  -0.5730347  -0.02267727  0.72888047\\n  0.4890463   0.73808646  0.66168416 -0.48346213  0.3968536  -0.92636496\\n  0.6797702  -0.41773283 -0.9786287  -0.71714014 -0.9695727   0.6248288\\n -0.03900099 -0.40964192  0.9122585  -0.65296835  0.61622036 -0.4733498\\n -0.9523012  -1.         -0.5561081  -0.37681437 -0.6536161  -0.37218368\\n -0.9424067  -0.9258921   0.62275434  0.8978233   0.3638345   0.9998675\\n -0.44690818  0.87671405 -0.4373252  -0.8825352   0.5384235  -0.530629\\n  0.7823686  -0.3776837  -0.30882937  0.3060952  -0.71995544  0.4373255\\n -0.78575045 -0.38281268 -0.8616527  -0.7536409  -0.5885311   0.9357709\\n -0.76836395 -0.96888655 -0.40807462 -0.33166093 -0.30454117  0.7425605\\n  0.79428697  0.39172864 -0.41011316  0.51670384  0.21637101  0.42317358\\n -0.7005433  -0.43788823  0.5053932  -0.4658955  -0.91352206 -0.9587357\\n -0.3672948   0.51842797  0.9640719   0.51974994  0.38689142  0.7933935\\n -0.47982106  0.6537116  -0.9494286   0.9638352  -0.27185687  0.42085215\\n -0.6140692   0.79576296 -0.68919003  0.34100613  0.7315457  -0.64860415\\n -0.752816   -0.15690476 -0.5794217  -0.54447114 -0.84522635  0.5553579\\n -0.46587276 -0.40053803 -0.15111384  0.85566384  0.9148186   0.552173\\n  0.53099185  0.5752907  -0.8175071  -0.30702698  0.34162313  0.28740177\\n  0.31102836  0.9789638  -0.87268287 -0.15209422 -0.88197106 -0.9646492\\n  0.03641352 -0.7096845  -0.21459448 -0.6503808   0.7282514  -0.7656246\\n  0.50895506  0.36327618 -0.67867196 -0.76237565  0.27544245 -0.5008944\\n  0.52304655 -0.323236    0.8747041   0.96279013 -0.76314396  0.05138829\\n  0.936031   -0.9809473  -0.69196916  0.35884795 -0.42421532  0.8030763\\n -0.6949282   0.98643404  0.9025335   0.6337663  -0.86914635 -0.9313242\\n -0.6549089  -0.74972904 -0.25288457  0.24470957  0.9190535   0.777086\\n  0.5684203   0.15121298 -0.42193976  0.96948975 -0.85713166 -0.9252206\\n -0.75855535 -0.3600327  -0.9730157   0.8085496   0.4111385   0.7084848\\n -0.54139245 -0.6625455  -0.89744085  0.62765336  0.32974142  0.9440023\\n -0.6943582  -0.6166262  -0.7298176  -0.88456404 -0.01128763 -0.29068488\\n -0.5578758   0.15940267 -0.8439102   0.48242944  0.5764245   0.564382\\n -0.9353512   0.99229026  1.          0.9486953   0.785227    0.77019805\\n -0.9999572  -0.81645143  0.9999966  -0.9919031  -1.         -0.86578333\\n -0.7373806   0.33198282 -1.         -0.38131642 -0.05029381 -0.7198686\\n  0.7969923   0.93745357  0.9302874  -1.          0.78614503  0.8926138\\n -0.76361823  0.8410725  -0.6199503   0.9532822   0.69236624  0.7183562\\n -0.34342942  0.5324385  -0.973985   -0.74103713 -0.744908   -0.8875409\\n  0.9995646   0.19972858 -0.85290396 -0.8261805   0.6752615  -0.19058448\\n -0.01731538 -0.9161605  -0.4487447   0.49394807  0.6691015   0.34496346\\n  0.43578914 -0.676106    0.41020203  0.4210728   0.26012275  0.73563504\\n -0.9273454  -0.53349245  0.60034204  0.18813023 -0.84489834 -0.9680189\\n  0.94873655 -0.45569798  0.80997366  1.          0.39367837 -0.7859802\\n  0.7296068   0.33242008 -0.25886336  1.          0.86383975 -0.961169\\n -0.76596385  0.8473237  -0.7119364  -0.7472618   0.9993935  -0.32839668\\n -0.75906175 -0.46149334  0.9630054  -0.97303724  0.9991435  -0.73261744\\n -0.92244065  0.90852827  0.842999   -0.4437131  -0.45926884  0.30294988\\n -0.87958795  0.4742387  -0.84412414  0.7296624   0.45021114 -0.13179004\\n  0.7631489  -0.23126145 -0.72891766  0.2916563  -0.7490027  -0.22149049\\n  0.96550244  0.48869753 -0.38195986  0.1958585  -0.45307463 -0.48039696\\n -0.95773995  0.8053276   1.         -0.44186378  0.6638886  -0.6583668\\n -0.07830103  0.08155884  0.619198    0.6057067  -0.3902514  -0.80833703\\n  0.78990984 -0.664888   -0.97826755  0.5884779   0.34625274 -0.17416961\\n  0.99999356  0.46094754  0.26013613  0.5922662   0.94904935  0.22287004\\n  0.29511797  0.8985257   0.95234364 -0.35476568  0.7357108   0.6481647\\n -0.89276725 -0.35998917 -0.7125309   0.16980883 -0.8196638  -0.1093974\\n -0.8866637   0.9393184   0.9624695   0.49798805  0.3790178   0.8626443\\n  1.         -0.9078356   0.5253434   0.7401212  -0.01363663 -0.999931\\n -0.66311383 -0.42373273 -0.03500776 -0.8306743  -0.37348297  0.41952258\\n -0.9150218   0.88838744  0.7409287  -0.8651302  -0.9640989  -0.51216036\\n  0.6246023   0.2490182  -0.9912665  -0.633821   -0.6271773   0.47222847\\n -0.4606335  -0.79628074 -0.43200624 -0.50250745  0.60235584 -0.46115303\\n  0.7529752   0.91431653  0.7149382  -0.9480201  -0.65159494 -0.23905255\\n -0.8031166   0.6033703  -0.75991726 -0.93880063 -0.298177    1.\\n -0.1232106   0.9412963   0.6479321   0.61771023 -0.45015624  0.21389557\\n  0.9716243   0.4375883  -0.80528885 -0.8310864   0.7349077  -0.5173722\\n  0.71623     0.6591863   0.90843856  0.5648147   0.84036124  0.25108504\\n -0.15783033  0.15899673  0.99321365 -0.2631976  -0.48512718 -0.58233523\\n -0.06787115 -0.5123235   0.28093314  1.          0.43222332  0.6516195\\n -0.97971797 -0.8727082  -0.8686744   1.          0.8394878  -0.6543304\\n  0.71127266  0.56792283 -0.17558375  0.42389846 -0.3766115  -0.3821282\\n  0.35112152  0.32806283  0.9333504  -0.53864247 -0.9371772  -0.6804247\\n  0.43788722 -0.9214611   0.9999831  -0.69910896 -0.3964221  -0.2555588\\n -0.7479656  -0.8138317  -0.0228206  -0.9676699  -0.15286402  0.3140379\\n  0.911013    0.39038548 -0.74969494 -0.9029317   0.88864744  0.72011447\\n -0.9024785  -0.89920855  0.90181905 -0.9753449   0.61260396  1.\\n  0.5486233   0.18359774  0.30293834 -0.4118856   0.41395697 -0.01247895\\n  0.39886713 -0.8653744  -0.49159965 -0.35239515  0.5169104  -0.25489447\\n -0.9184506   0.38180715  0.40059185 -0.70165414 -0.74850667 -0.29692233\\n  0.5801624   0.8178012  -0.45471466 -0.11792254  0.16745609 -0.14651012\\n -0.8871213  -0.43899447 -0.63426805 -0.9999973   0.82552713 -1.\\n  0.818623    0.4849511  -0.39483076  0.6921687   0.73232925  0.8280893\\n -0.48177162 -0.8950537   0.35766953  0.6584341  -0.46702573 -0.2385927\\n -0.42582542  0.4321361  -0.22165129  0.25620586 -0.81479645  0.7269382\\n -0.33475432  1.          0.29601622 -0.6979628  -0.6692782   0.44236788\\n -0.41938865  1.         -0.62487966 -0.9274438   0.42599788 -0.86518073\\n -0.6608464   0.54114467  0.21599494 -0.8493909  -0.92429024  0.89986455\\n  0.78196484 -0.7340725   0.66760933 -0.46765324 -0.5538683   0.16816425\\n  0.92487675  0.9717537   0.66422087  0.6892962  -0.9267169  -0.29200423\\n  0.90508425  0.42369276  0.22869281  0.2150396   1.          0.5916081\\n -0.8545653   0.57213575 -0.8935715  -0.4123943  -0.92642117  0.45141152\\n  0.34084746  0.8762211  -0.36269993  0.94762754 -0.87909573  0.16855603\\n -0.63009226 -0.64922774  0.5381374  -0.9069066  -0.9709085  -0.96152097\\n  0.5096512  -0.47448346 -0.31243655  0.4377281   0.27505383  0.579146\\n  0.5492946  -1.          0.9388225   0.51239103  0.9613074   0.8946914\\n  0.85634476  0.68504745  0.41375142 -0.9499714  -0.76861995 -0.40551656\\n -0.326162    0.5515302   0.70415133  0.71061903  0.35488018 -0.53193176\\n -0.76586163 -0.85694927 -0.8881409  -0.9842977   0.5449112  -0.62137115\\n -0.72302574  0.92424476  0.24172339 -0.1779108  -0.23199335 -0.9276603\\n  0.63932127  0.82685316 -0.03541147  0.19898328  0.46353865  0.7404129\\n  0.8524547   0.96879494 -0.94006133  0.8133059  -0.9090679   0.5932106\\n  0.78853256 -0.92445743  0.23614241  0.81772095 -0.42065537  0.41593218\\n -0.25859636 -0.7110122   0.5396636  -0.43937182  0.60113245 -0.5500285\\n -0.03729206 -0.47638673 -0.27633324 -0.5571194  -0.68766844  0.70303077\\n  0.39144462  0.816797    0.91914153 -0.22960874 -0.50290036 -0.31426203\\n -0.8323799  -0.820735    0.61111724 -0.17956723 -0.6289292   0.7590772\\n  0.08025801  0.94221526  0.22266573 -0.45990208 -0.38406292 -0.73377824\\n  0.8352482  -0.8501257  -0.70917785 -0.5383816   0.66200763  0.40077516\\n  0.9999956  -0.8977218  -0.895633   -0.6108586  -0.5206656   0.5112776\\n -0.40168685 -1.          0.4026309  -0.79755014  0.8361158  -0.42984495\\n  0.8215615  -0.7185242  -0.94855976 -0.34962702  0.6583559   0.7605806\\n -0.5600889  -0.3122686   0.7177787  -0.15809491  0.9678902   0.71344614\\n -0.91672266 -0.29856524  0.7742248  -0.9269657  -0.6368454   0.6006253 ]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split test data set into validation dataset\n"
      ],
      "metadata": {
        "id": "8X6vza5M1c4g"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spacy Models"
      ],
      "metadata": {
        "id": "44DFJuYi3yFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "LnVOpwiP4Rcc"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['SpacyVector'] = train['SpacyVector'].apply(lambda x: np.fromstring(x[1:-1], dtype=float, sep=' '))\n",
        "test['SpacyVector'] = test['SpacyVector'].apply(lambda x: np.fromstring(x[1:-1], dtype=float, sep=' '))"
      ],
      "metadata": {
        "id": "I8OHx24xCE0P"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_2d_spacy = np.stack(train['SpacyVector'].values)\n",
        "X_test_2d_spacy = np.stack(test['SpacyVector'].values)"
      ],
      "metadata": {
        "id": "Oz4p_ZwH3zxy"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train['label']\n",
        "y_test = test['label']"
      ],
      "metadata": {
        "id": "pDimUC-O4WIi"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NAIVE BAYES\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "clf_spacyNB = Pipeline([\n",
        "     ('scaler', MinMaxScaler()),\n",
        "     ('Multi NB', MultinomialNB())\n",
        "])\n",
        "\n",
        "clf_spacyNB.fit(X_train_2d_spacy, y_train)\n",
        "\n",
        "y_pred_nb_spacy = clf_spacyNB.predict(X_test_2d_spacy)\n",
        "\n",
        "print(classification_report(y_test, y_pred_nb_spacy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7BflrK94eOP",
        "outputId": "21622fb7-05cd-41fb-f87f-2fb2ebddbf01"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.38      0.51      5000\n",
            "           1       0.59      0.87      0.70      5000\n",
            "\n",
            "    accuracy                           0.63     10000\n",
            "   macro avg       0.67      0.63      0.60     10000\n",
            "weighted avg       0.67      0.63      0.60     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RANDOM FOREST\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "clf_spacyRF = Pipeline([\n",
        "     ('scaler', StandardScaler()),\n",
        "     ('RF', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "clf_spacyRF.fit(X_train_2d_spacy, y_train)\n",
        "\n",
        "y_pred_rf_spacy = clf_spacyRF.predict(X_test_2d_spacy)\n",
        "\n",
        "print(classification_report(y_test, y_pred_rf_spacy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASE4vnMMCNBh",
        "outputId": "8bbe9d5a-240a-4f2e-a1df-29005b4d9e2c"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.56      0.68      5000\n",
            "           1       0.68      0.92      0.78      5000\n",
            "\n",
            "    accuracy                           0.74     10000\n",
            "   macro avg       0.78      0.74      0.73     10000\n",
            "weighted avg       0.78      0.74      0.73     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LOGISTIC REGRESSION\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf_spacyLR = Pipeline([\n",
        "     ('scaler', StandardScaler()),\n",
        "     ('LR', LogisticRegression())\n",
        "])\n",
        "\n",
        "clf_spacyLR.fit(X_train_2d_spacy, y_train)\n",
        "\n",
        "y_pred_lr_spacy = clf_spacyLR.predict(X_test_2d_spacy)\n",
        "\n",
        "print(classification_report(y_test, y_pred_lr_spacy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcwBrHVkCe2e",
        "outputId": "6a46028d-bbf7-4b87-f899-719cf7825b6f"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.93      0.89      5000\n",
            "           1       0.92      0.84      0.88      5000\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "clf_spacyKNN = Pipeline([\n",
        "     ('scaler', StandardScaler()),\n",
        "     ('KNN', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "clf_spacyKNN.fit(X_train_2d_spacy, y_train)\n",
        "\n",
        "y_pred_knn_spacy = clf_spacyKNN.predict(X_test_2d_spacy)\n",
        "\n",
        "print(classification_report(y_test, y_pred_knn_spacy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzowaFBnDHeR",
        "outputId": "3d2aa35e-1662-43a8-f9e4-e5d769b5001e"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.72      0.79      5000\n",
            "           1       0.76      0.91      0.83      5000\n",
            "\n",
            "    accuracy                           0.81     10000\n",
            "   macro avg       0.83      0.81      0.81     10000\n",
            "weighted avg       0.83      0.81      0.81     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#XGBOOST\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "clf_spacyXGB = Pipeline([\n",
        "     ('scaler', StandardScaler()), #probably don't need to scale here\n",
        "     ('XGB', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
        "])\n",
        "\n",
        "clf_spacyXGB.fit(X_train_2d_spacy, y_train)\n",
        "\n",
        "y_pred_xgb_spacy = clf_spacyXGB.predict(X_test_2d_spacy)\n",
        "\n",
        "print(classification_report(y_test, y_pred_xgb_spacy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFppB4KyDVPg",
        "outputId": "6e3dae49-0b1b-4e7d-e582-461ac5c87654"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.55      0.70      5000\n",
            "           1       0.68      0.98      0.80      5000\n",
            "\n",
            "    accuracy                           0.76     10000\n",
            "   macro avg       0.82      0.76      0.75     10000\n",
            "weighted avg       0.82      0.76      0.75     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_xgb_spacy[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ol27wAXmOdvO",
        "outputId": "4480b5a0-949e-4e96-f646-a7ab28184cb2"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 0, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SUPPORT VECTOR CLASSIFIER\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "clf_spacySVM = Pipeline([\n",
        "     ('scaler', StandardScaler()),\n",
        "     ('SVM', SVC(kernel='rbf'))\n",
        "])\n",
        "\n",
        "clf_spacySVM.fit(X_train_2d_spacy, y_train)\n",
        "\n",
        "y_pred_svm_spacy = clf_spacySVM.predict(X_test_2d_spacy)\n",
        "\n",
        "print(classification_report(y_test, y_pred_svm_spacy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKpnhHgUDeXs",
        "outputId": "1fd79e3f-ecce-4903-8f22-86dce7632376"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.62      0.76      5000\n",
            "           1       0.72      0.99      0.84      5000\n",
            "\n",
            "    accuracy                           0.81     10000\n",
            "   macro avg       0.86      0.81      0.80     10000\n",
            "weighted avg       0.86      0.81      0.80     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RNN\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "bd0IjMxGD03K"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(train['SpacyVector'].tolist())\n",
        "X_test = np.array(test['SpacyVector'].tolist())\n",
        "\n",
        "y_train = to_categorical(np.array(train['label']))\n",
        "y_test = to_categorical(np.array(test['label']))\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "#Reshape the input data for RNN\n",
        "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "#Validation dataset\n",
        "validation_size = int(0.2 * len(X_test))\n",
        "X_val = X_test[:validation_size]\n",
        "y_val = y_test[:validation_size]\n",
        "X_test = X_test[validation_size:]\n",
        "y_test = y_test[validation_size:]\n",
        "\n",
        "#Build the RNN model\n",
        "RNNSpacy = Sequential()\n",
        "RNNSpacy.add(SimpleRNN(50, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False))\n",
        "RNNSpacy.add(Dropout(0.2))\n",
        "RNNSpacy.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "RNNSpacy.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "history = RNNSpacy.fit(X_train, y_train, epochs=30, batch_size=32,\n",
        "                    validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "loss, accuracy = RNNSpacy.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aGemXnH6D4LO",
        "outputId": "1b0c16e9-4525-4bfb-8e6a-cadbb2e6a08f"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 5s 3ms/step - loss: 0.2004 - accuracy: 0.9194 - val_loss: 0.2006 - val_accuracy: 0.9220\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.1298 - accuracy: 0.9522 - val_loss: 0.2837 - val_accuracy: 0.8905\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 6s 4ms/step - loss: 0.1097 - accuracy: 0.9590 - val_loss: 0.2302 - val_accuracy: 0.9180\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.0984 - accuracy: 0.9628 - val_loss: 0.1876 - val_accuracy: 0.9265\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 3s 3ms/step - loss: 0.0903 - accuracy: 0.9664 - val_loss: 0.1984 - val_accuracy: 0.9310\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 3s 3ms/step - loss: 0.0859 - accuracy: 0.9684 - val_loss: 0.2177 - val_accuracy: 0.9260\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.0800 - accuracy: 0.9699 - val_loss: 0.2559 - val_accuracy: 0.9125\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 0.1771 - accuracy: 0.9351\n",
            "Test Accuracy: 0.9351\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Models"
      ],
      "metadata": {
        "id": "C6SkjIalGqCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['BertVector'] = train['BertVector'].apply(lambda x: np.fromstring(x[1:-1], dtype=float, sep=' '))\n",
        "test['BertVector'] = test['BertVector'].apply(lambda x: np.fromstring(x[1:-1], dtype=float, sep=' '))"
      ],
      "metadata": {
        "id": "KhNi8S_nHDvW"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_2d_bert = np.stack(train['BertVector'].values)\n",
        "X_test_2d_bert = np.stack(test['BertVector'].values)"
      ],
      "metadata": {
        "id": "VxQ_ryN8GvFs"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train['label']\n",
        "y_test = test['label']"
      ],
      "metadata": {
        "id": "vYt4ji5rHTbH"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_bertNB = Pipeline([\n",
        "     ('scaler', MinMaxScaler()),\n",
        "     ('Multi NB', MultinomialNB())\n",
        "])\n",
        "\n",
        "clf_bertNB.fit(X_train_2d_bert, y_train)\n",
        "\n",
        "y_pred_nb_bert = clf_bertNB.predict(X_test_2d_bert)\n",
        "\n",
        "print(classification_report(y_test, y_pred_nb_bert))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eum5dRqKG2V-",
        "outputId": "ef3b4dd6-6cd7-409c-f5f4-656a05ef00f1"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.41      0.50      5000\n",
            "           1       0.56      0.76      0.65      5000\n",
            "\n",
            "    accuracy                           0.59     10000\n",
            "   macro avg       0.60      0.59      0.57     10000\n",
            "weighted avg       0.60      0.59      0.57     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_bertRF = Pipeline([\n",
        "     ('scaler', StandardScaler()),\n",
        "     ('RF', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "clf_bertRF.fit(X_train_2d_bert, y_train)\n",
        "\n",
        "y_pred_rf_bert = clf_bertRF.predict(X_test_2d_bert)\n",
        "\n",
        "print(classification_report(y_test, y_pred_rf_bert))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQc6LG1xHGQz",
        "outputId": "d84fbcf7-898b-4959-a4af-c26b834c2e30"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.48      0.64      5000\n",
            "           1       0.65      0.96      0.78      5000\n",
            "\n",
            "    accuracy                           0.72     10000\n",
            "   macro avg       0.79      0.72      0.71     10000\n",
            "weighted avg       0.79      0.72      0.71     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_bertLR = Pipeline([\n",
        "     ('scaler', StandardScaler()),\n",
        "     ('LR', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "clf_bertLR.fit(X_train_2d_bert, y_train)\n",
        "\n",
        "y_pred_lr_bert = clf_bertLR.predict(X_test_2d_bert)\n",
        "\n",
        "print(classification_report(y_test, y_pred_lr_bert))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GBwYuOrHgX2",
        "outputId": "7e61220f-c3ee-4254-8aeb-f90f0c9b6e1a"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.66      0.78      5000\n",
            "           1       0.74      0.98      0.84      5000\n",
            "\n",
            "    accuracy                           0.82     10000\n",
            "   macro avg       0.85      0.82      0.81     10000\n",
            "weighted avg       0.85      0.82      0.81     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_bertKNN = Pipeline([\n",
        "     ('scaler', StandardScaler()),\n",
        "     ('KNN', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "clf_bertKNN.fit(X_train_2d_bert, y_train)\n",
        "\n",
        "y_pred_knn_bert = clf_bertKNN.predict(X_test_2d_bert)\n",
        "\n",
        "print(classification_report(y_test, y_pred_knn_bert))\n",
        "#BERT has higher dimensions than spacy vectors, probably why it's doing worse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6lJfk5nHx-5",
        "outputId": "ea337f5a-6199-40a5-b7f2-101fa0a3b304"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.62      0.72      5000\n",
            "           1       0.71      0.90      0.79      5000\n",
            "\n",
            "    accuracy                           0.76     10000\n",
            "   macro avg       0.78      0.76      0.76     10000\n",
            "weighted avg       0.78      0.76      0.76     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_bertXGB = Pipeline([\n",
        "     ('scaler', StandardScaler()), #probably don't need to scale here\n",
        "     ('XGB', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
        "])\n",
        "\n",
        "clf_bertXGB.fit(X_train_2d_bert, y_train)\n",
        "\n",
        "y_pred_xgb_bert = clf_bertXGB.predict(X_test_2d_bert)\n",
        "\n",
        "print(classification_report(y_test, y_pred_xgb_bert))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9ciUykFIEfY",
        "outputId": "4e2da033-b1de-431e-ca6d-225c7b2e056c"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.57      0.71      5000\n",
            "           1       0.69      0.97      0.81      5000\n",
            "\n",
            "    accuracy                           0.77     10000\n",
            "   macro avg       0.82      0.77      0.76     10000\n",
            "weighted avg       0.82      0.77      0.76     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_bertSVM = Pipeline([\n",
        "     ('scaler', StandardScaler()),\n",
        "     ('SVM', SVC(kernel='rbf'))\n",
        "])\n",
        "\n",
        "clf_bertSVM.fit(X_train_2d_bert, y_train)\n",
        "\n",
        "y_pred_svm_bert = clf_bertSVM.predict(X_test_2d_bert)\n",
        "\n",
        "print(classification_report(y_test, y_pred_svm_bert))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I15FqJWQIaGQ",
        "outputId": "202ff71a-c0d3-41f0-e956-1dc0b2851ddb"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.65      0.77      5000\n",
            "           1       0.73      0.97      0.84      5000\n",
            "\n",
            "    accuracy                           0.81     10000\n",
            "   macro avg       0.85      0.81      0.81     10000\n",
            "weighted avg       0.85      0.81      0.81     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(train['BertVector'].tolist())\n",
        "X_test = np.array(test['BertVector'].tolist())\n",
        "\n",
        "y_train = to_categorical(np.array(train['label']))\n",
        "y_test = to_categorical(np.array(test['label']))\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "#Reshape the input data for RNN\n",
        "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "#Validation dataset\n",
        "validation_size = int(0.2 * len(X_test))\n",
        "X_val = X_test[:validation_size]\n",
        "y_val = y_test[:validation_size]\n",
        "X_test = X_test[validation_size:]\n",
        "y_test = y_test[validation_size:]\n",
        "\n",
        "#Build the RNN model\n",
        "RNNBert = Sequential()\n",
        "RNNBert.add(SimpleRNN(50, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False))\n",
        "RNNBert.add(Dropout(0.2))\n",
        "RNNBert.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "RNNBert.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "history = RNNBert.fit(X_train, y_train, epochs=30, batch_size=32,\n",
        "                    validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "loss, accuracy = RNNBert.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wYKZOT7YIkBP",
        "outputId": "8dad687f-09e9-43e8-cc1f-05b68c7c2fbe"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 7s 4ms/step - loss: 0.2837 - accuracy: 0.8832 - val_loss: 0.7323 - val_accuracy: 0.7495\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 3s 3ms/step - loss: 0.2250 - accuracy: 0.9112 - val_loss: 0.6195 - val_accuracy: 0.7880\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 3s 3ms/step - loss: 0.2074 - accuracy: 0.9176 - val_loss: 0.7510 - val_accuracy: 0.7505\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 4s 4ms/step - loss: 0.1968 - accuracy: 0.9229 - val_loss: 0.5730 - val_accuracy: 0.7995\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1903 - accuracy: 0.9262 - val_loss: 0.6953 - val_accuracy: 0.7615\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.1859 - accuracy: 0.9270 - val_loss: 0.6130 - val_accuracy: 0.7910\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.1793 - accuracy: 0.9300 - val_loss: 0.6080 - val_accuracy: 0.8045\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.5556 - accuracy: 0.8100\n",
            "Test Accuracy: 0.8100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Models"
      ],
      "metadata": {
        "id": "FzoenDtuKQrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "ED6kFnfPKZHh"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "T7ZOgmAuKy5K",
        "outputId": "2276f26c-ee48-4028-d2c9-08ca69dd5382"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "metadata": {
        "id": "cY324T7-KS3W"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "essay = \"\"\"\n",
        "Against Banning Books: A Defense of Intellectual Freedom\n",
        "\n",
        "Banning books has been a contentious issue throughout history, sparking debates on the limits of freedom of expression and the role of literature in society. In defending intellectual freedom, it is crucial to understand why banning books is detrimental to individual growth, societal progress, and the preservation of democratic values.\n",
        "\n",
        "Intellectual and Emotional Growth\n",
        "\n",
        "Books are essential tools for intellectual and emotional development. They provide diverse perspectives, provoke critical thinking, and stimulate imagination. By exploring different worlds, characters, and scenarios, readers develop empathy and gain a deeper understanding of the human experience. Banning books restricts access to this wealth of knowledge and stifles personal growth. It creates an environment where individuals are denied the opportunity to encounter challenging ideas, which are necessary for developing a well-rounded and informed worldview.\n",
        "\n",
        "The Role of Literature in Society\n",
        "\n",
        "Literature serves as a mirror to society, reflecting its complexities, flaws, and virtues. Through books, authors address critical social issues, challenge the status quo, and inspire change. For instance, works like George Orwell's \"1984\" and Harper Lee's \"To Kill a Mockingbird\" offer profound critiques of political and social injustices. Banning such books silences important voices and hinders societal progress. It prevents readers from engaging with works that could foster a more just and equitable society by sparking dialogue and encouraging activism.\n",
        "\n",
        "Preservation of Democratic Values\n",
        "\n",
        "A fundamental principle of democracy is the freedom of expression. Banning books is a direct violation of this principle. It is an act of censorship that undermines the democratic ideal of an open and informed citizenry. When authorities decide which books are acceptable, they impose their values and beliefs on the population, curtailing the diversity of thought that is essential for a healthy democracy. The freedom to read, question, and discuss various ideas is critical for the maintenance of democratic institutions and the protection of individual rights.\n",
        "\n",
        "Historical Lessons\n",
        "\n",
        "History provides numerous examples of the dangers of book banning and censorship. During the Nazi regime, book burnings were a symbol of the broader suppression of dissent and the eradication of cultural and intellectual diversity. Similarly, in more recent times, authoritarian regimes have used book bans to control information and maintain power. These examples highlight the importance of resisting censorship to preserve freedom and prevent the erosion of civil liberties.\n",
        "\n",
        "Encouraging Critical Thinking\n",
        "\n",
        "One of the primary goals of education is to develop critical thinking skills. Exposure to a wide range of ideas, including those that may be controversial or uncomfortable, is essential for this development. Banning books limits the scope of available information and deprives students of the opportunity to engage with complex issues. Instead of sheltering individuals from challenging ideas, educators and society should encourage open discussions and debates, fostering an environment where critical thinking can flourish.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Banning books is an affront to intellectual freedom, individual growth, societal progress, and democratic values. It deprives readers of the opportunity to explore diverse perspectives, stifles critical thinking, and undermines the very principles upon which democratic societies are built. In defending the freedom to read, we uphold the right to access a broad spectrum of ideas and ensure the continued growth and enrichment of individuals and society as a whole. Rather than banning books, we should celebrate and promote the diversity of thought they offer, recognizing their indispensable role in fostering a free, open, and enlightened world.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "OBVt4sKBKYU9"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorized_essay = nlp(essay).vector\n",
        "vectorized_essay.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyD60RaEKhnn",
        "outputId": "80d86ef8-0aae-4771-c203-7fdec1e0c2b4"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_vectorized_essay = vectorized_essay.reshape(1, 1, -1)\n",
        "\n",
        "prediction = RNNSpacy.predict(X_vectorized_essay)\n",
        "predicted_class = np.argmax(prediction)\n",
        "print(f'Predicted class: {predicted_class}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ljj6ODCMBgx",
        "outputId": "70b31b7a-8a89-4a2f-f74f-035917db933e"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 32ms/step\n",
            "Predicted class: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MyEssay = \"\"\"\n",
        "Banning books, or prohibiting the teaching of a novel due to its material, is currently the most widespread form of censorship in the United States (Webb). Even classic American novels that were once a staple in public schools, such as To Kill a Mockingbird and The Adventures of Huckleberry Finn, are not safe from the ban (Johnston). Proponents of banning books argue that the ban protects students from dealing with the complex subjects of racial bigotry, sexual exploitation, explicit language, and religious tropes while in school (Aliprandini et al.). Although there may be some merit to their concern, banning books merely shields students from learning about other cultures and the unpleasant realities that some face. Rather than trying to protect their students from the hardships of reality, public school districts should not ban books that contain either relevant issues or are of important historical context, barring that they are not a religions holy text.\n",
        "Censorship of literary works is not a new practice: Nazi burned books of opposing ideologies, the church ostracized Copernicus because of his works, and people who worshiped the Bible were once persecuted (Aliprandini et al.). Throughout history, low literacy rates and inaccessibility to literary works have prevented books from having a widespread influence on society; however, since 1900, both literary rates and the accessibility of printed works have skyrocketed, leading to an increase in the effects of books on society and therefore the censorship of them (Aliprandini et al.).\n",
        "Many of the novels on the American Language Associations list of banned or challenged books are of important historical context (Admin). One of these books, the aforementioned To Kill a Mockingbird by Harper Lee, tells the story of a lawyer in the south defending an innocent black man against a rape charge, and his children against the horrors of prejudice (Little). Despite the Pulitzer Prize winning novel being voted number one in PBS Great American Read, To Kill a Mockingbird has been banned in hundreds of districts across the country (Hoover). Lees use of racial slurs, despite them being historically accurate, is the main source of contention with the novel (Little). Critics argue that it is offensive for Lee, a white woman, to use racial slurs that have been synonymous with the oppression of African Americans for years (Little). In reality, Lees strong word choice and authentic storytelling paint a more historically accurate and poignant representation of life in a segregated town. By banning To Kill a Mockingbird and other historically significant novels, schools are masking students abilities to understand history, which only serves to create a generation built on ignorance.\n",
        "I read To Kill a Mockingbird before it was banned in my district. It was not the first, nor the last, highly contended book I had read during my time in high school. To Kill a Mockingbird stuck with me, in particular, because of its relevance. Although the novel takes place in the 1930s, many of the themes it explores are still relevant today. When I read To Kill a Mockingbird in 2017, issues of race were prevalent due to the resurfacing of neo Nazism in the news. While reading To Kill a Mockingbird I would often relate its themes to the news and, I eventually saw how the novel gives society the preventative message that racism has the ability to destroy lives. By banning To Kill a Mockingbird and other historically significant novels that contain precautionary themes we are dooming ourselves to make the same mistakes as the past while undermining our ability to fix these mistakes through knowledge.\n",
        "Banning books is not only detrimental to understanding history, but also to the future of literature. Book banning has the potential to dissuade authors from writing about mature topics, which can lead to future classics never being written (Webb). By thinking about what will be read at the moment, both authors and publishers will be wary of printing a book if it is not going to be immediately financially successful (Chief, Editor in). Of the books that are banned, many are well known classics that convey an unpleasant, yet necessary message that were not accepted at the time (Chief, Editor in). Over the years, as opinions on change, so does the opinions on certain novels. When F. Scott Fitzergerald published The Great Gatsby in 1924, which was still during prohibition, the general public was shocked to see how alcohol was presented in the novel (Chief, Editor in). Another classic work, William Goldings The Lord of the Flies was also poorly received upon release (Flowers). Although these works may not have been immediately successful, they ultimately transcended time. Through banning books society is discouraging authors to write about topics that may be out of the box but may prove to be relevant in the future.\n",
        "Of the proponents for banning books, not all are advocates for protecting children from mature material. Ironic as it may be, some argue that certain novels should be banned because a teacher may not teach the material well (Aliprandini et al.). According to James LaRue, the director of the American Library Associations Office for Intellectual Freedom, certain books are not banned on the basis of the material being too mature for students, but rather that the material makes both students and teachers feel uncomfortable when discussing it (Little). When reading and analyzing a novel that delves into more difficult themes, it is vital for teachers to be able to properly convey the authors intended message in a manner that is not offensive. Asking teachers to convey political or religious themes often becomes difficult when teachers are taught to not promote their political or religious beliefs and refrain from using vulgar or racially offensive language, whether it is in the text or not. However, even if a novel features these themes, it does not mean that these novels should be removed from the curriculum; there are ways to go about learning mature topics rather than banning them.\n",
        "The best way for students and teachers to learn from uncomfortable material is through conversation. Rather than having students fill out monotonous worksheets based on the commonly held opinions of a novel, teachers should encourage the students to think critically and formulate their own opinions that ignite discussion. To question the generally accepted understanding of a novels themes or characters is not wrong, but opens up a platform for discussion. Those who argue that banning books is acceptable if it may put a teacher into an uncomfortable situation are only hindering a students ability to think creatively.\n",
        "Who stands to benefit when John Steinbecks Of Mice and Men, which deals with mental illness, is banned; who stands to benefit when Dave Pilkeys Captain Underpants series, which deals with childlike wonder, is banned; who stands to benefit when Jessica Herthel and Jazz Jenningss I am Jazz, which deals with LGBTQ+ themes, is banned (Ringel). Books are among the best teachers, and to limit a students readings to approved, dull material is not protecting them, but rather squandering their capability to think critically and learn about history, places, people, and issues.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HJ5tO0lJMIuZ"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorized_essay1 = nlp(MyEssay).vector\n",
        "vectorized_essay1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBddo5yZOE_w",
        "outputId": "5f1ebdaa-7a9c-42ee-a1ba-ee366457938b"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_vectorized_essay1 = vectorized_essay1.reshape(1, 1, -1)\n",
        "\n",
        "prediction1 = RNNSpacy.predict(X_vectorized_essay1)\n",
        "predicted_class = np.argmax(prediction1)\n",
        "print(f'Predicted class: {predicted_class}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pnb1qK-MOHc7",
        "outputId": "2016def9-21bb-4f4d-9fc0-f7014df6f9c1"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 56ms/step\n",
            "Predicted class: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0vsUbWVzOWu1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}