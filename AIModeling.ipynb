{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkvhH27/E2fipy7cqm7txT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olinyoder2534/AIWritingDetector/blob/main/AIModeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q843xgDdsfCL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/testCleanEmbeddings.csv')\n",
        "test = pd.read_csv('/content/testCleanEmbeddings.csv')"
      ],
      "metadata": {
        "id": "cKQsw5PAsh5R"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "GY3tFmYH1amb",
        "outputId": "a49e6917-042e-4738-953c-9e5e26b1e5bc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label  \\\n",
              "0  Education is undoubtedly an Vital part of life...      1   \n",
              "1  I thick students would benefit doing school at...      0   \n",
              "2  Title: The Pros ANP Cons of a New School Sport...      1   \n",
              "3  Success IU life comes not from idling, but fro...      1   \n",
              "4  For driverless cars they can have a PLCs side ...      0   \n",
              "\n",
              "                                         SpacyVector  \\\n",
              "0  [-1.3600676   0.7374259  -1.8761181   0.089783...   \n",
              "1  [-1.24247360e+00  1.91180539e+00 -3.68909240e+...   \n",
              "2  [-1.7198364   1.1876675  -1.9843843   0.510850...   \n",
              "3  [-8.82074714e-01  1.28694320e+00 -2.69012070e+...   \n",
              "4  [-1.5124509   2.3015475  -3.4847832   0.049697...   \n",
              "\n",
              "                                          BertVector  \n",
              "0  [-7.9001939e-01 -6.7574787e-01 -9.9017417e-01 ...  \n",
              "1  [-0.26419863 -0.65571606 -0.98305005  0.384800...  \n",
              "2  [-0.78946614 -0.6624161  -0.9927442   0.741319...  \n",
              "3  [-6.10299885e-01 -4.07532245e-01 -9.04479682e-...  \n",
              "4  [-0.7148344  -0.49521807 -0.9724247   0.622209...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-84cbe48f-01be-467a-ac97-de60c32bcda4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>SpacyVector</th>\n",
              "      <th>BertVector</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Education is undoubtedly an Vital part of life...</td>\n",
              "      <td>1</td>\n",
              "      <td>[-1.3600676   0.7374259  -1.8761181   0.089783...</td>\n",
              "      <td>[-7.9001939e-01 -6.7574787e-01 -9.9017417e-01 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I thick students would benefit doing school at...</td>\n",
              "      <td>0</td>\n",
              "      <td>[-1.24247360e+00  1.91180539e+00 -3.68909240e+...</td>\n",
              "      <td>[-0.26419863 -0.65571606 -0.98305005  0.384800...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Title: The Pros ANP Cons of a New School Sport...</td>\n",
              "      <td>1</td>\n",
              "      <td>[-1.7198364   1.1876675  -1.9843843   0.510850...</td>\n",
              "      <td>[-0.78946614 -0.6624161  -0.9927442   0.741319...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Success IU life comes not from idling, but fro...</td>\n",
              "      <td>1</td>\n",
              "      <td>[-8.82074714e-01  1.28694320e+00 -2.69012070e+...</td>\n",
              "      <td>[-6.10299885e-01 -4.07532245e-01 -9.04479682e-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>For driverless cars they can have a PLCs side ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[-1.5124509   2.3015475  -3.4847832   0.049697...</td>\n",
              "      <td>[-0.7148344  -0.49521807 -0.9724247   0.622209...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-84cbe48f-01be-467a-ac97-de60c32bcda4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-84cbe48f-01be-467a-ac97-de60c32bcda4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-84cbe48f-01be-467a-ac97-de60c32bcda4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d853dd9c-e186-443a-b768-e98d27ae3e93\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d853dd9c-e186-443a-b768-e98d27ae3e93')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d853dd9c-e186-443a-b768-e98d27ae3e93 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train",
              "summary": "{\n  \"name\": \"train\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          \"Dear Principle,\\n\\nI think it is not a wise decision to pass a policy where you cannot play sports without a B average. The reason so is that not everyone can surpass academically, but many can physically. Just because you don't excel in academics, doesn't mean you can't graduate either. Sure you are required to get good grades to even graduate or go to college, but you can do that along with practicing in sports. But not just sports but other activities as well.\\n\\nJust getting good grades in classes generally can take you far in your career, but what kind of careers would you be able to choose with a limited amount of knowledge just from regular classes? When kids choose activities they think they would enjoy, it opens up more experience and knowledge for future career choices. If a kid wants to do the school newspaper, he/she could grow up to be a journalist or a reporter for the news. Or if a boy wants to be an actor, he could join the drama club.\\n\\nThere are many brilliant futures to many Kids with different types of classes and the knowledge given to them. There are many places in society where they feel like they belong.\\n\\nTaking special classes will deprive them of this possible future. We kids are the next generation to run things in society, don't you think it's only fair that you can give us our own chance to succeed with our own special skills in life?\\n\\nBut along with careers, kids just want to have fun in their classes. Kids will find school more entertaining to be able to do such activities. They might even become motivated to do better in school if they find school worth coming to. These kinds of options are all about thinking ahead. Like what would happen to the students if I do this? How will it benefit the school? Is this the right decision to make? These questions can make a big difference in the future. Tot to make any offense principle, but it is important to follow such guidelines as those. I don't run the school, but it's always good to have the consent of the student who come to it.\",\n          \"I strongly think that\\n\\nUnmasking The Fate Is A Natural Landform let me tell you how. The article says that the spate Taft was titling the planet snapping photos of possible landing sites for its sister ship. When he spotted human fate enormous head nearly two miles from end to end looking back at the cameras from the planet called Colonia. Other thinks that it is just a resembled of a human head by a shadow.\\n\\nThe author predicts that its two different side that us humans have to figure out. Web surfers were waiting for the image to appear on JPL website revealing a natural landform. Web surfers also tan remake and edit a photo so who will know if It's real or not we would never know. Unmasking is trying to figure out if the fate of the human really real. Mars global surveyor is a mapping spacecraft that normally looks straight down and steins the planet like a fax machine in narrow 2.5 km wide strips. They say they never passed the fate.\\n\\nThey also say that alien markings were hidden by haze. The mission controllers prepared to look again. It's not easy to target tycoon say margin.  \",\n          \"Dear State Senator, we should not keep the Electoral College. I'm Io favor of changing to election by popular vote for the president of the united states. The electoral college is unfair. It's outdated AOD irrational. Many people prefer election by popular votes. Voters should be satisfied with their vote directly towards the president. They should't have to be upset if they choose candidates AOD those candidates choose someone else as president.\\n\\nOne of my reasons is because under the electoral college system, voters vote not for the president, but for the slate of electors, who Io turn elect for the president. The electors can be anyone not holding public office. Depending OO the state, the electors are picked by state conventions, sometimes the state party's central committee, AOD sometimes the presidential candidates themselves. The electoral college ISO't the best way to handle elections because it is the electors who elect the president, not the people, which to me sounds unfair. Voters can't always control who their electors will vote for AOD voters do get confused sometimes about the electors. So if you really agreed OO ewe president, chances are that might not be the president who's Guion be elected because the electors can choose the other person running for president instead.\\n\\nThe electoral college is unfair. The electoral college consists of 538 electors. A majority of 270 elector votes is required to elect the president. Richard Nixon, Jimmy Carter, Bob Dole, the U.S. Chamber of Commerce, AOD the AFL CIO all agreed OO abolishing the electoral act. According to a Gallup poll Io 2000, over sixty percent of voter would prefer a direct election to the kind we have now. This year voters can expect another close election Io which the popular vote wooer could again lose the presidency. Voters dew't wait the popular vote wooer to lose the presidency but because of the electoral college it can happen. Who you vote for a presidential candidate you are actually voting for a slate of electors.\\n\\nLet's say that the state legislatures are technically responsible for picking electors. Those electors can always defy the will of the people. Faithless electors have occasionally refused to vote for their party's candidate AOD cast a deciding vote for whomever they pleased. Why OO earth would they do such a thing. They should care about what voters believe Io. Io 1960, people who favored separation based OO race Io the Louisiana legislature early actually succeeded Io replacing the Democratic electors with new electors so that they would oppose John F. Failed. This means the popular votes would not have actually good to Failed. That is not fair. Candidates dew't speed time Io states they know they have OO chance of viewing. They focus only OO the tight races Io the \\\"swing\\\" states.\\n\\nState Senator, I'm Io favor of changing to election by popular votes because it's fair. The electoral college should stay Io the past AOD should'OT be used anymore. The best way is for the election by popular votes. Voters should be satisfied with their vote directly towards the president. Electors have occasionally refused to vote for their party's candidate AOD decided to vote for whomever they wanted. Selfish much? People should't have to be upset if they choose candidates AOD those candidates choose someone else as president. These electors should consider what the people wait. I believe Elections by popular votes for president of the united states is the best option.  \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SpacyVector\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9991,\n        \"samples\": [\n          \"[-5.75849235e-01  2.48542619e+00 -3.04183578e+00 -9.58367050e-01\\n  1.37712777e+00  1.29404557e+00  4.04028296e-01  3.18414283e+00\\n -2.59611320e+00  4.31805253e-01  5.84739876e+00  1.64514840e+00\\n -4.07937908e+00  9.43821311e-01  9.26747441e-01 -1.47507642e-03\\n  1.55661142e+00 -2.44196248e+00 -1.49114537e+00 -1.31779790e+00\\n  1.12473023e+00 -7.10510314e-01 -1.02684593e+00 -2.14248729e+00\\n -1.27824175e+00 -1.06777191e+00 -1.70648229e+00 -4.80897576e-01\\n -1.24517488e+00  1.45847631e+00  1.83417332e+00 -9.56405401e-01\\n -1.35942709e+00 -1.65905464e+00 -1.92246825e-01  4.60219085e-01\\n -2.50349104e-01  3.93219411e-01  2.96355939e+00  2.17912197e+00\\n  1.37511179e-01  1.88835442e+00  7.46594191e-01 -5.28952122e-01\\n -1.99455154e+00  1.57510912e+00  1.63446772e+00 -4.06589937e+00\\n -1.51659083e+00  1.57976389e+00 -5.26076108e-02  1.27755606e+00\\n  6.20556653e-01 -5.15495396e+00 -2.23376346e+00  1.92216799e-01\\n -4.16460246e-01  2.05483341e+00  9.36582565e-01  7.14234829e-01\\n  2.73390460e+00 -4.87851053e-01 -7.65101671e-01 -1.83697772e+00\\n  1.33359134e+00  1.76873159e+00 -3.02096534e+00 -3.76356673e+00\\n  2.39308238e-01  3.14260197e+00  4.87214094e-03  2.10809112e-01\\n -1.05385721e+00 -3.42084587e-01  1.57496184e-01  1.85130763e+00\\n -2.82359028e+00  1.09014297e+00 -2.52990341e+00 -6.81689382e-01\\n -4.39187431e+00 -2.61476547e-01  2.12100601e+00  5.57541490e-01\\n  1.68714356e+00 -3.98440450e-01 -1.66527653e+00 -1.97161329e+00\\n  1.70911217e+00 -1.23190597e-01 -5.26732087e-01  2.75715470e-01\\n  1.73292172e+00 -4.22742653e+00  1.08180928e+00 -1.22907317e+00\\n  1.45410717e+00 -2.22986674e+00  9.33997631e-02  1.12252021e+00\\n  2.15063548e+00  1.55133259e+00  1.42941725e+00  1.08871651e+00\\n -1.53348815e+00  3.51582003e+00 -1.43450701e+00 -1.76016188e+00\\n -1.58918428e+00 -1.80093849e+00  1.14543366e+00 -9.87908602e-01\\n -2.26896793e-01  2.13361770e-01 -6.07611477e-01  2.44761205e+00\\n -2.20299911e+00 -1.36825728e+00  1.11751938e+00 -1.13582885e+00\\n -2.26459980e+00 -1.27437770e+00 -2.27020645e+00  1.02062213e+00\\n -9.79881883e-01 -2.62778354e+00  1.91305792e+00 -2.23724127e+00\\n  1.28390312e+00 -1.35814583e+00 -1.53104997e+00 -3.21170717e-01\\n  2.98585796e+00 -1.39758217e+00 -1.84184656e-01  7.86762774e-01\\n -2.06509042e+00 -8.31247509e-01  4.06977510e+00 -2.09327984e+00\\n -1.61448717e+00 -8.48389864e-01  9.90704656e-01  1.21813142e+00\\n -8.34521651e-01 -6.31067514e-01 -2.59145260e+00 -1.23285204e-01\\n -1.36987105e-01  2.60404795e-01 -1.07869458e+00  2.38579226e+00\\n  1.96918681e-01  3.95936519e-01 -8.47995818e-01  8.76024961e-01\\n  4.28309011e+00  5.74717283e-01 -1.46589971e+00 -1.65904987e+00\\n -1.73336816e+00 -2.27957177e+00 -1.77803445e+00  1.44545746e+00\\n -2.41585326e+00 -5.22876263e-01 -3.53996825e+00  1.00519061e+00\\n -7.58796871e-01 -2.87630141e-01  1.23885882e+00  9.40052047e-02\\n  1.48674822e+00  8.36948276e-01  2.25313902e+00 -6.01299942e-01\\n -7.97252119e-01  7.49978423e-01 -1.08212245e+00 -2.08086824e+00\\n -5.40287733e-01  1.46999761e-01  2.83828688e+00 -1.21752691e+00\\n -1.59482205e+00  8.82868409e-01 -1.56289375e+00 -3.04071546e+00\\n  1.67599511e+00  2.88901281e+00 -1.12940180e+00  4.65951860e-01\\n -1.03999972e+00 -1.07486415e+00 -3.96746159e-01 -3.77678871e-01\\n -2.85017872e+00  1.48246184e-01 -1.98865384e-01  1.52784073e+00\\n -1.83725822e+00 -1.12526131e+00 -1.32262361e+00 -1.69859064e+00\\n  8.29440892e-01  7.74308562e-01 -2.17549777e+00  9.42246795e-01\\n -1.35453716e-01 -2.73204833e-01  2.41280818e+00 -4.09923255e-01\\n -3.98447871e-01  1.74886191e+00 -5.58678269e-01  2.35395384e+00\\n -1.16860524e-01 -2.64947248e+00 -1.02095413e+00  5.11261761e-01\\n -1.95660603e+00  6.18983269e-01  2.21169561e-01  6.17886364e-01\\n -2.79405024e-02 -2.02367926e+00 -2.98615843e-01  2.04822588e+00\\n  2.17528152e+00  1.32090640e+00 -5.67359149e-01 -2.25951028e+00\\n -4.63479787e-01  5.18880427e-01  1.12932193e+00  1.16324615e+00\\n -1.10396767e+00  1.74264908e+00 -2.01952849e-02  1.25950828e-01\\n -2.56986094e+00  3.06683391e-01  8.98825109e-01  6.35191321e-01\\n -3.21762294e-01  6.19685411e-01 -2.47367430e+00  5.74834824e-01\\n  2.13925934e+00  2.79274321e+00 -3.64605278e-01 -3.09611738e-01\\n -5.07697964e+00  5.34176111e-01  3.41725439e-01 -2.93961787e+00\\n  1.65989470e+00 -1.37698665e-01 -6.25838697e-01  1.24055696e+00\\n -4.50753570e-02  3.90404320e+00  2.53706741e+00  2.47743058e+00\\n  2.73329449e+00 -7.04606101e-02  8.16201448e-01  2.57068062e+00\\n -4.48246717e+00  1.36248037e-01  9.62143958e-01 -1.19026482e+00\\n -7.66760945e-01 -1.50381792e+00 -3.85761321e-01 -2.07286692e+00\\n  1.05093873e+00 -1.52933276e+00 -1.42667270e+00  2.66816235e+00\\n -3.46369445e-01 -8.14917743e-01  2.92053849e-01  5.16267657e-01\\n  2.77151656e+00 -6.58657789e-01  1.75748563e+00  1.68069303e+00\\n -1.96460223e+00 -1.94820374e-01  1.44569385e+00 -9.06449854e-01\\n  7.15855598e-01 -1.04540586e+00 -1.32361019e+00 -5.25763445e-02\\n  4.74310875e-01  1.53891623e-01 -2.86604238e+00  2.26314306e+00]\",\n          \"[-2.7778342e+00  6.2121224e-01 -1.9267410e+00  4.3267244e-01\\n  4.3630967e+00  9.2609799e-01  5.5912513e-01  4.3862300e+00\\n -7.2894460e-01 -1.0149846e+00  6.8359399e+00  1.3779345e+00\\n -3.3126087e+00  1.3508806e+00 -8.0444561e-03  2.3766150e+00\\n  9.0984088e-01 -3.5454148e-01 -2.0141363e+00 -1.5939276e+00\\n  5.4684114e-01 -7.2906917e-01 -1.1493806e+00 -5.5149138e-01\\n  7.4646509e-01 -1.4334904e+00 -1.5273789e+00 -6.2928283e-01\\n -8.7528610e-01  5.5613112e-01  8.0552107e-01 -3.6445698e-01\\n -6.2014711e-01 -2.3592296e+00 -2.2491972e+00 -9.3070734e-01\\n -2.8062394e-01  7.2506988e-01  1.1916127e+00  8.3043832e-01\\n -4.8030272e-01  6.6662574e-01 -2.3941296e-01  1.2542106e+00\\n -1.6395507e+00  8.4768164e-01  1.4756837e+00 -2.0245481e+00\\n -1.0796977e+00  1.9059176e+00 -1.8863181e+00  1.7957711e+00\\n  1.3448972e-01 -4.7943759e+00 -1.0080106e+00  1.3568276e-02\\n  5.9346408e-01  1.3473902e+00  1.1703454e+00 -5.9348637e-01\\n  7.7905041e-01 -5.7998765e-01  1.4566439e-01 -1.5677648e+00\\n  2.3144088e+00  1.8367671e+00 -2.7759154e+00 -3.1726770e+00\\n  8.4093535e-01  2.8716338e+00  4.3386373e-01  1.8794681e-01\\n -2.4193630e+00  2.6723783e-02 -1.4084674e+00  1.3860837e+00\\n -3.1061215e+00  2.4308319e+00 -3.8552306e+00  3.8806760e-01\\n -4.4443045e+00  9.5747858e-02  1.3536038e+00  2.9096013e-01\\n  1.7473071e+00 -6.5513946e-02 -2.6780291e+00 -2.6132305e+00\\n  1.7947475e+00 -4.7198465e-01 -1.3108130e+00  4.7484688e-02\\n  1.6521255e+00 -3.7206850e+00  1.4665852e+00 -1.6352036e+00\\n  3.1311759e-01 -7.9401714e-01  4.2015335e-01  2.0028548e+00\\n  2.8769641e+00  1.4559093e+00  2.1440732e+00  3.1508374e+00\\n -1.2234200e+00  4.2151999e+00  3.9623451e-01 -2.0885665e+00\\n -5.7542253e-01 -2.6945362e+00  1.0907915e+00  9.4757903e-01\\n -1.3569307e+00  8.0485666e-01  5.4104370e-01  5.9234333e-01\\n -8.2683939e-01 -3.0518305e-02 -7.6926738e-02 -4.5477623e-01\\n -4.6019948e-01 -2.4440339e+00  5.6992805e-01  1.1731633e+00\\n -1.6451761e+00 -3.4278700e+00  1.1267291e+00 -2.2928991e+00\\n  2.2524707e+00 -1.2487067e+00 -2.7599335e+00 -7.0558482e-01\\n  4.0869999e+00 -7.2617340e-01 -2.2186229e-01  1.2492661e+00\\n -2.2367175e+00 -8.7660420e-01  2.5577626e+00 -2.0051193e+00\\n -2.4755299e+00 -1.0410951e+00  1.2922682e+00  1.8296833e+00\\n  5.8638424e-01  8.3720690e-01 -2.9173753e+00 -1.1125301e+00\\n  5.1857930e-01  8.2753891e-01 -1.4320187e-01  2.9511046e+00\\n -1.4408728e-01  1.6585965e+00 -1.0159010e+00  1.5254333e+00\\n  3.0313456e+00 -9.1376942e-01 -2.9640949e+00 -1.2764635e+00\\n -8.3144271e-01 -2.0632167e+00  7.5661205e-02  2.1108570e+00\\n -2.4468701e+00 -1.8671937e+00 -3.7925458e+00  1.5604371e+00\\n -2.4678697e-01  4.3987283e-01  9.0936792e-01 -6.9623250e-01\\n  2.1803370e+00  7.7496392e-01  1.2605357e+00  3.4036708e-01\\n -4.5758042e-01 -2.8461960e-01 -2.5075359e+00 -1.3700434e+00\\n -9.7944313e-01 -4.3578181e-01  1.8181272e+00 -2.9905224e-01\\n -1.2037612e+00  8.7765354e-01 -1.7417523e+00 -5.8496678e-01\\n  1.9813261e+00  3.1016591e+00 -6.2596637e-01 -1.4832727e+00\\n -4.1250294e-01 -9.7176886e-01 -3.9758533e-01  6.2920809e-01\\n -3.4029193e+00 -5.4290462e-01 -1.6275160e-01  1.1615617e+00\\n -1.8997325e+00 -1.6305931e+00 -6.8197441e-01 -1.5622109e+00\\n  2.9366660e+00  1.0330536e+00 -2.8152378e+00  1.2971169e+00\\n  1.0045145e-01 -1.6120838e-01  1.2068169e+00  8.0046546e-01\\n -1.9252253e+00  2.4204364e+00  6.5102524e-01  2.3327005e+00\\n  8.7121654e-01 -1.9496797e+00 -2.8874695e-01  5.1792216e-01\\n -2.0239196e+00  1.6228880e+00  8.0645747e-02  5.4711092e-01\\n -1.8568553e+00 -1.6566529e+00  2.4291715e-01  2.2552464e+00\\n  1.3398952e+00  5.5033219e-01  1.8934616e+00 -3.6636941e+00\\n -1.0035713e+00  1.7463439e+00  1.7310847e+00  1.3114496e+00\\n -9.3114477e-01  8.8402104e-01 -6.4970225e-01 -5.9935462e-01\\n -1.5458752e-01  3.5570112e-01  1.4955777e+00  7.7759874e-01\\n -1.0764034e+00  1.1286227e+00 -2.6937604e+00  2.6494044e-01\\n  1.5042031e+00  2.3248341e+00  4.5245215e-01 -2.6727054e+00\\n -5.2803750e+00 -5.4781187e-01  7.7172792e-01 -2.7622106e+00\\n  1.9706244e+00 -5.9482622e-01  1.6797116e-01  8.4144628e-01\\n -1.5495385e-01  4.5079074e+00  3.0208621e+00  2.5100198e+00\\n  1.9384114e+00 -2.0997262e-01  4.4377965e-01  2.4394965e+00\\n -4.3420453e+00  1.8357104e-01  3.6614713e-01  4.2903714e-02\\n  7.1419960e-01 -2.1542454e+00  3.8805971e-01 -6.8189494e-02\\n  1.5666095e+00 -1.3793913e+00 -1.3703760e+00  2.3027813e+00\\n  4.8208618e-01 -1.3060501e-01  4.6491471e-01  1.1478925e+00\\n  3.3135269e+00 -1.9503363e+00  1.8054048e+00  2.3122940e+00\\n -1.3489916e+00  2.9439952e-03  1.0016890e+00 -2.4375062e-02\\n -7.1391219e-01  1.1161697e+00 -1.2433640e+00  1.7247325e+00\\n  3.4904206e-01 -1.1052915e+00 -2.5817118e+00  1.4101542e+00]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BertVector\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9783,\n        \"samples\": [\n          \"[-5.84655166e-01 -5.93283296e-01 -9.91583586e-01  3.07402760e-01\\n  8.98329258e-01 -5.04252613e-01 -5.78145146e-01  2.23878160e-01\\n -9.60830152e-01 -9.99269366e-01 -7.94682145e-01  9.89187241e-01\\n  9.03909326e-01  8.20069432e-01  8.94208848e-02 -4.05000389e-01\\n -1.45917475e-01 -5.51055849e-01  4.86202061e-01  9.59021270e-01\\n  4.43787545e-01  1.00000000e+00 -7.47910500e-01  5.26212156e-01\\n  5.47363043e-01  9.95168030e-01 -8.13210428e-01  4.71950769e-01\\n  6.08511806e-01  4.89041358e-01  1.50310829e-01  3.92008662e-01\\n -9.76032257e-01 -3.02706808e-01 -9.92713749e-01 -9.54901218e-01\\n  6.73899412e-01  1.05071897e-02  7.41164088e-02 -1.63143381e-01\\n -2.91173100e-01  3.69011998e-01  9.99918103e-01 -8.02723706e-01\\n  7.67642796e-01 -3.76040012e-01 -9.99998748e-01  4.46115434e-01\\n -3.15276951e-01  9.82822120e-01  9.64783847e-01  9.93394613e-01\\n  3.27170253e-01  5.18107355e-01  5.83499789e-01 -8.53549421e-01\\n -1.66880637e-02  2.67861873e-01 -4.38648194e-01 -4.88565445e-01\\n -6.11692727e-01  5.44335306e-01 -9.22222674e-01 -5.04831791e-01\\n  9.83386636e-01  9.85736907e-01 -5.67892432e-01 -3.72415513e-01\\n -2.86417603e-01  7.10242540e-02  5.69750130e-01  4.77956057e-01\\n -6.58365130e-01 -7.01812565e-01  9.29158449e-01  3.43961716e-01\\n -6.42102242e-01  1.00000000e+00 -2.33976886e-01 -8.66752923e-01\\n  9.87098038e-01  9.46135461e-01  4.73253787e-01 -7.93517709e-01\\n  8.53583932e-01 -1.00000000e+00  5.76844871e-01 -3.19230646e-01\\n -9.14756000e-01  3.94436836e-01  6.62969351e-01 -5.11124611e-01\\n  9.91491020e-01  6.55224800e-01 -6.70814633e-01 -7.86612511e-01\\n -2.91896313e-01 -9.71643865e-01 -4.72058922e-01 -7.38199592e-01\\n  4.39961225e-01 -4.89806831e-01 -6.41462088e-01 -4.80346918e-01\\n  5.98304987e-01 -6.88311100e-01  3.71449113e-01  8.01949441e-01\\n  7.67828166e-01  6.46734238e-01  4.77081537e-01 -4.40161556e-01\\n  5.87474704e-01 -7.01527774e-01  6.41417265e-01 -4.86104995e-01\\n -9.55076098e-01 -6.15039289e-01 -9.43408191e-01  4.59070116e-01\\n -5.17464519e-01 -4.94016498e-01  3.23672652e-01 -9.34534192e-01\\n  6.39184237e-01 -5.46619833e-01 -9.88719344e-01 -1.00000000e+00\\n -7.16804266e-01 -6.51537359e-01 -5.70917368e-01 -4.40053284e-01\\n -8.88411760e-01 -8.89796019e-01  6.41555369e-01  6.68760896e-01\\n  4.32825327e-01  9.99291837e-01 -3.77552271e-01  6.28843606e-01\\n -6.47221684e-01 -9.44153488e-01  8.89041007e-01 -4.89433408e-01\\n  8.97335231e-01 -6.61686242e-01  5.39352238e-01  3.04912329e-01\\n -7.84955680e-01  4.73440647e-01 -7.99401999e-01 -3.75094414e-01\\n -9.59633470e-01 -2.52436757e-01 -4.17317986e-01  6.16167903e-01\\n -7.68615067e-01 -9.93426800e-01 -7.32178867e-01 -1.67975441e-01\\n -1.61415085e-01  1.47467405e-01  8.74983311e-01  4.22742754e-01\\n -7.22634196e-01  6.21174574e-01  4.60736454e-02  3.79496545e-01\\n -2.69508988e-01 -5.64262629e-01  4.34187740e-01 -5.72246730e-01\\n -9.82745469e-01 -9.13101256e-01 -4.86934304e-01  3.32458049e-01\\n  8.93071890e-01  1.45781651e-01  4.62724119e-01  9.28427219e-01\\n -4.87074554e-01  7.51023650e-01 -8.48885834e-01  9.36808109e-01\\n -2.47050345e-01  3.43798429e-01 -9.06740367e-01  7.94561923e-01\\n -5.34503013e-02  6.80500448e-01  3.37415040e-01 -8.72315109e-01\\n -3.19671780e-01 -1.78144291e-01 -6.81592941e-01 -4.64415044e-01\\n -9.52586412e-01  1.00254685e-01 -4.31127876e-01 -3.79849821e-01\\n -3.51545632e-01  7.11523294e-01 -1.93188135e-02 -1.98774174e-01\\n  9.11701381e-01  6.19222045e-01 -1.40794650e-01  1.11402586e-01\\n  4.11392450e-01  2.91304618e-01  3.20780367e-01  9.01171029e-01\\n -9.61611629e-01 -1.09475674e-02 -4.85029221e-01 -8.53727877e-01\\n  1.27680469e-02 -3.95040363e-01 -4.57465261e-01 -5.11848509e-01\\n  8.65201414e-01 -9.18503046e-01  7.11580336e-01  3.94084156e-01\\n  9.56204295e-01 -4.66710418e-01  4.42324281e-01 -6.26952052e-01\\n  6.52078927e-01 -4.95498210e-01  9.96547341e-01  9.80100453e-01\\n -5.67340374e-01 -8.30048442e-01  9.52364564e-01 -9.92883444e-01\\n -6.48841023e-01 -8.32170665e-01 -4.97411489e-01  2.76722699e-01\\n -5.90892673e-01  8.52429390e-01  9.70146060e-01  7.55894125e-01\\n -2.07643822e-01 -9.62106466e-01  2.32254893e-01 -8.44740748e-01\\n -4.10964578e-01  6.03865504e-01  9.85930800e-01  6.94984496e-01\\n  6.84531152e-01  5.13672046e-02 -4.13945049e-01 -7.35022798e-02\\n -9.97756779e-01 -8.00867915e-01 -9.44790721e-01 -4.37958896e-01\\n -9.59813654e-01  8.81469369e-01  4.44396526e-01  9.20338392e-01\\n -6.08869016e-01 -5.01630723e-01 -7.74706304e-01 -6.81946397e-01\\n  3.22032332e-01 -5.43868423e-01 -8.84456694e-01 -1.20235914e-02\\n -7.81359732e-01 -7.96389163e-01  2.27823444e-02 -3.62742662e-01\\n -9.06754613e-01  2.96072155e-01 -3.55669022e-01  6.44385278e-01\\n  4.72781867e-01  4.97318506e-01 -9.87299263e-01  8.72904301e-01\\n  1.00000000e+00  8.81285965e-01  4.07426298e-01 -4.16914493e-01\\n -9.99997556e-01 -9.64417279e-01  9.99804080e-01 -9.99280751e-01\\n -1.00000000e+00 -3.04976612e-01 -5.29300809e-01 -5.93454130e-02\\n -1.00000000e+00 -4.95129228e-01 -7.77707174e-02 -2.69733071e-01\\n  9.03772891e-01  7.92877257e-01 -7.03113433e-03 -1.00000000e+00\\n  2.95756549e-01  5.81581473e-01 -5.13960242e-01  9.77071345e-01\\n -6.35964751e-01  8.63350034e-01  6.14299059e-01  8.35896671e-01\\n -3.19807157e-02  5.26292205e-01 -9.93480802e-01 -2.32945442e-01\\n -9.38326061e-01 -9.63670492e-01  9.99971569e-01  3.95697564e-01\\n -6.60599470e-01 -1.32077247e-01  8.98095667e-01 -3.48618448e-01\\n  1.40918558e-02 -8.30560029e-01 -5.91471255e-01  7.20209002e-01\\n  6.22249782e-01  4.71457273e-01  3.88157785e-01 -9.94356498e-02\\n  4.68146205e-01  7.22528517e-01 -5.03797412e-01  5.08286536e-01\\n -6.09500051e-01  4.23191845e-01  7.59308517e-01  4.97787356e-01\\n -9.09090102e-01 -9.25075471e-01  7.46691585e-01 -3.41473699e-01\\n  9.15158987e-01  1.00000000e+00  7.77701974e-01 -7.37424707e-04\\n  7.03671873e-01  3.47747415e-01 -1.54630780e-01  1.00000000e+00\\n  8.89821708e-01 -9.14941072e-01 -6.22791886e-01  8.01853240e-01\\n -7.67141104e-01 -8.14723730e-01  9.92688179e-01 -3.07312518e-01\\n -9.71765757e-01 -8.32404792e-01  9.54056859e-01 -9.59989667e-01\\n  9.99831378e-01 -3.77657041e-02 -8.22996736e-01  6.74741447e-01\\n  5.63721955e-01 -7.53662169e-01  1.40994638e-01  1.05473123e-01\\n -7.77617097e-01  4.63340640e-01  1.46144941e-01  6.05999887e-01\\n  3.97934824e-01 -1.68140024e-01  4.78890628e-01  8.18032742e-01\\n -6.35653436e-01  3.02783579e-01 -8.50996494e-01 -5.03366053e-01\\n  9.81819034e-01  4.68884468e-01 -2.98913538e-01  3.18933338e-01\\n -3.55586946e-01 -9.48360205e-01 -7.99983978e-01  8.63683462e-01\\n  1.00000000e+00 -5.81730783e-01  9.45484817e-01 -6.70880854e-01\\n -3.08021754e-01  3.54431063e-01  6.96894586e-01  7.16027141e-01\\n -3.74939919e-01 -3.62017930e-01  8.69935572e-01  5.30341119e-02\\n -9.75426197e-01 -6.24185324e-01  4.42570269e-01 -1.90877318e-01\\n  9.99947190e-01  7.88693011e-01  3.76826316e-01  6.34011447e-01\\n  9.98090506e-01  4.97000525e-03 -2.87927121e-01  9.90343630e-01\\n  9.35290277e-01 -5.17294168e-01  6.04098201e-01 -4.66345817e-01\\n -9.82740819e-01 -5.19057930e-01 -6.56343043e-01  1.54951826e-01\\n -8.57767582e-01 -9.46639925e-02 -8.11799765e-01  8.21869791e-01\\n  9.86402750e-01  4.95516270e-01  2.97006488e-01  9.56810951e-01\\n  1.00000000e+00 -9.95625198e-01 -3.58140260e-01  9.71780241e-01\\n -8.83119047e-01 -9.99994993e-01  4.10623699e-01 -3.59126180e-01\\n -3.97034973e-01 -9.72295225e-01 -3.94135714e-01  3.82834435e-01\\n -7.07393050e-01  9.74171638e-01  8.97438288e-01  2.28589520e-01\\n -8.15250516e-01 -8.83352399e-01 -6.54538274e-02  3.36271346e-01\\n -9.98553395e-01 -2.54472792e-01 -3.73011380e-01  7.85255969e-01\\n -4.67956245e-01 -4.66420174e-01 -7.45041668e-01 -6.28397703e-01\\n  4.01278704e-01 -4.26736504e-01  5.45000851e-01  9.74934220e-01\\n  6.70744061e-01 -9.84818876e-01 -7.52482593e-01 -3.41295779e-01\\n -3.07072140e-02  5.30350566e-01 -2.92462409e-01 -9.83234882e-01\\n -4.10041451e-01  1.00000000e+00 -1.44550338e-01  9.91117060e-01\\n -2.01766327e-01  7.00346753e-02 -3.39426726e-01  4.52429175e-01\\n  9.85861301e-01  5.48767805e-01 -9.28444862e-01 -9.63285208e-01\\n  9.84877467e-01 -5.62920690e-01  7.09320605e-01  9.46325421e-01\\n  9.15695488e-01  2.15234250e-01  9.64418411e-01  5.21203458e-01\\n -2.27903292e-01  1.91847175e-01  6.85558498e-01 -2.22679764e-01\\n -4.95834231e-01 -3.79987895e-01 -3.73347998e-01 -5.20501137e-01\\n  9.27696109e-01  1.00000000e+00  3.48462433e-01  9.01894033e-01\\n -9.36593056e-01 -9.62478220e-01 -1.02530405e-01  1.00000000e+00\\n  7.33771443e-01  2.15528473e-01  6.80868447e-01  4.17692214e-01\\n -3.33868474e-01 -2.48371571e-01 -4.42747921e-01 -4.28141892e-01\\n  4.19255137e-01  2.44921535e-01  7.41537869e-01 -5.61402917e-01\\n -9.07725513e-01 -4.29998487e-01  4.45596129e-01 -7.65074790e-01\\n  9.99999285e-01 -6.85478091e-01 -4.89190102e-01 -3.11960906e-01\\n -8.39075208e-01 -9.98233318e-01 -1.73080459e-01 -8.53130996e-01\\n -4.55068737e-01  4.52746809e-01  7.27346361e-01  3.74571204e-01\\n -5.63818097e-01 -4.67801750e-01  9.76241231e-01  8.84809971e-01\\n -9.91362453e-01 -6.77852869e-01  7.03920722e-01 -6.22632682e-01\\n  6.81674063e-01  1.00000000e+00  5.94200909e-01  7.16460288e-01\\n  4.17432219e-01 -2.75658101e-01  5.84052980e-01 -8.24856758e-01\\n  3.03482831e-01 -4.32199299e-01 -6.49777949e-01 -3.40215415e-01\\n  4.56784427e-01 -5.03905892e-01 -9.84237373e-01 -1.43217176e-01\\n  4.51365173e-01 -5.10428965e-01 -6.59745216e-01 -3.88761252e-01\\n  6.31796360e-01  5.23188591e-01 -3.57957840e-01 -2.23909467e-01\\n  3.75476450e-01 -2.22271189e-01 -1.20970547e-01 -6.63409352e-01\\n -7.27670491e-01 -1.00000000e+00  2.42733300e-01 -1.00000000e+00\\n  8.69026601e-01  7.05646753e-01 -4.89448756e-01  4.30016279e-01\\n  9.40216720e-01  9.05267835e-01 -5.49313277e-02 -9.91208315e-01\\n -1.11508310e-01  2.23838404e-01 -5.00482202e-01 -5.17397702e-01\\n  2.96266168e-01  5.14569104e-01 -1.79432631e-01  3.57312053e-01\\n -9.24791873e-01  6.11957967e-01 -5.13735771e-01  1.00000000e+00\\n  4.12313104e-01 -7.86316216e-01  6.47231996e-01  4.31436479e-01\\n -4.42304373e-01  1.00000000e+00  4.35376436e-01 -8.73625815e-01\\n  4.69981581e-01 -8.59836578e-01 -3.01599056e-01  5.89606166e-01\\n  3.82101446e-01 -7.58931160e-01 -9.85407233e-01 -3.29568744e-01\\n -7.99414963e-02 -6.77294195e-01  8.51169765e-01 -3.58627886e-01\\n -5.22739530e-01  2.95241982e-01  9.81852710e-01  9.24694121e-01\\n  4.52520043e-01  1.64993331e-02 -9.72019255e-01 -7.90533483e-01\\n  8.28243136e-01  5.95265746e-01 -7.98989356e-01  2.23032057e-01\\n  1.00000000e+00  4.35406268e-01 -5.68677664e-01  1.78490728e-01\\n -1.45258799e-01 -4.60826457e-01 -4.56273794e-01  4.99987572e-01\\n  3.87846708e-01  7.58877516e-01 -4.79516983e-01  7.04038143e-01\\n -9.71331835e-01  2.74849266e-01 -7.11733878e-01 -8.83777261e-01\\n  4.72056150e-01 -5.68154275e-01 -9.22833204e-01 -8.70582879e-01\\n  6.78208888e-01 -5.31143129e-01 -3.47768635e-01  5.94449461e-01\\n  1.76960871e-01  5.05483627e-01  4.67223823e-01 -1.00000000e+00\\n  8.11685503e-01  5.64279735e-01  9.87108290e-01  7.59334564e-01\\n  7.56122708e-01  7.67571628e-01  4.16502118e-01 -8.08527827e-01\\n  6.49538100e-01 -4.18001622e-01 -4.37999964e-01 -1.76014349e-01\\n  7.42842674e-01  3.78622055e-01  3.99739563e-01 -4.89277363e-01\\n -8.31912339e-01 -9.63502109e-01 -9.96040940e-01 -9.49220061e-01\\n  4.66318518e-01 -9.13483977e-01  5.68218887e-01  8.66538525e-01\\n  2.52699226e-01 -3.97817284e-01 -7.32348323e-01 -9.63219523e-01\\n -8.71166170e-01  2.54192613e-02 -4.26788181e-01  2.20941365e-01\\n -3.78050357e-02  8.81120116e-02  6.93588704e-02  8.80849183e-01\\n -9.82747972e-01  7.67925829e-02 -9.60180044e-01  5.00391662e-01\\n  9.85111594e-01 -8.94658923e-01  3.59205246e-01  8.43058109e-01\\n -3.94082397e-01  4.97796386e-01 -4.51450944e-01  8.40763271e-01\\n  8.41249406e-01 -5.33339679e-01  4.64202046e-01 -5.25967777e-01\\n -2.51067817e-01 -5.31477273e-01 -3.31962436e-01 -3.75138760e-01\\n -5.06708682e-01  5.91966331e-01  1.66930839e-01  5.15488982e-01\\n  9.70674336e-01 -2.91914374e-01 -1.79641128e-01 -2.35158488e-01\\n -9.44695473e-01 -6.95397377e-01 -6.30328596e-01 -1.08645149e-01\\n -6.45178974e-01  9.32959497e-01  5.64423688e-02  9.82200980e-01\\n  7.39104509e-01 -4.86763030e-01 -5.48684180e-01 -4.66134161e-01\\n  2.31753364e-01 -9.48977172e-01 -7.52540350e-01 -5.19351184e-01\\n  5.47993124e-01  2.99305379e-01  1.00000000e+00 -9.75409269e-01\\n -9.87289011e-01 -8.72863054e-01 -5.46845555e-01  5.11454463e-01\\n -4.80231315e-01 -1.00000000e+00  3.27119023e-01 -8.48326683e-01\\n  9.21593904e-01 -8.41296315e-01  9.63679731e-01 -6.99844182e-01\\n -6.00907346e-03 -4.44393873e-01  9.49110985e-01  9.54279780e-01\\n -6.33636177e-01 -5.65437973e-01  4.91357028e-01 -6.25786781e-01\\n  9.94702876e-01  1.16995178e-01 -6.66587412e-01 -5.33779562e-01\\n  6.10672235e-01 -9.79260802e-01 -6.42152131e-01  3.85339186e-02]\",\n          \"[-0.8740354  -0.47254553 -0.9566363   0.8287332   0.80180407 -0.36000028\\n  0.51923317  0.40192968 -0.87858725 -0.99997646 -0.36628747  0.93568814\\n  0.9647344   0.6827583   0.8791379  -0.6144346  -0.11868738 -0.5618305\\n  0.5076576   0.40701762  0.67584085  0.99999624 -0.2519042   0.5082235\\n  0.48226288  0.9689722  -0.78223366  0.8966832   0.9020378   0.657889\\n -0.65269     0.49055198 -0.97643155 -0.39847946 -0.98351717 -0.9819556\\n  0.61031073 -0.5742817  -0.12720625 -0.101684   -0.8280643   0.39703226\\n  0.99998474 -0.7220297   0.4637531  -0.35160518 -1.          0.4379632\\n -0.8059365   0.9222015   0.83661395  0.92876774  0.23069994  0.5353035\\n  0.5717469  -0.7313271  -0.10534425  0.12313262 -0.40352303 -0.6480844\\n -0.6145375   0.42192093 -0.8223673  -0.8670453   0.71689785  0.8610837\\n -0.36449823 -0.4038083  -0.29526183  0.052964    0.7854941   0.50998914\\n -0.5937032  -0.8127355   0.7718871   0.44375286 -0.7577852   1.\\n -0.51105845 -0.9321223   0.9647629   0.88418734  0.6762333  -0.65098387\\n  0.7568148  -1.          0.7614615  -0.1761127  -0.9779262   0.39156848\\n  0.6017731  -0.31888542  0.9615276   0.76429373 -0.7166076  -0.70650625\\n -0.3829801  -0.8327099  -0.47887564 -0.49618533  0.32549903 -0.46792772\\n -0.5827948  -0.556686    0.4303333  -0.5730347  -0.02267727  0.72888047\\n  0.4890463   0.73808646  0.66168416 -0.48346213  0.3968536  -0.92636496\\n  0.6797702  -0.41773283 -0.9786287  -0.71714014 -0.9695727   0.6248288\\n -0.03900099 -0.40964192  0.9122585  -0.65296835  0.61622036 -0.4733498\\n -0.9523012  -1.         -0.5561081  -0.37681437 -0.6536161  -0.37218368\\n -0.9424067  -0.9258921   0.62275434  0.8978233   0.3638345   0.9998675\\n -0.44690818  0.87671405 -0.4373252  -0.8825352   0.5384235  -0.530629\\n  0.7823686  -0.3776837  -0.30882937  0.3060952  -0.71995544  0.4373255\\n -0.78575045 -0.38281268 -0.8616527  -0.7536409  -0.5885311   0.9357709\\n -0.76836395 -0.96888655 -0.40807462 -0.33166093 -0.30454117  0.7425605\\n  0.79428697  0.39172864 -0.41011316  0.51670384  0.21637101  0.42317358\\n -0.7005433  -0.43788823  0.5053932  -0.4658955  -0.91352206 -0.9587357\\n -0.3672948   0.51842797  0.9640719   0.51974994  0.38689142  0.7933935\\n -0.47982106  0.6537116  -0.9494286   0.9638352  -0.27185687  0.42085215\\n -0.6140692   0.79576296 -0.68919003  0.34100613  0.7315457  -0.64860415\\n -0.752816   -0.15690476 -0.5794217  -0.54447114 -0.84522635  0.5553579\\n -0.46587276 -0.40053803 -0.15111384  0.85566384  0.9148186   0.552173\\n  0.53099185  0.5752907  -0.8175071  -0.30702698  0.34162313  0.28740177\\n  0.31102836  0.9789638  -0.87268287 -0.15209422 -0.88197106 -0.9646492\\n  0.03641352 -0.7096845  -0.21459448 -0.6503808   0.7282514  -0.7656246\\n  0.50895506  0.36327618 -0.67867196 -0.76237565  0.27544245 -0.5008944\\n  0.52304655 -0.323236    0.8747041   0.96279013 -0.76314396  0.05138829\\n  0.936031   -0.9809473  -0.69196916  0.35884795 -0.42421532  0.8030763\\n -0.6949282   0.98643404  0.9025335   0.6337663  -0.86914635 -0.9313242\\n -0.6549089  -0.74972904 -0.25288457  0.24470957  0.9190535   0.777086\\n  0.5684203   0.15121298 -0.42193976  0.96948975 -0.85713166 -0.9252206\\n -0.75855535 -0.3600327  -0.9730157   0.8085496   0.4111385   0.7084848\\n -0.54139245 -0.6625455  -0.89744085  0.62765336  0.32974142  0.9440023\\n -0.6943582  -0.6166262  -0.7298176  -0.88456404 -0.01128763 -0.29068488\\n -0.5578758   0.15940267 -0.8439102   0.48242944  0.5764245   0.564382\\n -0.9353512   0.99229026  1.          0.9486953   0.785227    0.77019805\\n -0.9999572  -0.81645143  0.9999966  -0.9919031  -1.         -0.86578333\\n -0.7373806   0.33198282 -1.         -0.38131642 -0.05029381 -0.7198686\\n  0.7969923   0.93745357  0.9302874  -1.          0.78614503  0.8926138\\n -0.76361823  0.8410725  -0.6199503   0.9532822   0.69236624  0.7183562\\n -0.34342942  0.5324385  -0.973985   -0.74103713 -0.744908   -0.8875409\\n  0.9995646   0.19972858 -0.85290396 -0.8261805   0.6752615  -0.19058448\\n -0.01731538 -0.9161605  -0.4487447   0.49394807  0.6691015   0.34496346\\n  0.43578914 -0.676106    0.41020203  0.4210728   0.26012275  0.73563504\\n -0.9273454  -0.53349245  0.60034204  0.18813023 -0.84489834 -0.9680189\\n  0.94873655 -0.45569798  0.80997366  1.          0.39367837 -0.7859802\\n  0.7296068   0.33242008 -0.25886336  1.          0.86383975 -0.961169\\n -0.76596385  0.8473237  -0.7119364  -0.7472618   0.9993935  -0.32839668\\n -0.75906175 -0.46149334  0.9630054  -0.97303724  0.9991435  -0.73261744\\n -0.92244065  0.90852827  0.842999   -0.4437131  -0.45926884  0.30294988\\n -0.87958795  0.4742387  -0.84412414  0.7296624   0.45021114 -0.13179004\\n  0.7631489  -0.23126145 -0.72891766  0.2916563  -0.7490027  -0.22149049\\n  0.96550244  0.48869753 -0.38195986  0.1958585  -0.45307463 -0.48039696\\n -0.95773995  0.8053276   1.         -0.44186378  0.6638886  -0.6583668\\n -0.07830103  0.08155884  0.619198    0.6057067  -0.3902514  -0.80833703\\n  0.78990984 -0.664888   -0.97826755  0.5884779   0.34625274 -0.17416961\\n  0.99999356  0.46094754  0.26013613  0.5922662   0.94904935  0.22287004\\n  0.29511797  0.8985257   0.95234364 -0.35476568  0.7357108   0.6481647\\n -0.89276725 -0.35998917 -0.7125309   0.16980883 -0.8196638  -0.1093974\\n -0.8866637   0.9393184   0.9624695   0.49798805  0.3790178   0.8626443\\n  1.         -0.9078356   0.5253434   0.7401212  -0.01363663 -0.999931\\n -0.66311383 -0.42373273 -0.03500776 -0.8306743  -0.37348297  0.41952258\\n -0.9150218   0.88838744  0.7409287  -0.8651302  -0.9640989  -0.51216036\\n  0.6246023   0.2490182  -0.9912665  -0.633821   -0.6271773   0.47222847\\n -0.4606335  -0.79628074 -0.43200624 -0.50250745  0.60235584 -0.46115303\\n  0.7529752   0.91431653  0.7149382  -0.9480201  -0.65159494 -0.23905255\\n -0.8031166   0.6033703  -0.75991726 -0.93880063 -0.298177    1.\\n -0.1232106   0.9412963   0.6479321   0.61771023 -0.45015624  0.21389557\\n  0.9716243   0.4375883  -0.80528885 -0.8310864   0.7349077  -0.5173722\\n  0.71623     0.6591863   0.90843856  0.5648147   0.84036124  0.25108504\\n -0.15783033  0.15899673  0.99321365 -0.2631976  -0.48512718 -0.58233523\\n -0.06787115 -0.5123235   0.28093314  1.          0.43222332  0.6516195\\n -0.97971797 -0.8727082  -0.8686744   1.          0.8394878  -0.6543304\\n  0.71127266  0.56792283 -0.17558375  0.42389846 -0.3766115  -0.3821282\\n  0.35112152  0.32806283  0.9333504  -0.53864247 -0.9371772  -0.6804247\\n  0.43788722 -0.9214611   0.9999831  -0.69910896 -0.3964221  -0.2555588\\n -0.7479656  -0.8138317  -0.0228206  -0.9676699  -0.15286402  0.3140379\\n  0.911013    0.39038548 -0.74969494 -0.9029317   0.88864744  0.72011447\\n -0.9024785  -0.89920855  0.90181905 -0.9753449   0.61260396  1.\\n  0.5486233   0.18359774  0.30293834 -0.4118856   0.41395697 -0.01247895\\n  0.39886713 -0.8653744  -0.49159965 -0.35239515  0.5169104  -0.25489447\\n -0.9184506   0.38180715  0.40059185 -0.70165414 -0.74850667 -0.29692233\\n  0.5801624   0.8178012  -0.45471466 -0.11792254  0.16745609 -0.14651012\\n -0.8871213  -0.43899447 -0.63426805 -0.9999973   0.82552713 -1.\\n  0.818623    0.4849511  -0.39483076  0.6921687   0.73232925  0.8280893\\n -0.48177162 -0.8950537   0.35766953  0.6584341  -0.46702573 -0.2385927\\n -0.42582542  0.4321361  -0.22165129  0.25620586 -0.81479645  0.7269382\\n -0.33475432  1.          0.29601622 -0.6979628  -0.6692782   0.44236788\\n -0.41938865  1.         -0.62487966 -0.9274438   0.42599788 -0.86518073\\n -0.6608464   0.54114467  0.21599494 -0.8493909  -0.92429024  0.89986455\\n  0.78196484 -0.7340725   0.66760933 -0.46765324 -0.5538683   0.16816425\\n  0.92487675  0.9717537   0.66422087  0.6892962  -0.9267169  -0.29200423\\n  0.90508425  0.42369276  0.22869281  0.2150396   1.          0.5916081\\n -0.8545653   0.57213575 -0.8935715  -0.4123943  -0.92642117  0.45141152\\n  0.34084746  0.8762211  -0.36269993  0.94762754 -0.87909573  0.16855603\\n -0.63009226 -0.64922774  0.5381374  -0.9069066  -0.9709085  -0.96152097\\n  0.5096512  -0.47448346 -0.31243655  0.4377281   0.27505383  0.579146\\n  0.5492946  -1.          0.9388225   0.51239103  0.9613074   0.8946914\\n  0.85634476  0.68504745  0.41375142 -0.9499714  -0.76861995 -0.40551656\\n -0.326162    0.5515302   0.70415133  0.71061903  0.35488018 -0.53193176\\n -0.76586163 -0.85694927 -0.8881409  -0.9842977   0.5449112  -0.62137115\\n -0.72302574  0.92424476  0.24172339 -0.1779108  -0.23199335 -0.9276603\\n  0.63932127  0.82685316 -0.03541147  0.19898328  0.46353865  0.7404129\\n  0.8524547   0.96879494 -0.94006133  0.8133059  -0.9090679   0.5932106\\n  0.78853256 -0.92445743  0.23614241  0.81772095 -0.42065537  0.41593218\\n -0.25859636 -0.7110122   0.5396636  -0.43937182  0.60113245 -0.5500285\\n -0.03729206 -0.47638673 -0.27633324 -0.5571194  -0.68766844  0.70303077\\n  0.39144462  0.816797    0.91914153 -0.22960874 -0.50290036 -0.31426203\\n -0.8323799  -0.820735    0.61111724 -0.17956723 -0.6289292   0.7590772\\n  0.08025801  0.94221526  0.22266573 -0.45990208 -0.38406292 -0.73377824\\n  0.8352482  -0.8501257  -0.70917785 -0.5383816   0.66200763  0.40077516\\n  0.9999956  -0.8977218  -0.895633   -0.6108586  -0.5206656   0.5112776\\n -0.40168685 -1.          0.4026309  -0.79755014  0.8361158  -0.42984495\\n  0.8215615  -0.7185242  -0.94855976 -0.34962702  0.6583559   0.7605806\\n -0.5600889  -0.3122686   0.7177787  -0.15809491  0.9678902   0.71344614\\n -0.91672266 -0.29856524  0.7742248  -0.9269657  -0.6368454   0.6006253 ]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "yIO5Bk7f1b1K",
        "outputId": "add3cb86-24c9-45c2-9736-5130ff1cc347"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label  \\\n",
              "0  Education is undoubtedly an Vital part of life...      1   \n",
              "1  I thick students would benefit doing school at...      0   \n",
              "2  Title: The Pros ANP Cons of a New School Sport...      1   \n",
              "3  Success IU life comes not from idling, but fro...      1   \n",
              "4  For driverless cars they can have a PLCs side ...      0   \n",
              "\n",
              "                                         SpacyVector  \\\n",
              "0  [-1.3600676   0.7374259  -1.8761181   0.089783...   \n",
              "1  [-1.24247360e+00  1.91180539e+00 -3.68909240e+...   \n",
              "2  [-1.7198364   1.1876675  -1.9843843   0.510850...   \n",
              "3  [-8.82074714e-01  1.28694320e+00 -2.69012070e+...   \n",
              "4  [-1.5124509   2.3015475  -3.4847832   0.049697...   \n",
              "\n",
              "                                          BertVector  \n",
              "0  [-7.9001939e-01 -6.7574787e-01 -9.9017417e-01 ...  \n",
              "1  [-0.26419863 -0.65571606 -0.98305005  0.384800...  \n",
              "2  [-0.78946614 -0.6624161  -0.9927442   0.741319...  \n",
              "3  [-6.10299885e-01 -4.07532245e-01 -9.04479682e-...  \n",
              "4  [-0.7148344  -0.49521807 -0.9724247   0.622209...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-342c45c5-6463-467f-8c35-763164cf8fc4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>SpacyVector</th>\n",
              "      <th>BertVector</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Education is undoubtedly an Vital part of life...</td>\n",
              "      <td>1</td>\n",
              "      <td>[-1.3600676   0.7374259  -1.8761181   0.089783...</td>\n",
              "      <td>[-7.9001939e-01 -6.7574787e-01 -9.9017417e-01 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I thick students would benefit doing school at...</td>\n",
              "      <td>0</td>\n",
              "      <td>[-1.24247360e+00  1.91180539e+00 -3.68909240e+...</td>\n",
              "      <td>[-0.26419863 -0.65571606 -0.98305005  0.384800...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Title: The Pros ANP Cons of a New School Sport...</td>\n",
              "      <td>1</td>\n",
              "      <td>[-1.7198364   1.1876675  -1.9843843   0.510850...</td>\n",
              "      <td>[-0.78946614 -0.6624161  -0.9927442   0.741319...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Success IU life comes not from idling, but fro...</td>\n",
              "      <td>1</td>\n",
              "      <td>[-8.82074714e-01  1.28694320e+00 -2.69012070e+...</td>\n",
              "      <td>[-6.10299885e-01 -4.07532245e-01 -9.04479682e-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>For driverless cars they can have a PLCs side ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[-1.5124509   2.3015475  -3.4847832   0.049697...</td>\n",
              "      <td>[-0.7148344  -0.49521807 -0.9724247   0.622209...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-342c45c5-6463-467f-8c35-763164cf8fc4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-342c45c5-6463-467f-8c35-763164cf8fc4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-342c45c5-6463-467f-8c35-763164cf8fc4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f0d996d8-b76f-4e54-a098-045244421a87\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f0d996d8-b76f-4e54-a098-045244421a87')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f0d996d8-b76f-4e54-a098-045244421a87 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test",
              "summary": "{\n  \"name\": \"test\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          \"Dear Principle,\\n\\nI think it is not a wise decision to pass a policy where you cannot play sports without a B average. The reason so is that not everyone can surpass academically, but many can physically. Just because you don't excel in academics, doesn't mean you can't graduate either. Sure you are required to get good grades to even graduate or go to college, but you can do that along with practicing in sports. But not just sports but other activities as well.\\n\\nJust getting good grades in classes generally can take you far in your career, but what kind of careers would you be able to choose with a limited amount of knowledge just from regular classes? When kids choose activities they think they would enjoy, it opens up more experience and knowledge for future career choices. If a kid wants to do the school newspaper, he/she could grow up to be a journalist or a reporter for the news. Or if a boy wants to be an actor, he could join the drama club.\\n\\nThere are many brilliant futures to many Kids with different types of classes and the knowledge given to them. There are many places in society where they feel like they belong.\\n\\nTaking special classes will deprive them of this possible future. We kids are the next generation to run things in society, don't you think it's only fair that you can give us our own chance to succeed with our own special skills in life?\\n\\nBut along with careers, kids just want to have fun in their classes. Kids will find school more entertaining to be able to do such activities. They might even become motivated to do better in school if they find school worth coming to. These kinds of options are all about thinking ahead. Like what would happen to the students if I do this? How will it benefit the school? Is this the right decision to make? These questions can make a big difference in the future. Tot to make any offense principle, but it is important to follow such guidelines as those. I don't run the school, but it's always good to have the consent of the student who come to it.\",\n          \"I strongly think that\\n\\nUnmasking The Fate Is A Natural Landform let me tell you how. The article says that the spate Taft was titling the planet snapping photos of possible landing sites for its sister ship. When he spotted human fate enormous head nearly two miles from end to end looking back at the cameras from the planet called Colonia. Other thinks that it is just a resembled of a human head by a shadow.\\n\\nThe author predicts that its two different side that us humans have to figure out. Web surfers were waiting for the image to appear on JPL website revealing a natural landform. Web surfers also tan remake and edit a photo so who will know if It's real or not we would never know. Unmasking is trying to figure out if the fate of the human really real. Mars global surveyor is a mapping spacecraft that normally looks straight down and steins the planet like a fax machine in narrow 2.5 km wide strips. They say they never passed the fate.\\n\\nThey also say that alien markings were hidden by haze. The mission controllers prepared to look again. It's not easy to target tycoon say margin.  \",\n          \"Dear State Senator, we should not keep the Electoral College. I'm Io favor of changing to election by popular vote for the president of the united states. The electoral college is unfair. It's outdated AOD irrational. Many people prefer election by popular votes. Voters should be satisfied with their vote directly towards the president. They should't have to be upset if they choose candidates AOD those candidates choose someone else as president.\\n\\nOne of my reasons is because under the electoral college system, voters vote not for the president, but for the slate of electors, who Io turn elect for the president. The electors can be anyone not holding public office. Depending OO the state, the electors are picked by state conventions, sometimes the state party's central committee, AOD sometimes the presidential candidates themselves. The electoral college ISO't the best way to handle elections because it is the electors who elect the president, not the people, which to me sounds unfair. Voters can't always control who their electors will vote for AOD voters do get confused sometimes about the electors. So if you really agreed OO ewe president, chances are that might not be the president who's Guion be elected because the electors can choose the other person running for president instead.\\n\\nThe electoral college is unfair. The electoral college consists of 538 electors. A majority of 270 elector votes is required to elect the president. Richard Nixon, Jimmy Carter, Bob Dole, the U.S. Chamber of Commerce, AOD the AFL CIO all agreed OO abolishing the electoral act. According to a Gallup poll Io 2000, over sixty percent of voter would prefer a direct election to the kind we have now. This year voters can expect another close election Io which the popular vote wooer could again lose the presidency. Voters dew't wait the popular vote wooer to lose the presidency but because of the electoral college it can happen. Who you vote for a presidential candidate you are actually voting for a slate of electors.\\n\\nLet's say that the state legislatures are technically responsible for picking electors. Those electors can always defy the will of the people. Faithless electors have occasionally refused to vote for their party's candidate AOD cast a deciding vote for whomever they pleased. Why OO earth would they do such a thing. They should care about what voters believe Io. Io 1960, people who favored separation based OO race Io the Louisiana legislature early actually succeeded Io replacing the Democratic electors with new electors so that they would oppose John F. Failed. This means the popular votes would not have actually good to Failed. That is not fair. Candidates dew't speed time Io states they know they have OO chance of viewing. They focus only OO the tight races Io the \\\"swing\\\" states.\\n\\nState Senator, I'm Io favor of changing to election by popular votes because it's fair. The electoral college should stay Io the past AOD should'OT be used anymore. The best way is for the election by popular votes. Voters should be satisfied with their vote directly towards the president. Electors have occasionally refused to vote for their party's candidate AOD decided to vote for whomever they wanted. Selfish much? People should't have to be upset if they choose candidates AOD those candidates choose someone else as president. These electors should consider what the people wait. I believe Elections by popular votes for president of the united states is the best option.  \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SpacyVector\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9991,\n        \"samples\": [\n          \"[-5.75849235e-01  2.48542619e+00 -3.04183578e+00 -9.58367050e-01\\n  1.37712777e+00  1.29404557e+00  4.04028296e-01  3.18414283e+00\\n -2.59611320e+00  4.31805253e-01  5.84739876e+00  1.64514840e+00\\n -4.07937908e+00  9.43821311e-01  9.26747441e-01 -1.47507642e-03\\n  1.55661142e+00 -2.44196248e+00 -1.49114537e+00 -1.31779790e+00\\n  1.12473023e+00 -7.10510314e-01 -1.02684593e+00 -2.14248729e+00\\n -1.27824175e+00 -1.06777191e+00 -1.70648229e+00 -4.80897576e-01\\n -1.24517488e+00  1.45847631e+00  1.83417332e+00 -9.56405401e-01\\n -1.35942709e+00 -1.65905464e+00 -1.92246825e-01  4.60219085e-01\\n -2.50349104e-01  3.93219411e-01  2.96355939e+00  2.17912197e+00\\n  1.37511179e-01  1.88835442e+00  7.46594191e-01 -5.28952122e-01\\n -1.99455154e+00  1.57510912e+00  1.63446772e+00 -4.06589937e+00\\n -1.51659083e+00  1.57976389e+00 -5.26076108e-02  1.27755606e+00\\n  6.20556653e-01 -5.15495396e+00 -2.23376346e+00  1.92216799e-01\\n -4.16460246e-01  2.05483341e+00  9.36582565e-01  7.14234829e-01\\n  2.73390460e+00 -4.87851053e-01 -7.65101671e-01 -1.83697772e+00\\n  1.33359134e+00  1.76873159e+00 -3.02096534e+00 -3.76356673e+00\\n  2.39308238e-01  3.14260197e+00  4.87214094e-03  2.10809112e-01\\n -1.05385721e+00 -3.42084587e-01  1.57496184e-01  1.85130763e+00\\n -2.82359028e+00  1.09014297e+00 -2.52990341e+00 -6.81689382e-01\\n -4.39187431e+00 -2.61476547e-01  2.12100601e+00  5.57541490e-01\\n  1.68714356e+00 -3.98440450e-01 -1.66527653e+00 -1.97161329e+00\\n  1.70911217e+00 -1.23190597e-01 -5.26732087e-01  2.75715470e-01\\n  1.73292172e+00 -4.22742653e+00  1.08180928e+00 -1.22907317e+00\\n  1.45410717e+00 -2.22986674e+00  9.33997631e-02  1.12252021e+00\\n  2.15063548e+00  1.55133259e+00  1.42941725e+00  1.08871651e+00\\n -1.53348815e+00  3.51582003e+00 -1.43450701e+00 -1.76016188e+00\\n -1.58918428e+00 -1.80093849e+00  1.14543366e+00 -9.87908602e-01\\n -2.26896793e-01  2.13361770e-01 -6.07611477e-01  2.44761205e+00\\n -2.20299911e+00 -1.36825728e+00  1.11751938e+00 -1.13582885e+00\\n -2.26459980e+00 -1.27437770e+00 -2.27020645e+00  1.02062213e+00\\n -9.79881883e-01 -2.62778354e+00  1.91305792e+00 -2.23724127e+00\\n  1.28390312e+00 -1.35814583e+00 -1.53104997e+00 -3.21170717e-01\\n  2.98585796e+00 -1.39758217e+00 -1.84184656e-01  7.86762774e-01\\n -2.06509042e+00 -8.31247509e-01  4.06977510e+00 -2.09327984e+00\\n -1.61448717e+00 -8.48389864e-01  9.90704656e-01  1.21813142e+00\\n -8.34521651e-01 -6.31067514e-01 -2.59145260e+00 -1.23285204e-01\\n -1.36987105e-01  2.60404795e-01 -1.07869458e+00  2.38579226e+00\\n  1.96918681e-01  3.95936519e-01 -8.47995818e-01  8.76024961e-01\\n  4.28309011e+00  5.74717283e-01 -1.46589971e+00 -1.65904987e+00\\n -1.73336816e+00 -2.27957177e+00 -1.77803445e+00  1.44545746e+00\\n -2.41585326e+00 -5.22876263e-01 -3.53996825e+00  1.00519061e+00\\n -7.58796871e-01 -2.87630141e-01  1.23885882e+00  9.40052047e-02\\n  1.48674822e+00  8.36948276e-01  2.25313902e+00 -6.01299942e-01\\n -7.97252119e-01  7.49978423e-01 -1.08212245e+00 -2.08086824e+00\\n -5.40287733e-01  1.46999761e-01  2.83828688e+00 -1.21752691e+00\\n -1.59482205e+00  8.82868409e-01 -1.56289375e+00 -3.04071546e+00\\n  1.67599511e+00  2.88901281e+00 -1.12940180e+00  4.65951860e-01\\n -1.03999972e+00 -1.07486415e+00 -3.96746159e-01 -3.77678871e-01\\n -2.85017872e+00  1.48246184e-01 -1.98865384e-01  1.52784073e+00\\n -1.83725822e+00 -1.12526131e+00 -1.32262361e+00 -1.69859064e+00\\n  8.29440892e-01  7.74308562e-01 -2.17549777e+00  9.42246795e-01\\n -1.35453716e-01 -2.73204833e-01  2.41280818e+00 -4.09923255e-01\\n -3.98447871e-01  1.74886191e+00 -5.58678269e-01  2.35395384e+00\\n -1.16860524e-01 -2.64947248e+00 -1.02095413e+00  5.11261761e-01\\n -1.95660603e+00  6.18983269e-01  2.21169561e-01  6.17886364e-01\\n -2.79405024e-02 -2.02367926e+00 -2.98615843e-01  2.04822588e+00\\n  2.17528152e+00  1.32090640e+00 -5.67359149e-01 -2.25951028e+00\\n -4.63479787e-01  5.18880427e-01  1.12932193e+00  1.16324615e+00\\n -1.10396767e+00  1.74264908e+00 -2.01952849e-02  1.25950828e-01\\n -2.56986094e+00  3.06683391e-01  8.98825109e-01  6.35191321e-01\\n -3.21762294e-01  6.19685411e-01 -2.47367430e+00  5.74834824e-01\\n  2.13925934e+00  2.79274321e+00 -3.64605278e-01 -3.09611738e-01\\n -5.07697964e+00  5.34176111e-01  3.41725439e-01 -2.93961787e+00\\n  1.65989470e+00 -1.37698665e-01 -6.25838697e-01  1.24055696e+00\\n -4.50753570e-02  3.90404320e+00  2.53706741e+00  2.47743058e+00\\n  2.73329449e+00 -7.04606101e-02  8.16201448e-01  2.57068062e+00\\n -4.48246717e+00  1.36248037e-01  9.62143958e-01 -1.19026482e+00\\n -7.66760945e-01 -1.50381792e+00 -3.85761321e-01 -2.07286692e+00\\n  1.05093873e+00 -1.52933276e+00 -1.42667270e+00  2.66816235e+00\\n -3.46369445e-01 -8.14917743e-01  2.92053849e-01  5.16267657e-01\\n  2.77151656e+00 -6.58657789e-01  1.75748563e+00  1.68069303e+00\\n -1.96460223e+00 -1.94820374e-01  1.44569385e+00 -9.06449854e-01\\n  7.15855598e-01 -1.04540586e+00 -1.32361019e+00 -5.25763445e-02\\n  4.74310875e-01  1.53891623e-01 -2.86604238e+00  2.26314306e+00]\",\n          \"[-2.7778342e+00  6.2121224e-01 -1.9267410e+00  4.3267244e-01\\n  4.3630967e+00  9.2609799e-01  5.5912513e-01  4.3862300e+00\\n -7.2894460e-01 -1.0149846e+00  6.8359399e+00  1.3779345e+00\\n -3.3126087e+00  1.3508806e+00 -8.0444561e-03  2.3766150e+00\\n  9.0984088e-01 -3.5454148e-01 -2.0141363e+00 -1.5939276e+00\\n  5.4684114e-01 -7.2906917e-01 -1.1493806e+00 -5.5149138e-01\\n  7.4646509e-01 -1.4334904e+00 -1.5273789e+00 -6.2928283e-01\\n -8.7528610e-01  5.5613112e-01  8.0552107e-01 -3.6445698e-01\\n -6.2014711e-01 -2.3592296e+00 -2.2491972e+00 -9.3070734e-01\\n -2.8062394e-01  7.2506988e-01  1.1916127e+00  8.3043832e-01\\n -4.8030272e-01  6.6662574e-01 -2.3941296e-01  1.2542106e+00\\n -1.6395507e+00  8.4768164e-01  1.4756837e+00 -2.0245481e+00\\n -1.0796977e+00  1.9059176e+00 -1.8863181e+00  1.7957711e+00\\n  1.3448972e-01 -4.7943759e+00 -1.0080106e+00  1.3568276e-02\\n  5.9346408e-01  1.3473902e+00  1.1703454e+00 -5.9348637e-01\\n  7.7905041e-01 -5.7998765e-01  1.4566439e-01 -1.5677648e+00\\n  2.3144088e+00  1.8367671e+00 -2.7759154e+00 -3.1726770e+00\\n  8.4093535e-01  2.8716338e+00  4.3386373e-01  1.8794681e-01\\n -2.4193630e+00  2.6723783e-02 -1.4084674e+00  1.3860837e+00\\n -3.1061215e+00  2.4308319e+00 -3.8552306e+00  3.8806760e-01\\n -4.4443045e+00  9.5747858e-02  1.3536038e+00  2.9096013e-01\\n  1.7473071e+00 -6.5513946e-02 -2.6780291e+00 -2.6132305e+00\\n  1.7947475e+00 -4.7198465e-01 -1.3108130e+00  4.7484688e-02\\n  1.6521255e+00 -3.7206850e+00  1.4665852e+00 -1.6352036e+00\\n  3.1311759e-01 -7.9401714e-01  4.2015335e-01  2.0028548e+00\\n  2.8769641e+00  1.4559093e+00  2.1440732e+00  3.1508374e+00\\n -1.2234200e+00  4.2151999e+00  3.9623451e-01 -2.0885665e+00\\n -5.7542253e-01 -2.6945362e+00  1.0907915e+00  9.4757903e-01\\n -1.3569307e+00  8.0485666e-01  5.4104370e-01  5.9234333e-01\\n -8.2683939e-01 -3.0518305e-02 -7.6926738e-02 -4.5477623e-01\\n -4.6019948e-01 -2.4440339e+00  5.6992805e-01  1.1731633e+00\\n -1.6451761e+00 -3.4278700e+00  1.1267291e+00 -2.2928991e+00\\n  2.2524707e+00 -1.2487067e+00 -2.7599335e+00 -7.0558482e-01\\n  4.0869999e+00 -7.2617340e-01 -2.2186229e-01  1.2492661e+00\\n -2.2367175e+00 -8.7660420e-01  2.5577626e+00 -2.0051193e+00\\n -2.4755299e+00 -1.0410951e+00  1.2922682e+00  1.8296833e+00\\n  5.8638424e-01  8.3720690e-01 -2.9173753e+00 -1.1125301e+00\\n  5.1857930e-01  8.2753891e-01 -1.4320187e-01  2.9511046e+00\\n -1.4408728e-01  1.6585965e+00 -1.0159010e+00  1.5254333e+00\\n  3.0313456e+00 -9.1376942e-01 -2.9640949e+00 -1.2764635e+00\\n -8.3144271e-01 -2.0632167e+00  7.5661205e-02  2.1108570e+00\\n -2.4468701e+00 -1.8671937e+00 -3.7925458e+00  1.5604371e+00\\n -2.4678697e-01  4.3987283e-01  9.0936792e-01 -6.9623250e-01\\n  2.1803370e+00  7.7496392e-01  1.2605357e+00  3.4036708e-01\\n -4.5758042e-01 -2.8461960e-01 -2.5075359e+00 -1.3700434e+00\\n -9.7944313e-01 -4.3578181e-01  1.8181272e+00 -2.9905224e-01\\n -1.2037612e+00  8.7765354e-01 -1.7417523e+00 -5.8496678e-01\\n  1.9813261e+00  3.1016591e+00 -6.2596637e-01 -1.4832727e+00\\n -4.1250294e-01 -9.7176886e-01 -3.9758533e-01  6.2920809e-01\\n -3.4029193e+00 -5.4290462e-01 -1.6275160e-01  1.1615617e+00\\n -1.8997325e+00 -1.6305931e+00 -6.8197441e-01 -1.5622109e+00\\n  2.9366660e+00  1.0330536e+00 -2.8152378e+00  1.2971169e+00\\n  1.0045145e-01 -1.6120838e-01  1.2068169e+00  8.0046546e-01\\n -1.9252253e+00  2.4204364e+00  6.5102524e-01  2.3327005e+00\\n  8.7121654e-01 -1.9496797e+00 -2.8874695e-01  5.1792216e-01\\n -2.0239196e+00  1.6228880e+00  8.0645747e-02  5.4711092e-01\\n -1.8568553e+00 -1.6566529e+00  2.4291715e-01  2.2552464e+00\\n  1.3398952e+00  5.5033219e-01  1.8934616e+00 -3.6636941e+00\\n -1.0035713e+00  1.7463439e+00  1.7310847e+00  1.3114496e+00\\n -9.3114477e-01  8.8402104e-01 -6.4970225e-01 -5.9935462e-01\\n -1.5458752e-01  3.5570112e-01  1.4955777e+00  7.7759874e-01\\n -1.0764034e+00  1.1286227e+00 -2.6937604e+00  2.6494044e-01\\n  1.5042031e+00  2.3248341e+00  4.5245215e-01 -2.6727054e+00\\n -5.2803750e+00 -5.4781187e-01  7.7172792e-01 -2.7622106e+00\\n  1.9706244e+00 -5.9482622e-01  1.6797116e-01  8.4144628e-01\\n -1.5495385e-01  4.5079074e+00  3.0208621e+00  2.5100198e+00\\n  1.9384114e+00 -2.0997262e-01  4.4377965e-01  2.4394965e+00\\n -4.3420453e+00  1.8357104e-01  3.6614713e-01  4.2903714e-02\\n  7.1419960e-01 -2.1542454e+00  3.8805971e-01 -6.8189494e-02\\n  1.5666095e+00 -1.3793913e+00 -1.3703760e+00  2.3027813e+00\\n  4.8208618e-01 -1.3060501e-01  4.6491471e-01  1.1478925e+00\\n  3.3135269e+00 -1.9503363e+00  1.8054048e+00  2.3122940e+00\\n -1.3489916e+00  2.9439952e-03  1.0016890e+00 -2.4375062e-02\\n -7.1391219e-01  1.1161697e+00 -1.2433640e+00  1.7247325e+00\\n  3.4904206e-01 -1.1052915e+00 -2.5817118e+00  1.4101542e+00]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BertVector\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9783,\n        \"samples\": [\n          \"[-5.84655166e-01 -5.93283296e-01 -9.91583586e-01  3.07402760e-01\\n  8.98329258e-01 -5.04252613e-01 -5.78145146e-01  2.23878160e-01\\n -9.60830152e-01 -9.99269366e-01 -7.94682145e-01  9.89187241e-01\\n  9.03909326e-01  8.20069432e-01  8.94208848e-02 -4.05000389e-01\\n -1.45917475e-01 -5.51055849e-01  4.86202061e-01  9.59021270e-01\\n  4.43787545e-01  1.00000000e+00 -7.47910500e-01  5.26212156e-01\\n  5.47363043e-01  9.95168030e-01 -8.13210428e-01  4.71950769e-01\\n  6.08511806e-01  4.89041358e-01  1.50310829e-01  3.92008662e-01\\n -9.76032257e-01 -3.02706808e-01 -9.92713749e-01 -9.54901218e-01\\n  6.73899412e-01  1.05071897e-02  7.41164088e-02 -1.63143381e-01\\n -2.91173100e-01  3.69011998e-01  9.99918103e-01 -8.02723706e-01\\n  7.67642796e-01 -3.76040012e-01 -9.99998748e-01  4.46115434e-01\\n -3.15276951e-01  9.82822120e-01  9.64783847e-01  9.93394613e-01\\n  3.27170253e-01  5.18107355e-01  5.83499789e-01 -8.53549421e-01\\n -1.66880637e-02  2.67861873e-01 -4.38648194e-01 -4.88565445e-01\\n -6.11692727e-01  5.44335306e-01 -9.22222674e-01 -5.04831791e-01\\n  9.83386636e-01  9.85736907e-01 -5.67892432e-01 -3.72415513e-01\\n -2.86417603e-01  7.10242540e-02  5.69750130e-01  4.77956057e-01\\n -6.58365130e-01 -7.01812565e-01  9.29158449e-01  3.43961716e-01\\n -6.42102242e-01  1.00000000e+00 -2.33976886e-01 -8.66752923e-01\\n  9.87098038e-01  9.46135461e-01  4.73253787e-01 -7.93517709e-01\\n  8.53583932e-01 -1.00000000e+00  5.76844871e-01 -3.19230646e-01\\n -9.14756000e-01  3.94436836e-01  6.62969351e-01 -5.11124611e-01\\n  9.91491020e-01  6.55224800e-01 -6.70814633e-01 -7.86612511e-01\\n -2.91896313e-01 -9.71643865e-01 -4.72058922e-01 -7.38199592e-01\\n  4.39961225e-01 -4.89806831e-01 -6.41462088e-01 -4.80346918e-01\\n  5.98304987e-01 -6.88311100e-01  3.71449113e-01  8.01949441e-01\\n  7.67828166e-01  6.46734238e-01  4.77081537e-01 -4.40161556e-01\\n  5.87474704e-01 -7.01527774e-01  6.41417265e-01 -4.86104995e-01\\n -9.55076098e-01 -6.15039289e-01 -9.43408191e-01  4.59070116e-01\\n -5.17464519e-01 -4.94016498e-01  3.23672652e-01 -9.34534192e-01\\n  6.39184237e-01 -5.46619833e-01 -9.88719344e-01 -1.00000000e+00\\n -7.16804266e-01 -6.51537359e-01 -5.70917368e-01 -4.40053284e-01\\n -8.88411760e-01 -8.89796019e-01  6.41555369e-01  6.68760896e-01\\n  4.32825327e-01  9.99291837e-01 -3.77552271e-01  6.28843606e-01\\n -6.47221684e-01 -9.44153488e-01  8.89041007e-01 -4.89433408e-01\\n  8.97335231e-01 -6.61686242e-01  5.39352238e-01  3.04912329e-01\\n -7.84955680e-01  4.73440647e-01 -7.99401999e-01 -3.75094414e-01\\n -9.59633470e-01 -2.52436757e-01 -4.17317986e-01  6.16167903e-01\\n -7.68615067e-01 -9.93426800e-01 -7.32178867e-01 -1.67975441e-01\\n -1.61415085e-01  1.47467405e-01  8.74983311e-01  4.22742754e-01\\n -7.22634196e-01  6.21174574e-01  4.60736454e-02  3.79496545e-01\\n -2.69508988e-01 -5.64262629e-01  4.34187740e-01 -5.72246730e-01\\n -9.82745469e-01 -9.13101256e-01 -4.86934304e-01  3.32458049e-01\\n  8.93071890e-01  1.45781651e-01  4.62724119e-01  9.28427219e-01\\n -4.87074554e-01  7.51023650e-01 -8.48885834e-01  9.36808109e-01\\n -2.47050345e-01  3.43798429e-01 -9.06740367e-01  7.94561923e-01\\n -5.34503013e-02  6.80500448e-01  3.37415040e-01 -8.72315109e-01\\n -3.19671780e-01 -1.78144291e-01 -6.81592941e-01 -4.64415044e-01\\n -9.52586412e-01  1.00254685e-01 -4.31127876e-01 -3.79849821e-01\\n -3.51545632e-01  7.11523294e-01 -1.93188135e-02 -1.98774174e-01\\n  9.11701381e-01  6.19222045e-01 -1.40794650e-01  1.11402586e-01\\n  4.11392450e-01  2.91304618e-01  3.20780367e-01  9.01171029e-01\\n -9.61611629e-01 -1.09475674e-02 -4.85029221e-01 -8.53727877e-01\\n  1.27680469e-02 -3.95040363e-01 -4.57465261e-01 -5.11848509e-01\\n  8.65201414e-01 -9.18503046e-01  7.11580336e-01  3.94084156e-01\\n  9.56204295e-01 -4.66710418e-01  4.42324281e-01 -6.26952052e-01\\n  6.52078927e-01 -4.95498210e-01  9.96547341e-01  9.80100453e-01\\n -5.67340374e-01 -8.30048442e-01  9.52364564e-01 -9.92883444e-01\\n -6.48841023e-01 -8.32170665e-01 -4.97411489e-01  2.76722699e-01\\n -5.90892673e-01  8.52429390e-01  9.70146060e-01  7.55894125e-01\\n -2.07643822e-01 -9.62106466e-01  2.32254893e-01 -8.44740748e-01\\n -4.10964578e-01  6.03865504e-01  9.85930800e-01  6.94984496e-01\\n  6.84531152e-01  5.13672046e-02 -4.13945049e-01 -7.35022798e-02\\n -9.97756779e-01 -8.00867915e-01 -9.44790721e-01 -4.37958896e-01\\n -9.59813654e-01  8.81469369e-01  4.44396526e-01  9.20338392e-01\\n -6.08869016e-01 -5.01630723e-01 -7.74706304e-01 -6.81946397e-01\\n  3.22032332e-01 -5.43868423e-01 -8.84456694e-01 -1.20235914e-02\\n -7.81359732e-01 -7.96389163e-01  2.27823444e-02 -3.62742662e-01\\n -9.06754613e-01  2.96072155e-01 -3.55669022e-01  6.44385278e-01\\n  4.72781867e-01  4.97318506e-01 -9.87299263e-01  8.72904301e-01\\n  1.00000000e+00  8.81285965e-01  4.07426298e-01 -4.16914493e-01\\n -9.99997556e-01 -9.64417279e-01  9.99804080e-01 -9.99280751e-01\\n -1.00000000e+00 -3.04976612e-01 -5.29300809e-01 -5.93454130e-02\\n -1.00000000e+00 -4.95129228e-01 -7.77707174e-02 -2.69733071e-01\\n  9.03772891e-01  7.92877257e-01 -7.03113433e-03 -1.00000000e+00\\n  2.95756549e-01  5.81581473e-01 -5.13960242e-01  9.77071345e-01\\n -6.35964751e-01  8.63350034e-01  6.14299059e-01  8.35896671e-01\\n -3.19807157e-02  5.26292205e-01 -9.93480802e-01 -2.32945442e-01\\n -9.38326061e-01 -9.63670492e-01  9.99971569e-01  3.95697564e-01\\n -6.60599470e-01 -1.32077247e-01  8.98095667e-01 -3.48618448e-01\\n  1.40918558e-02 -8.30560029e-01 -5.91471255e-01  7.20209002e-01\\n  6.22249782e-01  4.71457273e-01  3.88157785e-01 -9.94356498e-02\\n  4.68146205e-01  7.22528517e-01 -5.03797412e-01  5.08286536e-01\\n -6.09500051e-01  4.23191845e-01  7.59308517e-01  4.97787356e-01\\n -9.09090102e-01 -9.25075471e-01  7.46691585e-01 -3.41473699e-01\\n  9.15158987e-01  1.00000000e+00  7.77701974e-01 -7.37424707e-04\\n  7.03671873e-01  3.47747415e-01 -1.54630780e-01  1.00000000e+00\\n  8.89821708e-01 -9.14941072e-01 -6.22791886e-01  8.01853240e-01\\n -7.67141104e-01 -8.14723730e-01  9.92688179e-01 -3.07312518e-01\\n -9.71765757e-01 -8.32404792e-01  9.54056859e-01 -9.59989667e-01\\n  9.99831378e-01 -3.77657041e-02 -8.22996736e-01  6.74741447e-01\\n  5.63721955e-01 -7.53662169e-01  1.40994638e-01  1.05473123e-01\\n -7.77617097e-01  4.63340640e-01  1.46144941e-01  6.05999887e-01\\n  3.97934824e-01 -1.68140024e-01  4.78890628e-01  8.18032742e-01\\n -6.35653436e-01  3.02783579e-01 -8.50996494e-01 -5.03366053e-01\\n  9.81819034e-01  4.68884468e-01 -2.98913538e-01  3.18933338e-01\\n -3.55586946e-01 -9.48360205e-01 -7.99983978e-01  8.63683462e-01\\n  1.00000000e+00 -5.81730783e-01  9.45484817e-01 -6.70880854e-01\\n -3.08021754e-01  3.54431063e-01  6.96894586e-01  7.16027141e-01\\n -3.74939919e-01 -3.62017930e-01  8.69935572e-01  5.30341119e-02\\n -9.75426197e-01 -6.24185324e-01  4.42570269e-01 -1.90877318e-01\\n  9.99947190e-01  7.88693011e-01  3.76826316e-01  6.34011447e-01\\n  9.98090506e-01  4.97000525e-03 -2.87927121e-01  9.90343630e-01\\n  9.35290277e-01 -5.17294168e-01  6.04098201e-01 -4.66345817e-01\\n -9.82740819e-01 -5.19057930e-01 -6.56343043e-01  1.54951826e-01\\n -8.57767582e-01 -9.46639925e-02 -8.11799765e-01  8.21869791e-01\\n  9.86402750e-01  4.95516270e-01  2.97006488e-01  9.56810951e-01\\n  1.00000000e+00 -9.95625198e-01 -3.58140260e-01  9.71780241e-01\\n -8.83119047e-01 -9.99994993e-01  4.10623699e-01 -3.59126180e-01\\n -3.97034973e-01 -9.72295225e-01 -3.94135714e-01  3.82834435e-01\\n -7.07393050e-01  9.74171638e-01  8.97438288e-01  2.28589520e-01\\n -8.15250516e-01 -8.83352399e-01 -6.54538274e-02  3.36271346e-01\\n -9.98553395e-01 -2.54472792e-01 -3.73011380e-01  7.85255969e-01\\n -4.67956245e-01 -4.66420174e-01 -7.45041668e-01 -6.28397703e-01\\n  4.01278704e-01 -4.26736504e-01  5.45000851e-01  9.74934220e-01\\n  6.70744061e-01 -9.84818876e-01 -7.52482593e-01 -3.41295779e-01\\n -3.07072140e-02  5.30350566e-01 -2.92462409e-01 -9.83234882e-01\\n -4.10041451e-01  1.00000000e+00 -1.44550338e-01  9.91117060e-01\\n -2.01766327e-01  7.00346753e-02 -3.39426726e-01  4.52429175e-01\\n  9.85861301e-01  5.48767805e-01 -9.28444862e-01 -9.63285208e-01\\n  9.84877467e-01 -5.62920690e-01  7.09320605e-01  9.46325421e-01\\n  9.15695488e-01  2.15234250e-01  9.64418411e-01  5.21203458e-01\\n -2.27903292e-01  1.91847175e-01  6.85558498e-01 -2.22679764e-01\\n -4.95834231e-01 -3.79987895e-01 -3.73347998e-01 -5.20501137e-01\\n  9.27696109e-01  1.00000000e+00  3.48462433e-01  9.01894033e-01\\n -9.36593056e-01 -9.62478220e-01 -1.02530405e-01  1.00000000e+00\\n  7.33771443e-01  2.15528473e-01  6.80868447e-01  4.17692214e-01\\n -3.33868474e-01 -2.48371571e-01 -4.42747921e-01 -4.28141892e-01\\n  4.19255137e-01  2.44921535e-01  7.41537869e-01 -5.61402917e-01\\n -9.07725513e-01 -4.29998487e-01  4.45596129e-01 -7.65074790e-01\\n  9.99999285e-01 -6.85478091e-01 -4.89190102e-01 -3.11960906e-01\\n -8.39075208e-01 -9.98233318e-01 -1.73080459e-01 -8.53130996e-01\\n -4.55068737e-01  4.52746809e-01  7.27346361e-01  3.74571204e-01\\n -5.63818097e-01 -4.67801750e-01  9.76241231e-01  8.84809971e-01\\n -9.91362453e-01 -6.77852869e-01  7.03920722e-01 -6.22632682e-01\\n  6.81674063e-01  1.00000000e+00  5.94200909e-01  7.16460288e-01\\n  4.17432219e-01 -2.75658101e-01  5.84052980e-01 -8.24856758e-01\\n  3.03482831e-01 -4.32199299e-01 -6.49777949e-01 -3.40215415e-01\\n  4.56784427e-01 -5.03905892e-01 -9.84237373e-01 -1.43217176e-01\\n  4.51365173e-01 -5.10428965e-01 -6.59745216e-01 -3.88761252e-01\\n  6.31796360e-01  5.23188591e-01 -3.57957840e-01 -2.23909467e-01\\n  3.75476450e-01 -2.22271189e-01 -1.20970547e-01 -6.63409352e-01\\n -7.27670491e-01 -1.00000000e+00  2.42733300e-01 -1.00000000e+00\\n  8.69026601e-01  7.05646753e-01 -4.89448756e-01  4.30016279e-01\\n  9.40216720e-01  9.05267835e-01 -5.49313277e-02 -9.91208315e-01\\n -1.11508310e-01  2.23838404e-01 -5.00482202e-01 -5.17397702e-01\\n  2.96266168e-01  5.14569104e-01 -1.79432631e-01  3.57312053e-01\\n -9.24791873e-01  6.11957967e-01 -5.13735771e-01  1.00000000e+00\\n  4.12313104e-01 -7.86316216e-01  6.47231996e-01  4.31436479e-01\\n -4.42304373e-01  1.00000000e+00  4.35376436e-01 -8.73625815e-01\\n  4.69981581e-01 -8.59836578e-01 -3.01599056e-01  5.89606166e-01\\n  3.82101446e-01 -7.58931160e-01 -9.85407233e-01 -3.29568744e-01\\n -7.99414963e-02 -6.77294195e-01  8.51169765e-01 -3.58627886e-01\\n -5.22739530e-01  2.95241982e-01  9.81852710e-01  9.24694121e-01\\n  4.52520043e-01  1.64993331e-02 -9.72019255e-01 -7.90533483e-01\\n  8.28243136e-01  5.95265746e-01 -7.98989356e-01  2.23032057e-01\\n  1.00000000e+00  4.35406268e-01 -5.68677664e-01  1.78490728e-01\\n -1.45258799e-01 -4.60826457e-01 -4.56273794e-01  4.99987572e-01\\n  3.87846708e-01  7.58877516e-01 -4.79516983e-01  7.04038143e-01\\n -9.71331835e-01  2.74849266e-01 -7.11733878e-01 -8.83777261e-01\\n  4.72056150e-01 -5.68154275e-01 -9.22833204e-01 -8.70582879e-01\\n  6.78208888e-01 -5.31143129e-01 -3.47768635e-01  5.94449461e-01\\n  1.76960871e-01  5.05483627e-01  4.67223823e-01 -1.00000000e+00\\n  8.11685503e-01  5.64279735e-01  9.87108290e-01  7.59334564e-01\\n  7.56122708e-01  7.67571628e-01  4.16502118e-01 -8.08527827e-01\\n  6.49538100e-01 -4.18001622e-01 -4.37999964e-01 -1.76014349e-01\\n  7.42842674e-01  3.78622055e-01  3.99739563e-01 -4.89277363e-01\\n -8.31912339e-01 -9.63502109e-01 -9.96040940e-01 -9.49220061e-01\\n  4.66318518e-01 -9.13483977e-01  5.68218887e-01  8.66538525e-01\\n  2.52699226e-01 -3.97817284e-01 -7.32348323e-01 -9.63219523e-01\\n -8.71166170e-01  2.54192613e-02 -4.26788181e-01  2.20941365e-01\\n -3.78050357e-02  8.81120116e-02  6.93588704e-02  8.80849183e-01\\n -9.82747972e-01  7.67925829e-02 -9.60180044e-01  5.00391662e-01\\n  9.85111594e-01 -8.94658923e-01  3.59205246e-01  8.43058109e-01\\n -3.94082397e-01  4.97796386e-01 -4.51450944e-01  8.40763271e-01\\n  8.41249406e-01 -5.33339679e-01  4.64202046e-01 -5.25967777e-01\\n -2.51067817e-01 -5.31477273e-01 -3.31962436e-01 -3.75138760e-01\\n -5.06708682e-01  5.91966331e-01  1.66930839e-01  5.15488982e-01\\n  9.70674336e-01 -2.91914374e-01 -1.79641128e-01 -2.35158488e-01\\n -9.44695473e-01 -6.95397377e-01 -6.30328596e-01 -1.08645149e-01\\n -6.45178974e-01  9.32959497e-01  5.64423688e-02  9.82200980e-01\\n  7.39104509e-01 -4.86763030e-01 -5.48684180e-01 -4.66134161e-01\\n  2.31753364e-01 -9.48977172e-01 -7.52540350e-01 -5.19351184e-01\\n  5.47993124e-01  2.99305379e-01  1.00000000e+00 -9.75409269e-01\\n -9.87289011e-01 -8.72863054e-01 -5.46845555e-01  5.11454463e-01\\n -4.80231315e-01 -1.00000000e+00  3.27119023e-01 -8.48326683e-01\\n  9.21593904e-01 -8.41296315e-01  9.63679731e-01 -6.99844182e-01\\n -6.00907346e-03 -4.44393873e-01  9.49110985e-01  9.54279780e-01\\n -6.33636177e-01 -5.65437973e-01  4.91357028e-01 -6.25786781e-01\\n  9.94702876e-01  1.16995178e-01 -6.66587412e-01 -5.33779562e-01\\n  6.10672235e-01 -9.79260802e-01 -6.42152131e-01  3.85339186e-02]\",\n          \"[-0.8740354  -0.47254553 -0.9566363   0.8287332   0.80180407 -0.36000028\\n  0.51923317  0.40192968 -0.87858725 -0.99997646 -0.36628747  0.93568814\\n  0.9647344   0.6827583   0.8791379  -0.6144346  -0.11868738 -0.5618305\\n  0.5076576   0.40701762  0.67584085  0.99999624 -0.2519042   0.5082235\\n  0.48226288  0.9689722  -0.78223366  0.8966832   0.9020378   0.657889\\n -0.65269     0.49055198 -0.97643155 -0.39847946 -0.98351717 -0.9819556\\n  0.61031073 -0.5742817  -0.12720625 -0.101684   -0.8280643   0.39703226\\n  0.99998474 -0.7220297   0.4637531  -0.35160518 -1.          0.4379632\\n -0.8059365   0.9222015   0.83661395  0.92876774  0.23069994  0.5353035\\n  0.5717469  -0.7313271  -0.10534425  0.12313262 -0.40352303 -0.6480844\\n -0.6145375   0.42192093 -0.8223673  -0.8670453   0.71689785  0.8610837\\n -0.36449823 -0.4038083  -0.29526183  0.052964    0.7854941   0.50998914\\n -0.5937032  -0.8127355   0.7718871   0.44375286 -0.7577852   1.\\n -0.51105845 -0.9321223   0.9647629   0.88418734  0.6762333  -0.65098387\\n  0.7568148  -1.          0.7614615  -0.1761127  -0.9779262   0.39156848\\n  0.6017731  -0.31888542  0.9615276   0.76429373 -0.7166076  -0.70650625\\n -0.3829801  -0.8327099  -0.47887564 -0.49618533  0.32549903 -0.46792772\\n -0.5827948  -0.556686    0.4303333  -0.5730347  -0.02267727  0.72888047\\n  0.4890463   0.73808646  0.66168416 -0.48346213  0.3968536  -0.92636496\\n  0.6797702  -0.41773283 -0.9786287  -0.71714014 -0.9695727   0.6248288\\n -0.03900099 -0.40964192  0.9122585  -0.65296835  0.61622036 -0.4733498\\n -0.9523012  -1.         -0.5561081  -0.37681437 -0.6536161  -0.37218368\\n -0.9424067  -0.9258921   0.62275434  0.8978233   0.3638345   0.9998675\\n -0.44690818  0.87671405 -0.4373252  -0.8825352   0.5384235  -0.530629\\n  0.7823686  -0.3776837  -0.30882937  0.3060952  -0.71995544  0.4373255\\n -0.78575045 -0.38281268 -0.8616527  -0.7536409  -0.5885311   0.9357709\\n -0.76836395 -0.96888655 -0.40807462 -0.33166093 -0.30454117  0.7425605\\n  0.79428697  0.39172864 -0.41011316  0.51670384  0.21637101  0.42317358\\n -0.7005433  -0.43788823  0.5053932  -0.4658955  -0.91352206 -0.9587357\\n -0.3672948   0.51842797  0.9640719   0.51974994  0.38689142  0.7933935\\n -0.47982106  0.6537116  -0.9494286   0.9638352  -0.27185687  0.42085215\\n -0.6140692   0.79576296 -0.68919003  0.34100613  0.7315457  -0.64860415\\n -0.752816   -0.15690476 -0.5794217  -0.54447114 -0.84522635  0.5553579\\n -0.46587276 -0.40053803 -0.15111384  0.85566384  0.9148186   0.552173\\n  0.53099185  0.5752907  -0.8175071  -0.30702698  0.34162313  0.28740177\\n  0.31102836  0.9789638  -0.87268287 -0.15209422 -0.88197106 -0.9646492\\n  0.03641352 -0.7096845  -0.21459448 -0.6503808   0.7282514  -0.7656246\\n  0.50895506  0.36327618 -0.67867196 -0.76237565  0.27544245 -0.5008944\\n  0.52304655 -0.323236    0.8747041   0.96279013 -0.76314396  0.05138829\\n  0.936031   -0.9809473  -0.69196916  0.35884795 -0.42421532  0.8030763\\n -0.6949282   0.98643404  0.9025335   0.6337663  -0.86914635 -0.9313242\\n -0.6549089  -0.74972904 -0.25288457  0.24470957  0.9190535   0.777086\\n  0.5684203   0.15121298 -0.42193976  0.96948975 -0.85713166 -0.9252206\\n -0.75855535 -0.3600327  -0.9730157   0.8085496   0.4111385   0.7084848\\n -0.54139245 -0.6625455  -0.89744085  0.62765336  0.32974142  0.9440023\\n -0.6943582  -0.6166262  -0.7298176  -0.88456404 -0.01128763 -0.29068488\\n -0.5578758   0.15940267 -0.8439102   0.48242944  0.5764245   0.564382\\n -0.9353512   0.99229026  1.          0.9486953   0.785227    0.77019805\\n -0.9999572  -0.81645143  0.9999966  -0.9919031  -1.         -0.86578333\\n -0.7373806   0.33198282 -1.         -0.38131642 -0.05029381 -0.7198686\\n  0.7969923   0.93745357  0.9302874  -1.          0.78614503  0.8926138\\n -0.76361823  0.8410725  -0.6199503   0.9532822   0.69236624  0.7183562\\n -0.34342942  0.5324385  -0.973985   -0.74103713 -0.744908   -0.8875409\\n  0.9995646   0.19972858 -0.85290396 -0.8261805   0.6752615  -0.19058448\\n -0.01731538 -0.9161605  -0.4487447   0.49394807  0.6691015   0.34496346\\n  0.43578914 -0.676106    0.41020203  0.4210728   0.26012275  0.73563504\\n -0.9273454  -0.53349245  0.60034204  0.18813023 -0.84489834 -0.9680189\\n  0.94873655 -0.45569798  0.80997366  1.          0.39367837 -0.7859802\\n  0.7296068   0.33242008 -0.25886336  1.          0.86383975 -0.961169\\n -0.76596385  0.8473237  -0.7119364  -0.7472618   0.9993935  -0.32839668\\n -0.75906175 -0.46149334  0.9630054  -0.97303724  0.9991435  -0.73261744\\n -0.92244065  0.90852827  0.842999   -0.4437131  -0.45926884  0.30294988\\n -0.87958795  0.4742387  -0.84412414  0.7296624   0.45021114 -0.13179004\\n  0.7631489  -0.23126145 -0.72891766  0.2916563  -0.7490027  -0.22149049\\n  0.96550244  0.48869753 -0.38195986  0.1958585  -0.45307463 -0.48039696\\n -0.95773995  0.8053276   1.         -0.44186378  0.6638886  -0.6583668\\n -0.07830103  0.08155884  0.619198    0.6057067  -0.3902514  -0.80833703\\n  0.78990984 -0.664888   -0.97826755  0.5884779   0.34625274 -0.17416961\\n  0.99999356  0.46094754  0.26013613  0.5922662   0.94904935  0.22287004\\n  0.29511797  0.8985257   0.95234364 -0.35476568  0.7357108   0.6481647\\n -0.89276725 -0.35998917 -0.7125309   0.16980883 -0.8196638  -0.1093974\\n -0.8866637   0.9393184   0.9624695   0.49798805  0.3790178   0.8626443\\n  1.         -0.9078356   0.5253434   0.7401212  -0.01363663 -0.999931\\n -0.66311383 -0.42373273 -0.03500776 -0.8306743  -0.37348297  0.41952258\\n -0.9150218   0.88838744  0.7409287  -0.8651302  -0.9640989  -0.51216036\\n  0.6246023   0.2490182  -0.9912665  -0.633821   -0.6271773   0.47222847\\n -0.4606335  -0.79628074 -0.43200624 -0.50250745  0.60235584 -0.46115303\\n  0.7529752   0.91431653  0.7149382  -0.9480201  -0.65159494 -0.23905255\\n -0.8031166   0.6033703  -0.75991726 -0.93880063 -0.298177    1.\\n -0.1232106   0.9412963   0.6479321   0.61771023 -0.45015624  0.21389557\\n  0.9716243   0.4375883  -0.80528885 -0.8310864   0.7349077  -0.5173722\\n  0.71623     0.6591863   0.90843856  0.5648147   0.84036124  0.25108504\\n -0.15783033  0.15899673  0.99321365 -0.2631976  -0.48512718 -0.58233523\\n -0.06787115 -0.5123235   0.28093314  1.          0.43222332  0.6516195\\n -0.97971797 -0.8727082  -0.8686744   1.          0.8394878  -0.6543304\\n  0.71127266  0.56792283 -0.17558375  0.42389846 -0.3766115  -0.3821282\\n  0.35112152  0.32806283  0.9333504  -0.53864247 -0.9371772  -0.6804247\\n  0.43788722 -0.9214611   0.9999831  -0.69910896 -0.3964221  -0.2555588\\n -0.7479656  -0.8138317  -0.0228206  -0.9676699  -0.15286402  0.3140379\\n  0.911013    0.39038548 -0.74969494 -0.9029317   0.88864744  0.72011447\\n -0.9024785  -0.89920855  0.90181905 -0.9753449   0.61260396  1.\\n  0.5486233   0.18359774  0.30293834 -0.4118856   0.41395697 -0.01247895\\n  0.39886713 -0.8653744  -0.49159965 -0.35239515  0.5169104  -0.25489447\\n -0.9184506   0.38180715  0.40059185 -0.70165414 -0.74850667 -0.29692233\\n  0.5801624   0.8178012  -0.45471466 -0.11792254  0.16745609 -0.14651012\\n -0.8871213  -0.43899447 -0.63426805 -0.9999973   0.82552713 -1.\\n  0.818623    0.4849511  -0.39483076  0.6921687   0.73232925  0.8280893\\n -0.48177162 -0.8950537   0.35766953  0.6584341  -0.46702573 -0.2385927\\n -0.42582542  0.4321361  -0.22165129  0.25620586 -0.81479645  0.7269382\\n -0.33475432  1.          0.29601622 -0.6979628  -0.6692782   0.44236788\\n -0.41938865  1.         -0.62487966 -0.9274438   0.42599788 -0.86518073\\n -0.6608464   0.54114467  0.21599494 -0.8493909  -0.92429024  0.89986455\\n  0.78196484 -0.7340725   0.66760933 -0.46765324 -0.5538683   0.16816425\\n  0.92487675  0.9717537   0.66422087  0.6892962  -0.9267169  -0.29200423\\n  0.90508425  0.42369276  0.22869281  0.2150396   1.          0.5916081\\n -0.8545653   0.57213575 -0.8935715  -0.4123943  -0.92642117  0.45141152\\n  0.34084746  0.8762211  -0.36269993  0.94762754 -0.87909573  0.16855603\\n -0.63009226 -0.64922774  0.5381374  -0.9069066  -0.9709085  -0.96152097\\n  0.5096512  -0.47448346 -0.31243655  0.4377281   0.27505383  0.579146\\n  0.5492946  -1.          0.9388225   0.51239103  0.9613074   0.8946914\\n  0.85634476  0.68504745  0.41375142 -0.9499714  -0.76861995 -0.40551656\\n -0.326162    0.5515302   0.70415133  0.71061903  0.35488018 -0.53193176\\n -0.76586163 -0.85694927 -0.8881409  -0.9842977   0.5449112  -0.62137115\\n -0.72302574  0.92424476  0.24172339 -0.1779108  -0.23199335 -0.9276603\\n  0.63932127  0.82685316 -0.03541147  0.19898328  0.46353865  0.7404129\\n  0.8524547   0.96879494 -0.94006133  0.8133059  -0.9090679   0.5932106\\n  0.78853256 -0.92445743  0.23614241  0.81772095 -0.42065537  0.41593218\\n -0.25859636 -0.7110122   0.5396636  -0.43937182  0.60113245 -0.5500285\\n -0.03729206 -0.47638673 -0.27633324 -0.5571194  -0.68766844  0.70303077\\n  0.39144462  0.816797    0.91914153 -0.22960874 -0.50290036 -0.31426203\\n -0.8323799  -0.820735    0.61111724 -0.17956723 -0.6289292   0.7590772\\n  0.08025801  0.94221526  0.22266573 -0.45990208 -0.38406292 -0.73377824\\n  0.8352482  -0.8501257  -0.70917785 -0.5383816   0.66200763  0.40077516\\n  0.9999956  -0.8977218  -0.895633   -0.6108586  -0.5206656   0.5112776\\n -0.40168685 -1.          0.4026309  -0.79755014  0.8361158  -0.42984495\\n  0.8215615  -0.7185242  -0.94855976 -0.34962702  0.6583559   0.7605806\\n -0.5600889  -0.3122686   0.7177787  -0.15809491  0.9678902   0.71344614\\n -0.91672266 -0.29856524  0.7742248  -0.9269657  -0.6368454   0.6006253 ]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split test data set into validation dataset\n"
      ],
      "metadata": {
        "id": "8X6vza5M1c4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spacy Models"
      ],
      "metadata": {
        "id": "44DFJuYi3yFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "LnVOpwiP4Rcc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['SpacyVector'] = train['SpacyVector'].apply(lambda x: np.fromstring(x[1:-1], dtype=float, sep=' '))\n",
        "test['SpacyVector'] = test['SpacyVector'].apply(lambda x: np.fromstring(x[1:-1], dtype=float, sep=' '))"
      ],
      "metadata": {
        "id": "I8OHx24xCE0P"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_2d_spacy = np.stack(train['SpacyVector'].values)\n",
        "X_test_2d_spacy = np.stack(test['SpacyVector'].values)"
      ],
      "metadata": {
        "id": "Oz4p_ZwH3zxy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train['label']\n",
        "y_test = test['label']"
      ],
      "metadata": {
        "id": "pDimUC-O4WIi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NAIVE BAYES\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "clf_spacyNB = Pipeline([\n",
        "     ('scaler', MinMaxScaler()),\n",
        "     ('Multi NB', MultinomialNB())\n",
        "])\n",
        "\n",
        "clf_spacyNB.fit(X_train_2d_spacy, y_train)\n",
        "\n",
        "y_pred_nb_spacy = clf_spacyNB.predict(X_test_2d_spacy)\n",
        "\n",
        "print(classification_report(y_test, y_pred_nb_spacy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7BflrK94eOP",
        "outputId": "3cb6f1df-2884-40c1-8cd0-abe9aa4d3f2c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.74      0.75      5000\n",
            "           1       0.75      0.77      0.76      5000\n",
            "\n",
            "    accuracy                           0.76     10000\n",
            "   macro avg       0.76      0.76      0.76     10000\n",
            "weighted avg       0.76      0.76      0.76     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RANDOM FOREST\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "clf_spacyRF = Pipeline([\n",
        "     ('scaler', StandardScaler()),\n",
        "     ('RF', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "clf_spacyRF.fit(X_train_2d_spacy, y_train)\n",
        "\n",
        "y_pred_rf_spacy = clf_spacyRF.predict(X_test_2d_spacy)\n",
        "\n",
        "print(classification_report(y_test, y_pred_rf_spacy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASE4vnMMCNBh",
        "outputId": "0c03591f-6402-4bc1-c506-1bbfa6d52772"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      5000\n",
            "           1       1.00      1.00      1.00      5000\n",
            "\n",
            "    accuracy                           1.00     10000\n",
            "   macro avg       1.00      1.00      1.00     10000\n",
            "weighted avg       1.00      1.00      1.00     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LOGISTIC REGRESSION\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf_spacyLR = Pipeline([\n",
        "     ('scaler', StandardScaler()),\n",
        "     ('LR', LogisticRegression())\n",
        "])\n",
        "\n",
        "clf_spacyLR.fit(X_train_2d_spacy, y_train)\n",
        "\n",
        "y_pred_lr_spacy = clf_spacyLR.predict(X_test_2d_spacy)\n",
        "\n",
        "print(classification_report(y_test, y_pred_lr_spacy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcwBrHVkCe2e",
        "outputId": "a01737d7-32f7-4227-ad1a-35c14736814e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97      5000\n",
            "           1       0.97      0.97      0.97      5000\n",
            "\n",
            "    accuracy                           0.97     10000\n",
            "   macro avg       0.97      0.97      0.97     10000\n",
            "weighted avg       0.97      0.97      0.97     10000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "clf_spacyKNN = Pipeline([\n",
        "     ('scaler', StandardScaler()),\n",
        "     ('KNN', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "clf_spacyKNN.fit(X_train_2d_spacy, y_train)\n",
        "\n",
        "y_pred_knn_spacy = clf_spacyKNN.predict(X_test_2d_spacy)\n",
        "\n",
        "print(classification_report(y_test, y_pred_knn_spacy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzowaFBnDHeR",
        "outputId": "7b90d9a9-4567-4edb-ba60-ca875d224b00"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.98      0.96      5000\n",
            "           1       0.98      0.95      0.96      5000\n",
            "\n",
            "    accuracy                           0.96     10000\n",
            "   macro avg       0.96      0.96      0.96     10000\n",
            "weighted avg       0.96      0.96      0.96     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#XGBOOST\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "clf_spacyXGB = Pipeline([\n",
        "     ('scaler', StandardScaler()), #probably don't need to scale here\n",
        "     ('XGB', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
        "])\n",
        "\n",
        "clf_spacyXGB.fit(X_train_2d_spacy, y_train)\n",
        "\n",
        "y_pred_xgb_spacy = clf_spacyXGB.predict(X_test_2d_spacy)\n",
        "\n",
        "print(classification_report(y_test, y_pred_xgb_spacy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFppB4KyDVPg",
        "outputId": "512eee00-e212-4cfa-927a-659ef30911f8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      5000\n",
            "           1       1.00      1.00      1.00      5000\n",
            "\n",
            "    accuracy                           1.00     10000\n",
            "   macro avg       1.00      1.00      1.00     10000\n",
            "weighted avg       1.00      1.00      1.00     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SUPPORT VECTOR CLASSIFIER\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "clf_spacySVM = Pipeline([\n",
        "     ('scaler', StandardScaler()),\n",
        "     ('SVM', SVC(kernel='rbf'))\n",
        "])\n",
        "\n",
        "clf_spacySVM.fit(X_train_2d_spacy, y_train)\n",
        "\n",
        "y_pred_svm_spacy = clf_spacySVM.predict(X_test_2d_spacy)\n",
        "\n",
        "print(classification_report(y_test, y_pred_svm_spacy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKpnhHgUDeXs",
        "outputId": "94d2485d-409a-45d9-bea3-24b9fbc7a766"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      5000\n",
            "           1       0.99      0.99      0.99      5000\n",
            "\n",
            "    accuracy                           0.99     10000\n",
            "   macro avg       0.99      0.99      0.99     10000\n",
            "weighted avg       0.99      0.99      0.99     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RNN\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "bd0IjMxGD03K"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(train['SpacyVector'].tolist())\n",
        "X_test = np.array(test['SpacyVector'].tolist())\n",
        "\n",
        "y_train = to_categorical(np.array(train['label']))\n",
        "y_test = to_categorical(np.array(test['label']))\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "#Reshape the input data for RNN\n",
        "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "#Validation dataset\n",
        "validation_size = int(0.2 * len(X_test))\n",
        "X_val = X_test[:validation_size]\n",
        "y_val = y_test[:validation_size]\n",
        "X_test = X_test[validation_size:]\n",
        "y_test = y_test[validation_size:]\n",
        "\n",
        "#Build the RNN model\n",
        "RNNSpacy = Sequential()\n",
        "RNNSpacy.add(SimpleRNN(50, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False))\n",
        "RNNSpacy.add(Dropout(0.2))\n",
        "RNNSpacy.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "RNNSpacy.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "history = RNNSpacy.fit(X_train, y_train, epochs=30, batch_size=32,\n",
        "                    validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "loss, accuracy = RNNSpacy.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aGemXnH6D4LO",
        "outputId": "6d927bee-2125-43a8-e368-8989bdaec915"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "313/313 [==============================] - 6s 5ms/step - loss: 0.2319 - accuracy: 0.9025 - val_loss: 0.1341 - val_accuracy: 0.9545\n",
            "Epoch 2/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1376 - accuracy: 0.9485 - val_loss: 0.0973 - val_accuracy: 0.9695\n",
            "Epoch 3/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1032 - accuracy: 0.9592 - val_loss: 0.0723 - val_accuracy: 0.9745\n",
            "Epoch 4/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0903 - accuracy: 0.9667 - val_loss: 0.0647 - val_accuracy: 0.9785\n",
            "Epoch 5/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0809 - accuracy: 0.9700 - val_loss: 0.0578 - val_accuracy: 0.9805\n",
            "Epoch 6/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0707 - accuracy: 0.9754 - val_loss: 0.0454 - val_accuracy: 0.9850\n",
            "Epoch 7/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0618 - accuracy: 0.9781 - val_loss: 0.0404 - val_accuracy: 0.9855\n",
            "Epoch 8/30\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0565 - accuracy: 0.9793 - val_loss: 0.0353 - val_accuracy: 0.9885\n",
            "Epoch 9/30\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0532 - accuracy: 0.9809 - val_loss: 0.0285 - val_accuracy: 0.9930\n",
            "Epoch 10/30\n",
            "313/313 [==============================] - 1s 5ms/step - loss: 0.0474 - accuracy: 0.9837 - val_loss: 0.0274 - val_accuracy: 0.9905\n",
            "Epoch 11/30\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0458 - accuracy: 0.9846 - val_loss: 0.0256 - val_accuracy: 0.9930\n",
            "Epoch 12/30\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0391 - accuracy: 0.9860 - val_loss: 0.0311 - val_accuracy: 0.9920\n",
            "Epoch 13/30\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0377 - accuracy: 0.9860 - val_loss: 0.0152 - val_accuracy: 0.9985\n",
            "Epoch 14/30\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0335 - accuracy: 0.9880 - val_loss: 0.0154 - val_accuracy: 0.9960\n",
            "Epoch 15/30\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0357 - accuracy: 0.9871 - val_loss: 0.0154 - val_accuracy: 0.9960\n",
            "Epoch 16/30\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0339 - accuracy: 0.9878 - val_loss: 0.0139 - val_accuracy: 0.9955\n",
            "Epoch 17/30\n",
            "313/313 [==============================] - 1s 5ms/step - loss: 0.0310 - accuracy: 0.9889 - val_loss: 0.0146 - val_accuracy: 0.9975\n",
            "Epoch 18/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0271 - accuracy: 0.9928 - val_loss: 0.0117 - val_accuracy: 0.9975\n",
            "Epoch 19/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0287 - accuracy: 0.9905 - val_loss: 0.0094 - val_accuracy: 0.9985\n",
            "Epoch 20/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0262 - accuracy: 0.9903 - val_loss: 0.0092 - val_accuracy: 0.9980\n",
            "Epoch 21/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0250 - accuracy: 0.9918 - val_loss: 0.0117 - val_accuracy: 0.9965\n",
            "Epoch 22/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0207 - accuracy: 0.9924 - val_loss: 0.0074 - val_accuracy: 0.9990\n",
            "Epoch 23/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0197 - accuracy: 0.9933 - val_loss: 0.0097 - val_accuracy: 0.9980\n",
            "Epoch 24/30\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0217 - accuracy: 0.9919 - val_loss: 0.0094 - val_accuracy: 0.9975\n",
            "Epoch 25/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0229 - accuracy: 0.9919 - val_loss: 0.0097 - val_accuracy: 0.9985\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 0.9983\n",
            "Test Accuracy: 0.9983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Models"
      ],
      "metadata": {
        "id": "C6SkjIalGqCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['BertVector'] = train['BertVector'].apply(lambda x: np.fromstring(x[1:-1], dtype=float, sep=' '))\n",
        "test['BertVector'] = test['BertVector'].apply(lambda x: np.fromstring(x[1:-1], dtype=float, sep=' '))"
      ],
      "metadata": {
        "id": "KhNi8S_nHDvW"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_2d_bert = np.stack(train['BertVector'].values)\n",
        "X_test_2d_bert = np.stack(test['BertVector'].values)"
      ],
      "metadata": {
        "id": "VxQ_ryN8GvFs"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train['label']\n",
        "y_test = test['label']"
      ],
      "metadata": {
        "id": "vYt4ji5rHTbH"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_bertNB = Pipeline([\n",
        "     ('scaler', MinMaxScaler()),\n",
        "     ('Multi NB', MultinomialNB())\n",
        "])\n",
        "\n",
        "clf_bertNB.fit(X_train_2d_bert, y_train)\n",
        "\n",
        "y_pred_nb_bert = clf_bertNB.predict(X_test_2d_bert)\n",
        "\n",
        "print(classification_report(y_test, y_pred_nb_bert))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eum5dRqKG2V-",
        "outputId": "41199ed6-2c09-4087-acf3-a122846df736"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.71      0.72      5000\n",
            "           1       0.72      0.75      0.74      5000\n",
            "\n",
            "    accuracy                           0.73     10000\n",
            "   macro avg       0.73      0.73      0.73     10000\n",
            "weighted avg       0.73      0.73      0.73     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_bertRF = Pipeline([\n",
        "     ('scaler', StandardScaler()),\n",
        "     ('RF', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "clf_bertRF.fit(X_train_2d_bert, y_train)\n",
        "\n",
        "y_pred_rf_bert = clf_bertRF.predict(X_test_2d_bert)\n",
        "\n",
        "print(classification_report(y_test, y_pred_rf_bert))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQc6LG1xHGQz",
        "outputId": "943cf6b4-73ec-4379-c6bc-7e1fd9f2d8e7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      5000\n",
            "           1       1.00      1.00      1.00      5000\n",
            "\n",
            "    accuracy                           1.00     10000\n",
            "   macro avg       1.00      1.00      1.00     10000\n",
            "weighted avg       1.00      1.00      1.00     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_bertLR = Pipeline([\n",
        "     ('scaler', StandardScaler()),\n",
        "     ('LR', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "clf_bertLR.fit(X_train_2d_bert, y_train)\n",
        "\n",
        "y_pred_lr_bert = clf_bertLR.predict(X_test_2d_bert)\n",
        "\n",
        "print(classification_report(y_test, y_pred_lr_bert))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GBwYuOrHgX2",
        "outputId": "b31ee289-3102-4ae6-f77a-72e907d54cd9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.98      0.97      5000\n",
            "           1       0.98      0.97      0.97      5000\n",
            "\n",
            "    accuracy                           0.97     10000\n",
            "   macro avg       0.97      0.97      0.97     10000\n",
            "weighted avg       0.97      0.97      0.97     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_bertKNN = Pipeline([\n",
        "     ('scaler', StandardScaler()),\n",
        "     ('KNN', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "clf_bertKNN.fit(X_train_2d_bert, y_train)\n",
        "\n",
        "y_pred_knn_bert = clf_bertKNN.predict(X_test_2d_bert)\n",
        "\n",
        "print(classification_report(y_test, y_pred_knn_bert))\n",
        "#BERT has higher dimensions than spacy vectors, probably why it's doing worse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6lJfk5nHx-5",
        "outputId": "4347d2c1-3f2e-4fe6-c659-b4d986bc9f7b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.93      0.92      5000\n",
            "           1       0.92      0.91      0.92      5000\n",
            "\n",
            "    accuracy                           0.92     10000\n",
            "   macro avg       0.92      0.92      0.92     10000\n",
            "weighted avg       0.92      0.92      0.92     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_bertXGB = Pipeline([\n",
        "     ('scaler', StandardScaler()), #probably don't need to scale here\n",
        "     ('XGB', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
        "])\n",
        "\n",
        "clf_bertXGB.fit(X_train_2d_bert, y_train)\n",
        "\n",
        "y_pred_xgb_bert = clf_bertXGB.predict(X_test_2d_bert)\n",
        "\n",
        "print(classification_report(y_test, y_pred_xgb_bert))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9ciUykFIEfY",
        "outputId": "6e4700e5-c248-46df-8130-be0ca502040e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      5000\n",
            "           1       1.00      1.00      1.00      5000\n",
            "\n",
            "    accuracy                           1.00     10000\n",
            "   macro avg       1.00      1.00      1.00     10000\n",
            "weighted avg       1.00      1.00      1.00     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_bertSVM = Pipeline([\n",
        "     ('scaler', StandardScaler()),\n",
        "     ('SVM', SVC(kernel='rbf'))\n",
        "])\n",
        "\n",
        "clf_bertSVM.fit(X_train_2d_bert, y_train)\n",
        "\n",
        "y_pred_svm_bert = clf_bertSVM.predict(X_test_2d_bert)\n",
        "\n",
        "print(classification_report(y_test, y_pred_svm_bert))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I15FqJWQIaGQ",
        "outputId": "d46d9921-8936-4570-c7ff-d2be845b22ba"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.95      0.95      5000\n",
            "           1       0.95      0.95      0.95      5000\n",
            "\n",
            "    accuracy                           0.95     10000\n",
            "   macro avg       0.95      0.95      0.95     10000\n",
            "weighted avg       0.95      0.95      0.95     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(train['BertVector'].tolist())\n",
        "X_test = np.array(test['BertVector'].tolist())\n",
        "\n",
        "y_train = to_categorical(np.array(train['label']))\n",
        "y_test = to_categorical(np.array(test['label']))\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "#Reshape the input data for RNN\n",
        "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "#Validation dataset\n",
        "validation_size = int(0.2 * len(X_test))\n",
        "X_val = X_test[:validation_size]\n",
        "y_val = y_test[:validation_size]\n",
        "X_test = X_test[validation_size:]\n",
        "y_test = y_test[validation_size:]\n",
        "\n",
        "#Build the RNN model\n",
        "RNNBert = Sequential()\n",
        "RNNBert.add(SimpleRNN(50, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False))\n",
        "RNNBert.add(Dropout(0.2))\n",
        "RNNBert.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "RNNBert.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "history = RNNBert.fit(X_train, y_train, epochs=30, batch_size=32,\n",
        "                    validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "loss, accuracy = RNNBert.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYKZOT7YIkBP",
        "outputId": "8395bb1d-d76c-4d68-a894-0520edb56031"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "313/313 [==============================] - 2s 4ms/step - loss: 0.3230 - accuracy: 0.8610 - val_loss: 0.2399 - val_accuracy: 0.9055\n",
            "Epoch 2/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2329 - accuracy: 0.9065 - val_loss: 0.3200 - val_accuracy: 0.8665\n",
            "Epoch 3/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2204 - accuracy: 0.9086 - val_loss: 0.1810 - val_accuracy: 0.9270\n",
            "Epoch 4/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2034 - accuracy: 0.9178 - val_loss: 0.1747 - val_accuracy: 0.9315\n",
            "Epoch 5/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1860 - accuracy: 0.9251 - val_loss: 0.1750 - val_accuracy: 0.9310\n",
            "Epoch 6/30\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.1814 - accuracy: 0.9268 - val_loss: 0.1676 - val_accuracy: 0.9265\n",
            "Epoch 7/30\n",
            "313/313 [==============================] - 1s 5ms/step - loss: 0.1768 - accuracy: 0.9282 - val_loss: 0.1414 - val_accuracy: 0.9450\n",
            "Epoch 8/30\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.1679 - accuracy: 0.9321 - val_loss: 0.1427 - val_accuracy: 0.9410\n",
            "Epoch 9/30\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.1597 - accuracy: 0.9358 - val_loss: 0.1711 - val_accuracy: 0.9290\n",
            "Epoch 10/30\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.1552 - accuracy: 0.9372 - val_loss: 0.1315 - val_accuracy: 0.9500\n",
            "Epoch 11/30\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.1442 - accuracy: 0.9421 - val_loss: 0.1287 - val_accuracy: 0.9540\n",
            "Epoch 12/30\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.1366 - accuracy: 0.9442 - val_loss: 0.1363 - val_accuracy: 0.9420\n",
            "Epoch 13/30\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.1362 - accuracy: 0.9477 - val_loss: 0.1245 - val_accuracy: 0.9540\n",
            "Epoch 14/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1398 - accuracy: 0.9432 - val_loss: 0.1355 - val_accuracy: 0.9440\n",
            "Epoch 15/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1364 - accuracy: 0.9441 - val_loss: 0.0989 - val_accuracy: 0.9650\n",
            "Epoch 16/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1300 - accuracy: 0.9490 - val_loss: 0.0924 - val_accuracy: 0.9660\n",
            "Epoch 17/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1248 - accuracy: 0.9497 - val_loss: 0.0929 - val_accuracy: 0.9655\n",
            "Epoch 18/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1216 - accuracy: 0.9522 - val_loss: 0.1188 - val_accuracy: 0.9560\n",
            "Epoch 19/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1169 - accuracy: 0.9535 - val_loss: 0.0874 - val_accuracy: 0.9695\n",
            "Epoch 20/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1140 - accuracy: 0.9561 - val_loss: 0.0891 - val_accuracy: 0.9665\n",
            "Epoch 21/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1154 - accuracy: 0.9567 - val_loss: 0.0831 - val_accuracy: 0.9665\n",
            "Epoch 22/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1083 - accuracy: 0.9576 - val_loss: 0.0788 - val_accuracy: 0.9740\n",
            "Epoch 23/30\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0996 - accuracy: 0.9597 - val_loss: 0.0805 - val_accuracy: 0.9705\n",
            "Epoch 24/30\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.1072 - accuracy: 0.9576 - val_loss: 0.0936 - val_accuracy: 0.9625\n",
            "Epoch 25/30\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.1105 - accuracy: 0.9564 - val_loss: 0.0774 - val_accuracy: 0.9725\n",
            "Epoch 26/30\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.1020 - accuracy: 0.9591 - val_loss: 0.0712 - val_accuracy: 0.9750\n",
            "Epoch 27/30\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0974 - accuracy: 0.9641 - val_loss: 0.0872 - val_accuracy: 0.9655\n",
            "Epoch 28/30\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0968 - accuracy: 0.9620 - val_loss: 0.0694 - val_accuracy: 0.9715\n",
            "Epoch 29/30\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0967 - accuracy: 0.9614 - val_loss: 0.0709 - val_accuracy: 0.9710\n",
            "Epoch 30/30\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.1047 - accuracy: 0.9598 - val_loss: 0.0884 - val_accuracy: 0.9640\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.0856 - accuracy: 0.9661\n",
            "Test Accuracy: 0.9661\n"
          ]
        }
      ]
    }
  ]
}